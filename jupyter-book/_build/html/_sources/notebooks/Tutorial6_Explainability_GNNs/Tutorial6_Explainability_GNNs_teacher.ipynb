{"cells": [{"cell_type": "markdown", "id": "80c2f79e", "metadata": {}, "source": ["# Tutorial 6 - Explainability & Geometric DL - _June 16, 2023_\n", "\n", "This tutorial is divided in two parts:\n", "1. In the first part the goal will be to train and *explain* a network trained to detect Covid-19 from chest X-rays.\n", "2. In the second part you will look at the equivariance properties of a U-Net for segmentation.\n", "\n", "As before, we indicate questions with \u2753 and coding exercises with \u2328\ufe0f."]}, {"cell_type": "markdown", "id": "77022c27", "metadata": {}, "source": ["## Part 1 - Explain Covid-19 diagnosis\n", "\n", "As Covid-19 became a hot topic in healthcare research, a large amount of medical images has been made publicly available to study the disease, and a lot of machine learning studies using these images were published. This tutorial was inspired by the methodology described in [(DeGrave et al., 2021)](https://www.nature.com/articles/s42256-021-00338-7), in which the authors combined several public data sets to train and explain a network learning to detect Covid-19 from chest X-rays.\n", "\n", "More precisely the data set for this part is made of two data sets:\n", "1. Covid-19 images are from a public data set available on [GitHub](https://github.com/ieee8023/covid-chestxray-dataset),\n", "2. For normal participants, we reused the images of **Tutorial 3** on rib segmentation.\n", "\n", "Images were all resized to the same shape, then the original shape of the participant has been modified.\n", "\n", "The main goal of this tutorial is to interpret a deep learning network trained to detect Covid-19 from chest X-rays. Then the network is a classifier which learns to find the correct diagnosis (\"normal\" or \"covid\") associated to an input image."]}, {"cell_type": "code", "execution_count": null, "id": "036f8dd1", "metadata": {}, "outputs": [], "source": ["from os import path, listdir, remove\n", "import monai\n", "import numpy as np\n", "import pandas as pd\n", "import torch"]}, {"cell_type": "markdown", "id": "a843d0be", "metadata": {}, "source": ["### Data set management\n", "\n", "Download the data, unzip it, and set the data path:"]}, {"cell_type": "code", "execution_count": null, "id": "88668c19", "metadata": {}, "outputs": [], "source": ["!wget https://surfdrive.surf.nl/files/index.php/s/nku51iYxnr8ue7c/download -O Tutorial_6.zip\n", "!unzip -qo Tutorial_6.zip\n", "data_path = \"Tutorial_6\""]}, {"cell_type": "markdown", "id": "f6752797", "metadata": {}, "source": ["In the same way as in Tutorial 3, we will use `monai.data.CacheDataset` to build our data set from:\n", "- the list of samples built by `build_sample_list` (choose the mode to access train, validation or test data),\n", "- a composition of transforms that will be applied to our image. The first transform that must be applied is `LoadChestData`, which will load images according to the paths.\n", "\n", "The main difference with Tutorial 3 is the structure of our samples. Before, a sample included an image (`'img'`) and a mask (`'mask'`). As we are now performing a classification task, our sample (after the application of `LoadChestData`)  will now contain an image (`'img'`) and a label (`'label'`). This label is an integer value:\n", "- 0 corresponds to \"normal\" diagnosis,\n", "- 1 corresponds to \"covid\" diagnosis."]}, {"cell_type": "code", "execution_count": null, "id": "2122b991", "metadata": {}, "outputs": [], "source": ["def build_sample_list(data_path, mode=\"train\"):\n", "    \"\"\"\n", "    This function creates a list containing all the samples of a subset.        \n", "        \n", "    Args:\n", "        data_path (str): path to the root folder of the data set.\n", "        mode (str): subset that must be loaded. Must be chosen in [\"train\", \"val\", \"test\"].\n", "        \n", "    Returns:\n", "        (List[Dict[str, str]]) list of all samples of the data set. \n", "        One sample is a dictionary with the following keys:\n", "            - img_path (str): path to the image file.\n", "            - idx (str): unique index allowing to identify an individual image (associated to the diagnosis).\n", "            - diagnosis (str): value of the diagnosis, \"covid\" or \"normal\".\n", "    \"\"\"\n", "    \n", "    possible_modes = [\"train\", \"val\", \"test\"]\n", "    \n", "    if mode not in possible_modes:\n", "        raise ValueError(f\"Please choose a mode in {possible_modes}.\\n\"\n", "                         f\"Current mode is {mode}.\")\n", "    \n", "    data_path = path.join(data_path, mode)\n", "    file_name_list = [file_name for file_name in listdir(data_path) if not file_name.startswith(\".\")]\n", "    sample_dict_list = list()\n", "    \n", "    for file_name in file_name_list:\n", "        keys_str = path.splitext(file_name)[0]\n", "        keys = {\n", "            pair_str.split(\"-\")[0]: pair_str.split(\"-\")[1] \n", "            for pair_str in keys_str.split(\"_\")\n", "        }\n", "        keys[\"img_path\"] = path.join(data_path, file_name)\n", "        sample_dict_list.append(keys)\n", "    return sample_dict_list\n", "\n", "\n", "class LoadChestData(monai.transforms.Transform):\n", "    \"\"\"\n", "    This transform loads the image and computes the label corresponding to a sample computed by `build_sample_list`.\n", "    After the transform, the sample includes three new keys:\n", "        - img (Tensor): corresponds to the chest X-ray image.\n", "        - label (int): is the code corresponding to the diagnosis.\n", "        - img_meta_dict (dict): includes meta-data that may be useful to apply some transforms of Monai.\n", "    \"\"\"\n", "    def __init__(self):\n", "        self.label_code = {\"normal\": 0, \"covid\": 1}\n", "\n", "    def __call__(self, sample):\n", "        from PIL import Image\n", "        \n", "        image = Image.open(sample[\"img_path\"]).convert('L') # import as grayscale image\n", "        image = np.array(image, dtype=np.uint8)\n", "        label = self.label_code[sample[\"diagnosis\"]]\n", "        sample.update({\n", "            \"img\": torch.from_numpy(image).unsqueeze(0).float(), \n", "            \"label\": label,\n", "            \"img_meta_dict\": {\"affine\": np.eye(2)},\n", "            \n", "        })\n", "        return sample"]}, {"cell_type": "markdown", "id": "154d967c", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "  \u2328\ufe0f <b>Exercise</b>: Use <code>build_sample_list</code> and <code>LoadChestData</code> to build the training set.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "c424e5a2", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["train_sample_list = build_sample_list(data_path, mode=\"train\")\n", "train_data = monai.data.CacheDataset(train_sample_list, transform=LoadChestData())"]}, {"cell_type": "markdown", "id": "e4da0a27", "metadata": {}, "source": ["> \u2753 Describe your data. Are the classes balanced? Are there differences between train, validation and test data? Add any relevant information to your answer.\n", "\n", "You can use `visualize_sample` to see the images in a data set."]}, {"cell_type": "code", "execution_count": null, "id": "a38d6eba", "metadata": {}, "outputs": [], "source": ["def visualize_sample(sample):\n", "    \"\"\"\n", "    Plot the chest X-ray image included in a sample transformed by `LoadChestData`.\n", "    \"\"\"\n", "    import matplotlib.pyplot as plt\n", "    \n", "    if not isinstance(sample, dict):\n", "        raise ValueError(f\"Sample should be a dictionary. Current type is {type(sample)}\")\n", "    \n", "    # Visualize the x-ray and describe the sample in title\n", "    image = np.squeeze(sample['img'])\n", "    plt.imshow(image, 'gray')\n", "    plt.title(f\"Image #{sample['idx']} associated with {sample['diagnosis']} diagnosis\")\n", "    \n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "c6ec0fa1", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["modes = [\"train\", \"val\", \"test\"]\n", "\n", "for mode in modes:\n", "    print(f\"{mode} set:\")\n", "    sample_list = build_sample_list(data_path, mode=mode)\n", "    n_diagnosis_dict = {\"normal\": 0, \"covid\": 0}\n", "    for sample in sample_list:\n", "        diagnosis = sample[\"diagnosis\"]\n", "        n_diagnosis_dict[diagnosis] += 1\n", "    print(f\"{len(sample_list)} images: {n_diagnosis_dict['normal']} normal / {n_diagnosis_dict['covid']} covid\")"]}, {"cell_type": "code", "execution_count": null, "id": "5651c246", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["n_samples = 20\n", "\n", "for sample_idx in range(n_samples):\n", "    visualize_sample(train_data[sample_idx])"]}, {"cell_type": "markdown", "id": "7048e861", "metadata": {"tags": ["teacher"]}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    <b>Answer:</b> There are half as many patients as normal participants. The distribution of the diagnoses is similar in the train, validation and test sets. We need to take this imbalance into account during the evaluation procedure, to avoid to overestimate the performance of the trained network.\n", "\n", "<p> Covid images are quite heterogeneous: some participants have medical devices, strange annotations may be added to the image... Moreover normal images are quite homogeneous, it may be easy to learn a rule to identify them based on the position of the participant / texture of the image.</p>\n", "</div>"]}, {"cell_type": "markdown", "id": "b3e4b363", "metadata": {}, "source": ["After this analysis you may want to add more transforms to your `CacheDataset` to perform data augmentation. Use `monai.transforms.Compose` to add all the transforms you want!"]}, {"cell_type": "markdown", "id": "5ac60250", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328\ufe0f <b>Exercise</b>: Complete the cell below and add additional transformations to your training data to perform data augmentation.\n", "</div>"]}, {"cell_type": "markdown", "id": "64d3264a", "metadata": {}, "source": ["<div class=\"alert alert-block alert-danger\">\n", "    <b>Resizing</b>: Your teachers already cropped or resized the images, so they all have the same size (512x512). At the end of the tutorial, this information is hard-coded in some functions, so if you change the size of the images now, TERRIBLE THINGS MAY HAPPEN LATER.\n", "</div>"]}, {"cell_type": "markdown", "id": "2d42d4a1", "metadata": {}, "source": ["You may change the batch size if you want. However, even though a larger batch size will accelerate the training process, at some point it may also [deteriorate the performance](https://twitter.com/ylecun/status/989610208497360896)."]}, {"cell_type": "code", "execution_count": null, "id": "e02fcb5a", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["train_transforms = monai.transforms.Compose(\n", "    [\n", "        LoadChestData(),\n", "        monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n", "        monai.transforms.RandFlipd(keys=['img'], prob=0.5, spatial_axis=1),\n", "        monai.transforms.RandGaussianNoised(keys=['img'], prob=0.5, mean=0.0, std=0.1)\n", "    ]\n", ")\n", "\n", "train_dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=\"train\"), transform=train_transforms)\n", "train_loader = monai.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n", "\n", "validation_dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=\"val\"), transform=LoadChestData())\n", "validation_loader = monai.data.DataLoader(validation_dataset, batch_size=16, shuffle=False)"]}, {"cell_type": "markdown", "id": "6fb18311", "metadata": {}, "source": ["### Train a classifier\n", "\n", "It is now time to train a classifier to perform a binary classification task: covid VS normal images.\n", "Check that you are working on a GPU by running the following cell:\n", "- if the device is \"cuda\" you are working on a GPU,\n", "- if the device is \"cpu\" call a teacher."]}, {"cell_type": "code", "execution_count": null, "id": "99a4593b", "metadata": {}, "outputs": [], "source": ["# Check whether we're using a GPU\n", "if torch.cuda.is_available():\n", "    n_gpus = torch.cuda.device_count()  # Total number of GPUs\n", "    gpu_idx = random.randint(0, n_gpus - 1)  # Random GPU index\n", "    device = torch.device(f'cuda:{gpu_idx}')\n", "    print('Using GPU: {}'.format(device))\n", "else:\n", "    device = torch.device('cpu')\n", "    print('GPU not found. Using CPU.')"]}, {"cell_type": "markdown", "id": "86f27043", "metadata": {}, "source": ["We will again use Weight & Biases to log all our results. As we want to log our transformation and because Weight & Biases only log simple objects, we provide again `from_compose_to_list` to write your transforms in your config file."]}, {"cell_type": "code", "execution_count": null, "id": "738898fd", "metadata": {}, "outputs": [], "source": ["import wandb\n", "\n", "wandb.login()"]}, {"cell_type": "code", "execution_count": null, "id": "5e777e29", "metadata": {}, "outputs": [], "source": ["def from_compose_to_list(transform_compose):\n", "    \"\"\"\n", "    Transform an object monai.transforms.Compose in a list fully describing the transform.\n", "    /!\\ Random seed is not saved, then reproducibility is not enabled.\n", "    \"\"\"\n", "    from copy import deepcopy\n", "        \n", "    if not isinstance(transform_compose, monai.transforms.Compose):\n", "        raise TypeError(\"transform_compose should be a monai.transforms.Compose object.\")\n", "    \n", "    output_list = list()\n", "    for transform in transform_compose.transforms:\n", "        kwargs = deepcopy(vars(transform))\n", "        \n", "        # Remove attributes which are not arguments\n", "        args = list(transform.__init__.__code__.co_varnames[1: transform.__init__.__code__.co_argcount])\n", "        for key, obj in vars(transform).items():\n", "            if key not in args:\n", "                del kwargs[key]\n", "\n", "        output_list.append({\"class\": transform.__class__, \"kwargs\": kwargs})\n", "    return output_list"]}, {"cell_type": "markdown", "id": "2916f955", "metadata": {}, "source": ["In this tutorial, we work with the `Classifier` of Monai. This network includes a series of convolutional layers and ends with a fully-connected layer computing two values:\n", "- the first value corresponds to the prediction for \"normal\",\n", "- the second value corresponds to the prediction for \"covid\".\n", "\n", "Then the final prediction of the network will correspond to the node with the highest value.\n", "\n", "<img src=\"https://i.imgur.com/5tqNSnm.png\" width=\"400\"/>"]}, {"cell_type": "code", "execution_count": null, "id": "57819396", "metadata": {}, "outputs": [], "source": ["model = monai.networks.nets.Classifier(\n", "    in_shape=train_dataset[0][\"img\"].shape,\n", "    classes=2,\n", "    channels=[16, 32, 64, 128, 128, 128],\n", "    strides=[2, 2, 2, 2, 2],\n", "    num_res_units=0\n", ").to(device)"]}, {"cell_type": "markdown", "id": "dd262916", "metadata": {}, "source": ["As the goal of this tutorial is to interpret a neural network, the training loop is already ready to be used. You can of course try to improve your results by changing your settings!"]}, {"cell_type": "code", "execution_count": null, "id": "b0b5e310", "metadata": {"lines_to_next_cell": 0}, "outputs": [], "source": ["from tqdm.notebook import tqdm\n", "\n", "# Set your parameters here\n", "learning_rate = 1e-4\n", "epochs = 20\n", "\n", "# Set the loss function and the optimizer\n", "loss_function = torch.nn.CrossEntropyLoss()\n", "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n", "\n", "run = wandb.init(\n", "    project='tutorial6_explainability',\n", "    name='covid_detection',\n", "    config={\n", "        'loss function': str(loss_function), \n", "        'lr': learning_rate,\n", "        'transform': from_compose_to_list(train_transforms),\n", "        'batch_size': train_loader.batch_size,\n", "        'epochs': epochs,\n", "        'n_conv': len(model.net)\n", "    }\n", ")\n", "\n", "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics later\n", "\n", "for epoch in tqdm(range(epochs)):\n", "    model.train()    \n", "    epoch_loss = 0\n", "    for batch_data in train_loader: \n", "        optimizer.zero_grad()\n", "        outputs = model(batch_data[\"img\"].to(device))\n", "        loss = loss_function(outputs, batch_data[\"label\"].to(device))\n", "        loss.backward()\n", "        optimizer.step()\n", "        epoch_loss += loss.item()\n", "    train_loss = epoch_loss / len(train_loader)\n", "    \n", "    val_loss = 0\n", "    for batch_data in validation_loader:\n", "        model.eval()\n", "        outputs = model(batch_data[\"img\"].to(device))\n", "        loss = loss_function(outputs, batch_data[\"label\"].to(device))\n", "        val_loss+= loss.item()\n", "    val_loss = val_loss / len(validation_loader)\n", "    \n", "    wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n", "\n", "# Log trained model in W&B\n", "torch.save(model.state_dict(), r'covid_classifier.pt')\n", "artifact = wandb.Artifact(name=f\"covid_classifier\", type=\"model\")\n", "artifact.add_file(r'covid_classifier.pt')\n", "wandb.log_artifact(artifact)\n", "remove(r'covid_classifier.pt')\n", "\n", "run.finish()"]}, {"cell_type": "markdown", "id": "621c5f56", "metadata": {}, "source": ["### Evaluate your trained classifier\n", "\n", "If you are satisfied with the performance of your model, you can now evaluate it (and log its performance in W&B).\n", "\n", "We provide two tools to evaluate the performance of the network on a data set:\n", "- `compute_prediction` will output a DataFrame with the individual prediction of the model on each image\n", "- `compute_confusion_matrix` computes the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), which allows to see how images belonging to each class were predicted."]}, {"cell_type": "code", "execution_count": null, "id": "a439a4ac", "metadata": {}, "outputs": [], "source": ["def compute_prediction(dataloader, model):\n", "    \"\"\"\n", "    Computes a DataFrame whose rows correspond to images in the data set wrapped by dataloader.\n", "    \n", "    Args:\n", "        dataloader: a DataLoader wrapping a DataSet.\n", "        model: a torch or monai network.\n", "        \n", "    Returns:\n", "        A pandas DataFrame with 4 columns:\n", "        - img_path (str): path to the image file,\n", "        - diagnosis (str): diagnosis (\"covid\" or \"normal\")\n", "        - label (int): ground truth label corresponding to the diagnosis (0 is \"normal\" and 1 is \"covid\")\n", "        - prediction (int): prediction of the network, corresponding to the node with the highest value.\n", "            Can be directly compared to \"label\".\n", "    \"\"\"\n", "    \n", "    model.eval()\n", "    results_df = pd.DataFrame(columns=[\"img_path\", \"diagnosis\", \"label\", \"prediction\"])\n", "    \n", "    for batch_dict in dataloader:\n", "        image_paths, diagnoses, labels = batch_dict[\"img_path\"], batch_dict[\"diagnosis\"], batch_dict[\"label\"]\n", "        images = batch_dict[\"img\"]\n", "        outputs = model(images.to(device))\n", "        prediction = torch.argmax(outputs.data, axis=1)\n", "        for idx in range(len(prediction)):\n", "            row = [image_paths[idx], diagnoses[idx], labels[idx].item(), prediction[idx].item()]\n", "            row_df = pd.DataFrame([row], columns=results_df.columns)\n", "            results_df = pd.concat([results_df, row_df])\n", "    \n", "    results_df.reset_index(inplace=True, drop=True)\n", "    return results_df\n", "\n", "\n", "def compute_confusion_matrix(dataloader, model):\n", "    \"\"\"\n", "    Computes the confusion matrix for the labels and predictions \"normal\" and \"covid\"\n", "    \n", "    Args:\n", "        dataloader (DataLoader): a DataLoader wrapping the evaluated data set.\n", "        model (Module): a torch or monai network.\n", "        \n", "    Returns:\n", "        (pd.DataFrame) the confusion matrix\n", "    \"\"\"\n", "    \n", "    prediction_df = compute_prediction(dataloader, model)\n", "    confusion_df = pd.DataFrame(index=[\"covid\", \"normal\"], columns=[\"covid\", \"normal\"])\n", "    confusion_df.loc[\"normal\", \"normal\"] = len(prediction_df[(prediction_df.label == 0) & (prediction_df.prediction == 0)])\n", "    confusion_df.loc[\"normal\", \"covid\"] = len(prediction_df[(prediction_df.label == 0) & (prediction_df.prediction == 1)])\n", "    confusion_df.loc[\"covid\", \"covid\"] = len(prediction_df[(prediction_df.label == 1) & (prediction_df.prediction == 1)])\n", "    confusion_df.loc[\"covid\", \"normal\"] = len(prediction_df[(prediction_df.label == 1) & (prediction_df.prediction == 0)])\n", "    return confusion_df"]}, {"cell_type": "markdown", "id": "06472ca1", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328\ufe0f <b>Exercise</b>: Complete the cell below to evaluate your network on the test set. Log your confusion matrix with <code>log_confusion_matrix</code>.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "e0faff1c", "metadata": {}, "outputs": [], "source": ["def log_confusion_matrix(run_id, confusion_df, mode=\"test\"):\n", "    \"\"\"\n", "    Saves the values of a confusion matrix to W&B interface.\n", "    \n", "    Args:\n", "        run_id (str): ID of the run you want to log to.\n", "            In the training cell, this value was assigned to the variable \"run_id\".\n", "            You can also retrieve it in the log of the cell or on the W&B interface.\n", "        confusion_df (pd.DataFrame): output of `compute_confusion_matrix`.\n", "        mode (str): name of the subset used to compute `confusion_df`.\n", "            May correspond to \"train\", \"validation\" or \"test\".\n", "    \"\"\"\n", "    print(f\"Logging the results on {mode} set of run {run_id}\")\n", "    api = wandb.Api()\n", "    run = api.run(f\"tutorial6_explainability/{run_id}\")\n", "    run.summary[f\"{mode}_TP\"] = confusion_df.loc[\"covid\", \"covid\"]\n", "    run.summary[f\"{mode}_FP\"] = confusion_df.loc[\"normal\", \"covid\"]\n", "    run.summary[f\"{mode}_TN\"] = confusion_df.loc[\"normal\", \"normal\"]\n", "    run.summary[f\"{mode}_FN\"] = confusion_df.loc[\"covid\", \"normal\"]\n", "    run.save()"]}, {"cell_type": "code", "execution_count": null, "id": "535a9ac2", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["mode = \"test\"\n", "dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=mode), transform=LoadChestData())\n", "dataloader = monai.data.DataLoader(dataset, batch_size=16, shuffle=False)\n", "\n", "confusion_df = compute_confusion_matrix(dataloader, model)\n", "print(confusion_df)\n", "log_confusion_matrix(run_id, confusion_df, mode=mode)"]}, {"cell_type": "markdown", "id": "0a5a7fe2", "metadata": {}, "source": ["### Explain your trained classifier\n", "\n", "You may have obtained very good results with your trained classifier on test data, i.e., data that was never seen before by the classifier during training. That's the first step to validate your network!\n", "\n", "In this section we are now going to check which parts of the image the network focuses on to compute its prediction. In this tutorial, we will use the algorithm described in [(Selvaraju et al., 2017)](https://ieeexplore.ieee.org/document/8237336): Grad-CAM."]}, {"cell_type": "markdown", "id": "69ec7cfc", "metadata": {}, "source": ["#### Grad-CAM algorithm\n", "\n", "The Grad-CAM map is the weighted sum of the feature maps produced by the last convolutional layer of your network:\n", "\n", "1. The feature maps of the last convolutional layer are computed during a forward pass\n", "\n", "<img src=\"https://i.imgur.com/zDnqG3H.png\" width=\"500\"/>\n", "\n", "2. Gradients at the level of the last convolutional layer are computed. This operation actually computes an output of the same size as the feature maps (in our example 128x16x16). Then gradients are pooled to only obtain one value per feature map (in our example 128 scalars).\n", "\n", "\n", "<img src=\"https://i.imgur.com/N4Pdqxu.png\" width=\"500\"/>\n", "\n", "3. Grad-CAM is the sum of the feature maps multiplied by their respective pooled gradients. This results in one low-resolution map (in our case 16x16)\n", "\n", "<img src=\"https://i.imgur.com/oLXsouS.png\" width=\"550\"/>\n", "\n", "4. The map is upsampled to the size of the input image (in our case 512x512).\n", "\n", "<img src=\"https://i.imgur.com/jkW7tr7.png\" width=\"550\"/>\n", "\n", "Because of the upsampling step, Grad-CAM maps look very smooth, which is why they are very appreciated in the community. But be aware that actually this map, even if it was resized, has a low spatial resolution. This is why you should be careful when choosing your architecture when you want to use Grad-CAM: if the resolution of the feature maps you want to use is too low (for example 4x4), in the end you won't be able to see anything."]}, {"cell_type": "code", "execution_count": null, "id": "8ad7c6a8", "metadata": {}, "outputs": [], "source": ["class GradCam:\n", "    \"\"\"\n", "    Produces Grad-CAM to a monai.networks.nets.Classifier\n", "    \"\"\"\n", "    def __init__(self, model):\n", "        self.model = model\n", "        self.model.eval()\n", "        self.device = next(model.parameters()).device\n", "\n", "    def generate_gradients(self, input_batch, target_class=None):\n", "        \"\"\"\n", "        Generate the gradients map corresponding to the input_tensor.\n", "        \n", "        Args:\n", "            input_tensor (Tensor): tensor representing a batch of images.\n", "            target_class (int): allows to choose from which node the gradients are back-propagated.\n", "                Default will back-propagate from the node corresponding to the true class of the image.\n", "            \n", "        Returns:\n", "            (Tensor): the gradients map\n", "        \"\"\"\n", "        input_tensor = input_batch[\"img\"].to(self.device)        \n", "        # Dissect model\n", "        conv_part = self.model.net\n", "        final_part = self.model.final\n", "        \n", "        # Get last conv feature map\n", "        feature_maps = conv_part(input_tensor).detach()\n", "        feature_maps.requires_grad = True\n", "        model_output = final_part(feature_maps)\n", "        # Target for backprop\n", "        one_hot_output = torch.zeros_like(model_output)\n", "        if target_class is not None:\n", "            one_hot_output[:, target_class] = 1\n", "        else:\n", "            labels = input_batch[\"label\"]\n", "            for i, target_class in enumerate(labels):\n", "                one_hot_output[i, target_class] = 1\n", "        one_hot_output = one_hot_output.to(self.device)\n", "        # Backward pass\n", "        model_output.backward(gradient=one_hot_output)\n", "        # Convert Pytorch variable to numpy array\n", "        gradients = feature_maps.grad\n", "        pooled_gradients = torch.mean(gradients, dim=[2, 3]).unsqueeze(2).unsqueeze(3)\n", "        \n", "        # Weight feature maps according to pooled gradients\n", "        feature_maps.requires_grad = False\n", "        feature_maps *= pooled_gradients\n", "        # Take the mean of all weighted feature maps\n", "        grad_cam = torch.mean(feature_maps, dim=1).cpu()\n", "        resize_transform = monai.transforms.Resize(input_tensor.shape[-2::], mode=\"bilinear\")\n", "        \n", "        return resize_transform(grad_cam).unsqueeze(1)"]}, {"cell_type": "markdown", "id": "a01454cc", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328\ufe0f <b>Exercise</b>: Visualize Grad-CAM maps obtained on the test set with the function <code>visualize_grad_cam</code>.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "2bc4fc8e", "metadata": {}, "outputs": [], "source": ["def visualize_grad_cam(batch_dict, model, target_class=None, v_display=None):\n", "    \"\"\"\n", "    Plots chest X-rays images with their corresponding grad-CAM maps.\n", "    \n", "    Args:\n", "        batch_dict (dict): batch of samples produced by a DataLoader.\n", "        model (Classifier): a monai Classifier with two output classes.\n", "        target_class (int): allows to choose from which node the gradients are back-propagated.\n", "            Default will back-propagate from the node corresponding to the true class of the image.\n", "        v_display (float): changes the scale of the gradient maps.\n", "    \"\"\"\n", "    import matplotlib.pyplot as plt\n", "    \n", "    gradients_transform = GradCam(model)\n", "    gradients = gradients_transform.generate_gradients(batch_dict, target_class)\n", "    outputs = model(batch_dict[\"img\"].to(device))\n", "    prediction = torch.argmax(outputs.data, axis=1)\n", "    for i in range(len(gradients)):\n", "        plt.imshow(batch_dict[\"img\"][i, 0], cmap=\"gray\")\n", "        if v_display is None:\n", "            v = max(-gradients.min(), gradients.max())\n", "        else:\n", "            v = v_display\n", "        plt.imshow(gradients[i, 0], alpha=0.5, vmin=-v, vmax=v, cmap=\"bwr\")\n", "        plt.title(f\"Label={sample['label'][i]}, prediction={prediction[i]}\")\n", "        plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "0db8b9ec", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["test_dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=\"test\"), transform=LoadChestData())\n", "test_loader = monai.data.DataLoader(test_dataset, batch_size=16)\n", "for sample in test_loader:\n", "    visualize_grad_cam(sample, model)"]}, {"cell_type": "markdown", "id": "5880e76b", "metadata": {}, "source": ["> \u2753 What parts of the images is the network mostly using to complete its task? Is it clinically relevant? Why did this happen?"]}, {"cell_type": "markdown", "id": "cec45f87", "metadata": {"tags": ["teacher"]}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    <b>Answer:</b> as the training process is not deterministic, the interpretability maps differ from run to run. However, we can assess that for many images, the network relies on patterns that are not clinically relevant such as the position of the shoulders, the presence of the letters 'L' and 'R' in the image...\n", "\n", "<p>The network learnt a shortcut (a bias in our data set) instead of relevant features (for example in the lungs). This could happen as we worked on a biased data set: normal participants and covid patients belonged to different data sets in the first place!</p>\n", "</div> "]}, {"cell_type": "markdown", "id": "879be97d", "metadata": {}, "source": ["### Experimental confirmation of learnt shortcuts\n", "\n", "In this last section, the goal is to fool a network by generating data to artificially transform a \"normal\" image in a \"covid\" image for the network.\n", "\n", "These images are generated according to the existing images of your data set, which are transformed according to custom transforms, which were manually made by your teachers to reproduce biases that were possibly learnt by your network in your data set.\n", "\n", "<div class=\"alert alert-block alert-danger\">\n", "    <b>Resizing</b>: If you changed the size of the images, TERRIBLE THINGS WILL HAPPEN NOW.\n", "</div>\n", "\n", "The first two classes, `MaskingTransform` and `CropResizeTransform` are not meant to be used directly. They are utilities used to create other transforms in a cell below."]}, {"cell_type": "code", "execution_count": null, "id": "0b13f5c1", "metadata": {}, "outputs": [], "source": ["from copy import copy\n", "\n", "class MaskingTransform(monai.transforms.Transform):\n", "    \"\"\"This transform applies a binary mask of the same size as the image it is applied to.\"\"\"\n", "    def __init__(self, mask_pt, label=None, value=0):\n", "        \"\"\"\n", "        Args:\n", "            mask_pt (Tensor): a binary mask that will be applied to occlude an image.\n", "            label (int): if given, transform will only be performed on images with the given label.\n", "                Default will transform all images.\n", "            value (float): constant value used to perturb the image.\n", "        \"\"\"\n", "        self.label = label\n", "        self.value = value\n", "        self.mask_pt = mask_pt.float()\n", "        self.invert_mask_pt = self.invert_mask(self.mask_pt)\n", "        \n", "    def __call__(self, sample):\n", "        sample = copy(sample)\n", "        \n", "        if self.label is None or self.label == sample[\"label\"]:\n", "            image = sample[\"img\"] * self.invert_mask_pt + self.mask_pt * self.value\n", "            sample[\"img\"] = image\n", "\n", "        return sample\n", "    \n", "    @staticmethod\n", "    def invert_mask(pt):\n", "        negative_image = -pt + 1\n", "        return (negative_image - negative_image.min()) / (negative_image.max() - negative_image.min())\n", "\n", "    \n", "class CropResizeTransform(monai.transforms.Transform):\n", "    \"\"\"This transform crop a region of interest and resize the image to its initial size.\"\"\"\n", "    def __init__(self, roi_center, roi_size, label=None):\n", "        \"\"\"\n", "        Args:\n", "            roi_center (Tuple[int, int]): coordinates of the center of the region of interest.\n", "            roi_size (Tuple[int, int]): size of the region of interest.\n", "            label (int): if given, transform will only be performed on images with the given label.\n", "                Default will transform all images.\n", "        \"\"\"\n", "        self.label = label\n", "        self.crop_transform = monai.transforms.SpatialCrop(roi_center=roi_center, roi_size=roi_size)\n", "        self.resize_transform = monai.transforms.Resize((512, 512), mode=\"bilinear\")\n", "        \n", "    def __call__(self, sample):        \n", "        sample = copy(sample)\n", "        \n", "        if self.label is None or self.label == sample[\"label\"]:\n", "            image = self.resize_transform(self.crop_transform(sample[\"img\"]))\n", "            sample[\"img\"] = image\n", "\n", "        return sample"]}, {"cell_type": "markdown", "id": "dfc3a5aa", "metadata": {}, "source": ["In the cell below we provide 5 transforms which can be easily applied to your samples.\n", "\n", "Use the key `label` if you want to apply them to one label only (0 or 1) in your data set!"]}, {"cell_type": "code", "execution_count": null, "id": "a9344519", "metadata": {}, "outputs": [], "source": ["class RemoveShoulders(MaskingTransform):\n", "    def __init__(self, label=None):\n", "        mask_pt = torch.zeros((1, 512, 512))\n", "        mask_pt[:, :100, :200] = 1\n", "        mask_pt[:, :100, 312:] = 1\n", "        super().__init__(mask_pt, label)\n", "    \n", "\n", "class AddSideBackground(MaskingTransform):\n", "    def __init__(self, label=None):\n", "        mask_pt = torch.zeros((1, 512, 512))\n", "        mask_pt[:, 150:, :50] = 1\n", "        mask_pt[:, 150:, 512 - 50:] = 1\n", "        super().__init__(mask_pt, label)\n", "    \n", "    \n", "class CropSideBackground(CropResizeTransform):\n", "    def __init__(self, label=None):\n", "        roi_center = (256, 256)\n", "        roi_size = (512, 412)\n", "        super().__init__(roi_center, roi_size, label)\n", "    \n", "\n", "class CropShouldersUp(CropResizeTransform):\n", "    def __init__(self, label=None):\n", "        roi_center = (312, 256)\n", "        roi_size = (412, 512)\n", "        super().__init__(roi_center, roi_size, label)\n", "\n", "    \n", "class RWriter(MaskingTransform):\n", "    def __init__(self, label=None, upsampling=3):\n", "        from PIL import Image, ImageDraw, ImageFont\n", "        \n", "        size = 512\n", "        h_offset = 120\n", "        v_offset = 60\n", "\n", "        # Create black image with white R letter\n", "        image = Image.new(\"L\", (size // upsampling, size // upsampling)) # As the size of the font cannot be easily chosen, a smaller image is created and the resizing will increase the size of the font\n", "        draw = ImageDraw.Draw(image)\n", "        draw.text((h_offset // upsampling, v_offset // upsampling), \"R\", fill=\"white\")\n", "        image = image.resize((size, size))\n", "        \n", "        # Convert to Tensor\n", "        mask_pt = torch.from_numpy(np.asarray(image)).float() / 255\n", "        super().__init__(mask_pt, label, value=1)"]}, {"cell_type": "markdown", "id": "00d80d30", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328\ufe0f <b>Exercise</b>: Visualize the images produced by each transform on a sample to understand what they do.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "5f659070", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["sample = train_dataset[0]\n", "\n", "transforms = [RemoveShoulders, AddSideBackground, CropSideBackground, CropShouldersUp, RWriter]\n", "\n", "print(\"Original image\")\n", "visualize_sample(sample)\n", "\n", "for transform in transforms:\n", "    print(f\"Transform {transform.__name__}\")\n", "    transformed_sample = transform()(sample)\n", "    visualize_sample(transformed_sample)"]}, {"cell_type": "markdown", "id": "6566ed65", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328\ufe0f <b>Exercise</b>: Compute and compare the confusion matrices obtained with or without transforms of your choice on the test images,.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "07d296f8", "metadata": {"tags": ["teacher"]}, "outputs": [], "source": ["test_dataset = monai.data.CacheDataset(\n", "    build_sample_list(data_path, mode=\"test\"), \n", "    transform=monai.transforms.Compose(\n", "        [LoadChestData()]\n", "    )\n", ")\n", "test_loader = monai.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n", "confusion_df = compute_confusion_matrix(test_loader, model)\n", "print(\"Original confusion matrix\")\n", "print(confusion_df)\n", "\n", "test_dataset = monai.data.CacheDataset(\n", "    build_sample_list(data_path, mode=\"test\"), \n", "    transform=monai.transforms.Compose(\n", "        [LoadChestData(), AddSideBackground(), RemoveShoulders(), RWriter()]\n", "    )\n", ")\n", "test_loader = monai.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n", "confusion_df = compute_confusion_matrix(test_loader, model)\n", "print(\"Post-transforms confusion matrix\")\n", "print(confusion_df)"]}, {"cell_type": "markdown", "id": "64264cff", "metadata": {}, "source": ["> \u2753 Does this experiment highlight the same biases you found with Grad-CAM? What are the limitations of this procedure? Are some transforms more prone to limitations than others?"]}, {"cell_type": "markdown", "id": "151b8c86", "metadata": {"tags": ["teacher"]}, "source": ["<div class=\"alert alert-block alert-info\">\n", "    <b>Answer:</b> This procedure was also performed by <a href=\"https://www.nature.com/articles/s42256-021-00338-7\">(DeGrave et al., 2021)</a>. \n", "    <p>The main problem of this procedure is its use of perturbations to generate synthetic data. If generated data are unrealistic, they do not belong to the original training distribution anymore, and it is not possible to know if the network was mistaken because the perturbation targeted an important part of the image or just because it cannot handle the image anymore.</p>\n", "    <p>In this tutorial, transforms deriving from <code>MaskingTransform</code> may lead to more unrealistic images than the ones deriving from <code>CropResizeTransform</code>.</p>\n", "    <ul>\n", "     <li><code>MaskingTransform</code> objects occlude images with black rectangles or add the letter R (written in a font which does not belong to the original training distribution).</li>\n", "     <li><code>CropResizeTransform</code> objects only crop and resize images, which are transformations that were already encountered while preprocessing this data set.</li>\n", "    </ul>\n", "</div> "]}, {"cell_type": "markdown", "id": "975c7574", "metadata": {}, "source": ["### Take-home messages\n", "\n", "1. Data curation and data analysis prior to training is not fun, but it is absolutely essential. Knowing your data set and its biases will avoid bad surprises after months choosing hyperparameters and training hundreds of networks...\n", "2. We always need more data for deep learning. However, mixing different sources to increase the size of your data set may not always be a good idea. You should always assess if data sets are compatible, or even better, use only one for training and the other for testing to assess the generalizability to other cohorts!\n", "3. Be fair and explain precisely your limitations. Modest but honest results are better than breakthroughs relying on lies."]}, {"cell_type": "markdown", "id": "dbf7de20", "metadata": {}, "source": ["## Part 2 - Equivariance\n", "In this part, we are going to look at the equivariance properties of a neural network architecture that you should by now be very familiar with: the U-Net. We will use the same problem as in **Tutorial 3**: chest X-ray segmentation. Because training a network is not the focus here, we have pretrained a network that you can use for these experiments.\n", "First, set the data path as before and have it point to where you have stored the data for Tutorial 3. If you don't have that data anymore, you can download it, unzip it, and set the data path as follows."]}, {"cell_type": "code", "execution_count": null, "id": "b3c4baf1", "metadata": {}, "outputs": [], "source": ["!wget https://surfdrive.surf.nl/files/index.php/s/Y4psc2pQnfkJuoT/download -O Tutorial_3.zip\n", "!unzip -qo Tutorial_3.zip\n", "data_path = \"ribs\""]}, {"cell_type": "markdown", "id": "cdb1dd47", "metadata": {}, "source": ["### Data loading\n", "Next, we will use the same utility functions as in Tutorial 3 to build a dictionary of files and load rib data."]}, {"cell_type": "code", "execution_count": null, "id": "efcd13c4", "metadata": {}, "outputs": [], "source": ["import os\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import glob\n", "import monai\n", "from PIL import Image\n", "import torch\n", "\n", "def build_dict_ribs(data_path, mode='train'):\n", "    \"\"\"\n", "    This function returns a list of dictionaries, each dictionary containing the keys 'img' and 'mask' \n", "    that returns the path to the corresponding image.\n", "    \n", "    Args:\n", "        data_path (str): path to the root folder of the data set.\n", "        mode (str): subset used. Must correspond to 'train', 'val' or 'test'.\n", "        \n", "    Returns:\n", "        (List[Dict[str, str]]) list of the dictionaries containing the paths of X-ray images and masks.\n", "    \"\"\"\n", "    # test if mode is correct\n", "    if mode not in [\"train\", \"val\", \"test\"]:\n", "        raise ValueError(f\"Please choose a mode in ['train', 'val', 'test']. Current mode is {mode}.\")\n", "    \n", "    # define empty dictionary\n", "    dicts = []\n", "    # list all .png files in directory, including the path\n", "    paths_xray = glob.glob(os.path.join(data_path, mode, 'img', '*.png'))\n", "    # make a corresponding list for all the mask files\n", "    for xray_path in paths_xray:\n", "        if mode == 'test':\n", "            suffix = 'val'\n", "        else:\n", "            suffix = mode\n", "        # find the binary mask that belongs to the original image, based on indexing in the filename\n", "        image_index = os.path.split(xray_path)[1].split('_')[-1].split('.')[0]\n", "        # define path to mask file based on this index and add to list of mask paths\n", "        mask_path = os.path.join(data_path, mode, 'mask', f'VinDr_RibCXR_{suffix}_{image_index}.png')\n", "        if os.path.exists(mask_path):\n", "            dicts.append({'img': xray_path, 'mask': mask_path})\n", "    return dicts\n", "\n", "class LoadRibData(monai.transforms.Transform):\n", "    \"\"\"\n", "    This custom Monai transform loads the data from the rib segmentation dataset.\n", "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n", "    \"\"\"\n", "    def __init__(self, keys=None):\n", "        pass\n", "\n", "    def __call__(self, sample):\n", "        image = Image.open(sample['img']).convert('L') # import as grayscale image\n", "        image = np.array(image, dtype=np.uint8)\n", "        mask = Image.open(sample['mask']).convert('L') # import as grayscale image\n", "        mask = np.array(mask, dtype=np.uint8)\n", "        # mask has value 255 on rib pixels. Convert to binary array\n", "        mask[np.where(mask==255)] = 1\n", "        return {'img': image, 'mask': mask, 'img_meta_dict': {'affine': np.eye(2)}, \n", "                'mask_meta_dict': {'affine': np.eye(2)}}"]}, {"cell_type": "markdown", "id": "d3a26fbf", "metadata": {}, "source": ["Use the cell below to make a validation loader with a single image. This is sufficient for the small experiment that you will perform."]}, {"cell_type": "code", "execution_count": null, "id": "d6e3f3c4", "metadata": {}, "outputs": [], "source": ["validation_dict_list = build_dict_ribs(data_path, mode='val')\n", "validation_transform = monai.transforms.Compose(\n", "    [\n", "        LoadRibData(),\n", "        monai.transforms.AddChanneld(keys=['img', 'mask']),\n", "        monai.transforms.HistogramNormalized(keys=['img']),     \n", "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0, maxv=1),\n", "        monai.transforms.Zoomd(keys=['img', 'mask'], zoom=0.25, mode=['bilinear', 'nearest'], keep_size=False),\n", "        # monai.transforms.RandSpatialCropd(keys=['img', 'mask'], roi_size=[384, 384], random_size=False)\n", "        monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_center=[300, 300], roi_size=[384 + 64, 384])        \n", "    ]\n", ")\n", "validation_data = monai.data.CacheDataset([validation_dict_list[3]], transform=validation_transform)\n", "validation_loader = monai.data.DataLoader(validation_data, batch_size=1, shuffle=False)"]}, {"cell_type": "markdown", "id": "db3278a5", "metadata": {}, "source": ["### Loading a pretrained model\n", "We have already trained a model for you, the parameters of which were shared in JupyterLab as well.\n", "**Note**: if you downloaded the data set yourself, the model should be in the same folder as the images.\n", "If you already downloaded the data set but not the model, the model file is available [here](https://surfdrive.surf.nl/files/index.php/s/GS0fWjGT2cOjMOQ)."]}, {"cell_type": "code", "execution_count": null, "id": "56679421", "metadata": {}, "outputs": [], "source": ["pretrained_file = path.join(data_path, \"trainedUNet.pt\")"]}, {"cell_type": "markdown", "id": "a95e339e", "metadata": {}, "source": ["Next, we initialize a standard U-Net architecture and load the parameters of the pretrained network using the <code>load_state_dict</code> function."]}, {"cell_type": "code", "execution_count": null, "id": "b3e79f3e", "metadata": {}, "outputs": [], "source": ["import torch\n", "import monai\n", "\n", "# Check whether we're using a GPU\n", "if torch.cuda.is_available():\n", "    n_gpus = torch.cuda.device_count()  # Total number of GPUs\n", "    gpu_idx = random.randint(0, n_gpus - 1)  # Random GPU index\n", "    device = torch.device(f'cuda:{gpu_idx}')\n", "    print('Using GPU: {}'.format(device))\n", "else:\n", "    device = torch.device('cpu')\n", "    print('GPU not found. Using CPU.')\n", "\n", "model = monai.networks.nets.UNet(\n", "    spatial_dims=2,\n", "    in_channels=1,\n", "    out_channels=1,\n", "    channels = (8, 16, 32, 64, 128),\n", "    strides=(2, 2, 2, 2),\n", "    num_res_units=2,\n", "    dropout=0.5\n", ").to(device)\n", "\n", "model.load_state_dict(torch.load(pretrained_file))\n", "model.eval()"]}, {"cell_type": "markdown", "id": "d9abfe57", "metadata": {}, "source": ["Let's use the pretrained network to segment (part of) our image. Run the cell below."]}, {"cell_type": "code", "execution_count": null, "id": "0665f434", "metadata": {}, "outputs": [], "source": ["for sample in validation_loader:\n", "\n", "    img = sample['img'][:, :, :384, :384]    \n", "    mask = sample['mask'][:, :, :384, :384]\n", "    output_noshift = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()   \n", "    \n", "    fig, ax = plt.subplots(1,2, figsize = [12, 10])    \n", "    # Plot X-ray image\n", "    ax[0].imshow(img.squeeze(), 'gray')\n", "    # Plot ground truth\n", "    mask = np.squeeze(mask)\n", "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n", "    ax[0].imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n", "    ax[0].set_title('Ground truth')\n", "    # Plot output\n", "    overlay_output = np.ma.masked_where(output_noshift < 0.1, output_noshift > 0.99)\n", "    ax[1].imshow(img.squeeze(), 'gray')\n", "    ax[1].imshow(overlay_output.squeeze(), 'Reds', alpha = 0.7, clim=[0,1])\n", "    ax[1].set_title('Prediction')\n", "    plt.show()      "]}, {"cell_type": "markdown", "id": "4a5678f4", "metadata": {}, "source": ["As you can see, segmentation isn't perfect, but that's also not the goal of this exercise. What we are going to look into is the translation equivariance (**Lecture 8**) of the U-Net. That is: if you translate the image by $d$ pixels, does the output also simply change by $d$ pixels. Note that this is a nice feature to have for a segmentation network: in principle we'd want our network to give us the same label for a pixel regardless of where the image was cut. The image below visualizes this principle. For segmentation of the pixels in the orange square, it shouldn't matter if we provide the red square or the green square as input to the U-Net.\n", "\n", "<img src='https://i.imgur.com/ujJq2Be.png' width='400px'></img>\n", "\n", "\n", "\n", "> \u2753 What do you think will happen to the U-Net's prediction if we give it a slightly shifted version of the image as input?"]}, {"cell_type": "markdown", "id": "07b05097", "metadata": {}, "source": ["Now we make a small script that performs the above experiment. First, we obtain the segmentation in the red box and we call this <code>output_noshift</code>. Then we shift the green box by an offset and each time obtain a segmentation in this box using the same model. We start small with a shift/offset of just a **single pixel**.\n", "> \u2753 Run the cell below and observe the outputs. Can you spot differences between the two segmentation masks?"]}, {"cell_type": "code", "execution_count": null, "id": "e5388dc8", "metadata": {}, "outputs": [], "source": ["offset = 1\n", "\n", "for sample in validation_loader:\n", "\n", "    # Original image\n", "    img = sample['img'][:, :, :384, :384]    \n", "    mask = sample['mask'][:, :, :384, :384]\n", "    output_noshift = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()   \n", "\n", "    # Plot X-ray image\n", "    fig, ax = plt.subplots(1,2, figsize = [12, 10])    \n", "    ax[0].imshow(img.squeeze(), 'gray')\n", "    # Plot ground truth\n", "    mask = np.squeeze(mask)\n", "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n", "    ax[0].imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n", "    ax[0].set_title('Ground truth')\n", "    # Plot output\n", "    overlay_output = np.ma.masked_where(output_noshift < 0.1, output_noshift >0.99)\n", "    ax[1].imshow(img.squeeze(), 'gray')\n", "    ax[1].imshow(overlay_output.squeeze(), 'Reds', alpha = 0.7, clim=[0,1])\n", "    ax[1].set_title('Prediction')\n", "    plt.show()\n", "    \n", "    # Shifted image\n", "    img = sample['img'][:, :, offset:offset+384, :384]\n", "    mask = sample['mask'][:, :, offset:offset+384, :384]\n", "    output = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()\n", "\n", "    # Plot X-ray image\n", "    fig, ax = plt.subplots(1,2, figsize = [12, 10])\n", "    ax[0].imshow(img.squeeze(), 'gray')\n", "    # Plot ground truth\n", "    mask = np.squeeze(mask)\n", "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n", "    ax[0].imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n", "    ax[0].set_title('Ground truth shifted')\n", "    # Plot output\n", "    overlay_output = np.ma.masked_where(output < 0.1, output >0.99)\n", "    ax[1].imshow(img.squeeze(), 'gray')\n", "    ax[1].imshow(overlay_output.squeeze(), 'Reds', alpha = 0.7, clim=[0,1])\n", "    ax[1].set_title('Prediction shifted')\n", "    plt.show()"]}, {"cell_type": "markdown", "id": "a01a3b56", "metadata": {}, "source": ["To highlight the differences between both segmentation masks a bit more, we make a difference image. We correct for the shift applied so that we're not comparing apples and oranges. The next cell shows the difference image between the original image and what we get when we process an image that is shifted by one pixel.\n", "> \u2753 Given these results, is a U-Net translation equivariant, invariant, or neither?"]}, {"cell_type": "code", "execution_count": null, "id": "89e76055", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(6, 6))\n", "diffout = output_noshift[offset:, :384] - output[:-offset, :384]\n", "plt.imshow(diffout, cmap='seismic', clim=[-1, 1])\n", "plt.title('Offset {}'.format(offset))\n", "plt.colorbar()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "93c7c648", "metadata": {}, "source": ["We can repeat this for larger offsets. Let's take offsets up to 64 pixels, and each time compute the difference between the original and shifted image, in a subimage that should be unaffected by the shift. We store the L1 norm of the difference image in an array <code>norms</code> and plot these as a function of offset.\n", "> \u2753 The resulting plot shows that the U-Net is equivariant for none of the translations. This is due to a combination of border effects and downsampling layers. However, the plot also shows a particular pattern, in which the norm *dips* every 16 pixels of offset. Can you explain this based on the U-Net architecture? "]}, {"cell_type": "code", "execution_count": null, "id": "9972cdde", "metadata": {"lines_to_next_cell": 0}, "outputs": [], "source": ["norms = []\n", "offsets = []\n", "plot_differences = False  # Set to True to plot difference images for every offset\n", "\n", "img = sample['img'][:, :, :384, :384]    \n", "mask = sample['mask'][:, :, :384, :384]\n", "output_noshift = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()   \n", "\n", "for offset in range(1, 65):\n", "    for sample in validation_loader:\n", "        img = sample['img'][:, :, offset:offset+384, :384]\n", "        mask = sample['mask'][:, :, offset:offset+384, :384]\n", "\n", "        output = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()  \n", "\n", "        diffout = (output_noshift[offset:, :384] - output[:-offset, :384])[100:284, 100:284]\n", "        offsets.append(offset)\n", "        norms.append(np.sum(np.abs(diffout)))\n", "        if plot_differences:\n", "            plt.figure()\n", "            plt.imshow(diffout, cmap='seismic', clim=[-1, 1])\n", "            plt.title(f\"Offset {offset}\")\n", "            plt.colorbar()\n", "            plt.show()\n", "\n", "plt.figure()\n", "plt.plot(offsets, norms)\n", "plt.xlabel('Offset')\n", "plt.ylabel('Difference')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "66ce5089", "metadata": {}, "source": ["## **This is the end of this course!**"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}