{"cells": [{"cell_type": "markdown", "id": "7c6282d5", "metadata": {}, "source": ["# Tutorial 4\n", "## May 24, 2023\n", "In the previous tutorials, you have familiarized yourself with PyTorch, MONAI, and Weights & Biases. In last week's lectures 4 and 5, you have heard about image reconstruction and image registration with (convolutional) neural networks. This week, you again get the chance to put what you have learned into practice. The tutorial consists of two parts. First, you will develop, train, and evaluate a CNN for denoising of (synthetic) CT images. Second, you will develop, train, and evaluate a CNN that learns to perform deformable image registration in the chest X-ray images that we have also used in the second tutorial. Along the way, there will be questions (\u2753) and <b style='background-color:rgba(80,255,80,0.4); padding:2px'>\n", "exercises.</b>"]}, {"cell_type": "markdown", "id": "5f0b3614", "metadata": {}, "source": ["First, let's take care of the necessities:\n", "- If you're using Google Colab, make sure to select a GPU Runtime.\n", "- Connect to Weights & Biases using the code below.\n", "- Install a few libraries that we will use in this tutorial."]}, {"cell_type": "code", "execution_count": null, "id": "a55ba6d1", "metadata": {}, "outputs": [], "source": ["import os\n", "import wandb\n", "\n", "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n", "wandb.login()"]}, {"cell_type": "code", "execution_count": null, "id": "99102ddd", "metadata": {}, "outputs": [], "source": ["!pip install dival\n", "!pip install kornia\n", "!pip install monai"]}, {"cell_type": "markdown", "id": "8b9cccbb", "metadata": {}, "source": ["## Part 1: Reconstruction\n", "In the first part of this tutorial, you will reconstruct CT images. To not use too much disk storage, we will synthetise images on the fly using the Deep Inversion Validation Library [(dival)](https://github.com/jleuschn/dival). These are 2D images with $128\\times 128$ pixels that contain a random number of ellipses with random sizes and random intensities. \n", "\n", "First, make a dataset of ellipses. This will make an object that we can call for images using a generator. Next, we take a look at what this dataset contains. We will use the <code>generator</code> to ask for a sample. Each sample contains a sinogram and a ground truth (original) synthetic image that we can visualize. You may recall from the lecture that the sinogram is made up of integrals along projections. The horizontal axis in the sinogram corresponds to the location $s$ along the detector, the vertical axis to the projection angle $\\theta$.\n", "\n", "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/Tomographic_fig1.png\" width=\"400px\"></img>"]}, {"cell_type": "code", "execution_count": null, "id": "6dd21534", "metadata": {}, "outputs": [], "source": ["import dival\n", "\n", "dataset = dival.get_standard_dataset('ellipses', impl='skimage')\n", "dat_gen = dataset.generator(part='train')"]}, {"cell_type": "markdown", "id": "83f74a46", "metadata": {}, "source": ["Run the cell below to show a sinogram and image in the dataset."]}, {"cell_type": "code", "execution_count": null, "id": "228aaca9", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# Get a sample from the generator\n", "sinogram, ground_truth = next(dat_gen)\n", "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n", "\n", "# Show the sinogram\n", "axs[0].imshow(sinogram, cmap='gray', extent=[0, 183, -90, 90])\n", "axs[0].set_title('Sinogram')\n", "axs[0].set_xlabel('$s$')\n", "axs[0].set_ylabel('$\\Theta$')\n", "\n", "# Show the ground truth image\n", "axs[1].imshow(ground_truth, cmap='gray')\n", "axs[1].set_title('Ground truth')\n", "axs[1].set_xlabel('$x$')\n", "axs[1].set_ylabel('$y$')\n", "plt.show()   "]}, {"cell_type": "markdown", "id": "f45c1432", "metadata": {}, "source": ["> \u2753 What kind of CT reconstruction problem is this? Limited-view or sparse-angle CT? Why?"]}, {"cell_type": "markdown", "id": "7a7cb467", "metadata": {"tags": ["student"]}, "source": ["Answer:"]}, {"cell_type": "markdown", "id": "cc4c0d0f", "metadata": {}, "source": ["Not only does the sinogram contain few angles, it also contains added white noise. If we simply backproject the sinogram to the image domain we end up with a low-quality image. Let's give it a try using the standard [Filtered Backprojection](https://en.wikipedia.org/wiki/Radon_transform#Reconstruction_approaches) (FBP) algorithm for CT and its implementation in [scikit-image](https://scikit-image.org/)."]}, {"cell_type": "code", "execution_count": null, "id": "44b71847", "metadata": {}, "outputs": [], "source": ["import skimage.transform as sktr\n", "\n", "# Get a sample from the generator\n", "sinogram, ground_truth = next(dat_gen)\n", "sinogram = np.asarray(sinogram).transpose()\n", "\n", "# This defines the projectiona angles\n", "theta = np.linspace(-90., 90., sinogram.shape[1], endpoint=True)\n", "\n", "# Perform FBP\n", "fbp_recon = sktr.iradon(sinogram, theta=theta, filter_name='ramp')[28:-27, 28:-27]\n", "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n", "axs[0].imshow(sinogram.transpose(), cmap='gray', extent=[0, 183, -90, 90])\n", "axs[0].set_title('Sinogram')\n", "axs[0].set_xlabel('$s$')\n", "axs[0].set_ylabel('$\\Theta$')\n", "axs[1].imshow(ground_truth, cmap='gray', clim=[0, 1])\n", "axs[1].set_title('Ground truth')\n", "axs[1].set_xlabel('$x$')\n", "axs[1].set_ylabel('$y$')\n", "axs[2].imshow(fbp_recon, cmap='gray', clim=[0, 1])\n", "axs[2].set_title('FBP')\n", "axs[2].set_xlabel('$x$')\n", "axs[2].set_ylabel('$y$')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "225e482d", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "   \u2328 <b>Exercise</b>: What do you think of the quality of the reconstructed FBP algorithm? Use the cell below to quantify the similarity between the images using the structural similarity index (SSIM). Does this reflect your intuition? Also compute the PSNR using the <a href=\"https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.peak_signal_noise_ratio\"><code>peak_signal_noise_ratio</code></a> method in scikit-image.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "46592ccc", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["import skimage.metrics as skme\n", "\n", "print('SSIM = {:.2f}'.format(skme.structural_similarity(np.asarray(ground_truth), fbp_recon, data_range=np.max(ground_truth)-np.min(ground_truth))))\n", "# \u2328 FILL IN"]}, {"cell_type": "markdown", "id": "78f54d21", "metadata": {}, "source": ["### Datasets and dataloaders\n", "\n", "Our (or your) goal now is to obtain high(er) quality reconstructed images based on the sinogram measurements. As you have seen in the lecture, this can be done in four ways:\n", "1. Train a reconstruction method that directly maps from the measurement (sinogram) domain to the image domain.\n", "2. **Preprocessing** Clean up the sinogram using a neural network, then backproject to the image domain.\n", "3. **Postprocessing** First backproject to the image domain, then improve the reconstruction using a neural network.\n", "4. Iterative methods that integrate data consistency.\n", "\n", "Here, we will follow the third approach, postprocessing. We create reconstructions from the generated sinograms using filtered backprojection and use a neural network to learn corrections on this FBP image and improve the reconstruction, as shown in the image below. The data that we need for training this network is the reconstructions from FBP, and the ground-truth reconstructions from the dival dataset. \n", "<img src='https://imgur.com/df4RYzE.png%27></img>'></img>\n", "\n", "We will make a training dataset of 512 samples from the ellipses dival dataset that we store in a MONAI <code>DataSet</code>. The code below does this in four steps:\n", "1. Create a dival generator, that creates sinograms and ground-truth reconstructions.\n", "2. Make a dictionary (like we did in the previous tutorial) that contains the ground-truth reconstructions and the reconstructions constructed by FBP as separate keys.\n", "3. Define the transforms for the data (also like the previous tutorial). In this case we require an additional 'channels' dimension, as that is what the neural network expects. We will not make use of extra data augmentation.\n", "4. Construct the dataset using the dictionary and the defined transform."]}, {"cell_type": "code", "execution_count": null, "id": "2e7acd0f", "metadata": {}, "outputs": [], "source": ["import tqdm\n", "import monai\n", "\n", "theta = np.linspace(-90., 90., sinogram.shape[1], endpoint=True)\n", "\n", "# Make a generator for the training part of the dataset\n", "train_gen = dataset.generator(part='train')\n", "train_samples = []\n", "\n", "# Make a list of (in this case) 512 random training samples. We store the filtered backprojection (FBP) and ground truth image\n", "# in a dictionary for each sample, and add these to a list.\n", "for ns in tqdm.tqdm(range(512)):\n", "    sinogram, ground_truth = next(train_gen)\n", "    sinogram = np.asarray(sinogram).transpose()\n", "    fbp_recon = sktr.iradon(sinogram, theta=theta, filter_name='ramp')[28:-27, 28:-27]\n", "    train_samples.append({'fbp': fbp_recon, 'ground_truth': np.asarray(ground_truth)})\n", "\n", "# You can add or remove transforms here\n", "train_transform = monai.transforms.Compose([\n", "    monai.transforms.AddChanneld(keys=['fbp', 'ground_truth'])\n", "])    \n", "\n", "# Use the list of dictionaries and the transform to initialize a MONAI CacheDataset\n", "train_dataset = monai.data.CacheDataset(train_samples, transform=train_transform)    "]}, {"cell_type": "markdown", "id": "7d8efe81", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Also make a validation dataset and call it <code>val_dataset</code>. This dataset can be smaller, e.g., 64 or 128 samples.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "fd46fbc7", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["# Your code goes here"]}, {"cell_type": "markdown", "id": "19f402ba", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Now, make a dataloader for both the validation and training data, called <code>train_loader</code> and <code>validation_loader</code>, that we can use for sampling batches during training of the network. Give them a reasonable batch size, e.g., 16.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "3e48a969", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["# \u2328\ufe0f FILL IN\n", "train_loader = ...\n", "validation_loader = ..."]}, {"cell_type": "markdown", "id": "ada286fb", "metadata": {}, "source": ["### Model\n", "Now that we have datasets and dataloaders, the next step is to define a model, optimizer and criterion. Because we want to improve the FBP-reconstructed image, we are dealing with an image-to-image task. A standard U-Net as implemented in MONAI is therefore a good starting point. First, make sure that you are using the GPU (CUDA), otherwise training will be extremely slow."]}, {"cell_type": "code", "execution_count": null, "id": "cf1c233f", "metadata": {}, "outputs": [], "source": ["import torch\n", "\n", "if torch.cuda.is_available():\n", "    device = torch.device(\"cuda\")\n", "elif torch.backends.mps.is_available():\n", "    device = torch.device(\"mps\")\n", "else:\n", "    device = \"cpu\"\n", "print(f'The used device is {device}')"]}, {"cell_type": "markdown", "id": "0809a595", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Initialize a U-Net with the correct settings, e.g. channels and dimensions, and call it <code>model</code>. Here, it's convenient to use the <a href=\"https://docs.monai.io/en/stable/networks.html#monai.networks.nets.BasicUNet\"><code>BasicUNet</code></a> as implemented in MONAI.\n", "\n", "</div>"]}, {"cell_type": "markdown", "id": "b0a42d4a", "metadata": {}, "source": ["### Loss function\n", "An important aspect is the loss function that you will use to optimize the model. The problem that we are trying to solve using a neural network is a *regression* problem, which differs from the *classification* approach we covered in the segmentation tutorial. Instead of classifying each pixel as a certain class, we alter their intensities to obtain a better overall reconstruction of the image. \n", "\n", "Because this task is substantially different, we need to change our loss function. In the previous tutorial we used the Dice loss, which measures the overlap for each of the classes to segment. In this case, an L2 (mean squared error) or L1 (mean average error) loss suits our objective. Alternatively, we can use a loss that aims to maximize the structural similarity (SSIM). For this, we use the [kornia](https://kornia.readthedocs.io/en/latest/) library."]}, {"cell_type": "code", "execution_count": null, "id": "4da06bf1", "metadata": {}, "outputs": [], "source": ["import kornia \n", "\n", "# Three loss functions, turn them on or off by commenting\n", "\n", "loss_function = torch.nn.MSELoss()\n", "# loss_function = torch.nn.L1Loss()\n", "# loss_function = kornia.losses.SSIMLoss(window_size=3)"]}, {"cell_type": "markdown", "id": "2835449f", "metadata": {}, "source": ["As in previous tutorials, we use an adaptive SGD (Adam) optimizer to train our network. This tutorial, we add a [learning rate scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html). This scheduler lowers the learning rate every *step_size* steps, meaning that the optimizer will take smaller steps in the direction of the gradient after a set amount of epochs. Therefore, the optimizer can potentially find a better local minimum for the weights of the neural network."]}, {"cell_type": "code", "execution_count": null, "id": "23b89950", "metadata": {}, "outputs": [], "source": ["optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n", "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"]}, {"cell_type": "markdown", "id": "c4cb0c44", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Complete the code below and train the U-Net.\n", "    \n", "\u2753 What does the model learn? Look carefully at how we determine the output of the model. Can you describe what happens in the following line: <code>outputs = model(batch_data['fbp'].float().to(device)) + batch_data[\"fbp\"].float().to(device)</code>?\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "00a221a8", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["from tqdm.notebook import tqdm\n", "import wandb\n", "from skimage.metrics import structural_similarity as ssim\n", "\n", "\n", "run = wandb.init(\n", "    project='tutorial3_reconstruction',\n", "    name='test',\n", "    config={\n", "        'loss function': str(loss_function), \n", "        'lr': optimizer.param_groups[0][\"lr\"],\n", "        'batch_size': train_loader.batch_size,\n", "    }\n", ")\n", "# Do not hesitate to enrich this list of settings to be able to correctly keep track of your experiments!\n", "# For example you should include information on your model architecture\n", "\n", "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics\n", "\n", "def log_to_wandb(epoch, train_loss, val_loss, batch_data, outputs):\n", "    \"\"\" Function that logs ongoing training variables to W&B \"\"\"\n", "\n", "    # Create list of images that have segmentation masks for model output and ground truth\n", "    # log_imgs = [wandb.Image(PIL.Image.fromarray(img.detach().cpu().numpy())) for img in outputs]\n", "    val_ssim = []\n", "    for im_id in range(batch_data['ground_truth'].shape[0]):\n", "        val_ssim.append(ssim(batch_data['ground_truth'].detach().cpu().numpy()[im_id, 0, :, :].squeeze(), \n", "                             outputs.detach().cpu().numpy()[im_id, 0, :, :].squeeze() ))\n", "    val_ssim = np.mean(np.asarray(val_ssim))\n", "    # Send epoch, losses and images to W&B\n", "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'val_ssim': val_ssim}) \n", "    \n", "for epoch in tqdm(range(75)):\n", "    model.train()    \n", "    epoch_loss = 0\n", "    step = 0\n", "    for batch_data in train_loader: \n", "        step += 1\n", "        optimizer.zero_grad()\n", "        outputs = model(batch_data[\"fbp\"].float().to(device)) + batch_data[\"fbp\"].float().to(device)\n", "        # FILL IN\n", "    # validation part\n", "    step = 0\n", "    val_loss = 0\n", "    for batch_data in validation_loader:\n", "        step += 1\n", "        model.eval()\n", "        outputs = model(batch_data['fbp'].float().to(device)) + batch_data[\"fbp\"].float().to(device)\n", "        # FILL IN\n", "    log_to_wandb(epoch, train_loss, val_loss, batch_data, outputs)\n", "    # Scheduler also needs to make a step during training\n", "    scheduler.step()\n", "\n", "# Store the network parameters        \n", "torch.save(model.state_dict(), r'trainedUNet.pt')\n", "run.finish()"]}, {"cell_type": "markdown", "id": "287143a6", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Now make a <code>DataSet</code> and <code>DataLoader</code> for the test set. Just a handful of images should be enough.\n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "f14224e9", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["import tqdm\n", "\n", "test_gen = dataset.generator(part='test')\n", "....\n", "test_dataset = ....\n", "\n", "test_loader = monai.data.DataLoader(test_dataset, batch_size=1)"]}, {"cell_type": "markdown", "id": "2fdbc021", "metadata": {}, "source": ["> \u2753 Visualize a number of reconstructions from the neural network and compare them to the fbp reconstructed images, using the code below. The performance of the network is evaluated using the structural similarity [function](https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity) in scikit-image. Does the neural network improve this metric a lot compared to the filtered back projection?"]}, {"cell_type": "code", "execution_count": null, "id": "efc59405", "metadata": {}, "outputs": [], "source": ["model.eval()\n", "\n", "for test_sample in test_loader:\n", "    output = model(test_sample['fbp'].to(device)) + test_sample['fbp'].to(device)\n", "    output = output.detach().cpu().numpy()[0, 0, :, :].squeeze()\n", "    ground_truth = test_sample['ground_truth'][0, 0, :, :].squeeze()\n", "    fbp_recon = test_sample['fbp'][0, 0, :, :].squeeze()\n", "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n", "    axs[0].imshow(fbp_recon, cmap='gray', clim=[0, 1])\n", "    axs[0].set_title('FBP SSIM={:.2f}'.format(ssim(ground_truth.cpu().numpy(), fbp_recon.cpu().numpy())))\n", "    axs[0].set_xlabel('$x$')\n", "    axs[0].set_ylabel('$y$')\n", "    axs[1].imshow(ground_truth, cmap='gray', clim=[0, 1])\n", "    axs[1].set_title('Ground truth')\n", "    axs[1].set_xlabel('$x$')\n", "    axs[1].set_ylabel('$y$')\n", "    axs[2].imshow(output, cmap='gray', clim=[0, 1])\n", "    axs[2].set_title('CNN SSIM={:.2f}'.format(ssim(ground_truth.cpu().numpy(), output)))\n", "    axs[2].set_xlabel('$x$')\n", "    axs[2].set_ylabel('$y$')\n", "    plt.show()   "]}, {"cell_type": "markdown", "id": "64cd6c49", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "\u2328 <b>Exercise</b>: \n", "Instead of a U-Net, try a different model, e.g., a <a href=\"https://docs.monai.io/en/stable/networks.html#segresnet\">SegResNet</a> in Monai.\n", "Evaluate how the different loss functions affect the performance of the network. Notes that the SSIM on the validation set is also written to Weights & Biases during training. Which loss leads to the best SSIM scores? Which loss results in the worst SSIM scores?\n", "    </div>"]}, {"cell_type": "markdown", "id": "e1be8458", "metadata": {"tags": ["student"]}, "source": ["Answer:"]}, {"cell_type": "markdown", "id": "b3c85b84", "metadata": {}, "source": ["## Part 2: Registration"]}, {"cell_type": "code", "execution_count": null, "id": "2bd4eb92", "metadata": {}, "outputs": [], "source": ["import monai\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import torch\n", "import wandb"]}, {"cell_type": "markdown", "id": "243c030e", "metadata": {}, "source": ["In the second part of the tutorial, we will register chest X-ray images. We will reuse the data of Tutorial 3. As always, we first set the paths. This should be the path ending in 'ribs'."]}, {"cell_type": "code", "execution_count": null, "id": "e147bf1c", "metadata": {}, "outputs": [], "source": ["# ONLY IF YOU USE JUPYTER: ADD PATH \u2328\ufe0f\n", "data_path = r'/Users/jmwolterink/Downloads/ribs'# WHEREDIDYOUPUTTHEDATA?"]}, {"cell_type": "code", "execution_count": null, "id": "900cf0ec", "metadata": {}, "outputs": [], "source": ["# ONLY IF YOU USE COLAB: ADD PATH \u2328\ufe0f\n", "from google.colab import drive\n", "\n", "drive.mount('/content/drive')\n", "data_path = r'/content/drive/My Drive/Tutorial3'"]}, {"cell_type": "code", "execution_count": null, "id": "099a2661", "metadata": {}, "outputs": [], "source": ["# check if data_path exists:\n", "import os\n", "\n", "if not os.path.exists(data_path):\n", "    print(\"Please update your data path to an existing folder.\")\n", "elif not set([\"train\", \"val\", \"test\"]).issubset(set(os.listdir(data_path))):\n", "    print(\"Please update your data path to the correct folder (should contain train, val and test folders).\")\n", "else:\n", "    print(\"Congrats! You selected the correct folder :)\")"]}, {"cell_type": "markdown", "id": "d2002a02", "metadata": {}, "source": ["### Data management\n", "\n", "In this part we prepare all the tools needed to load and visualize our samples. One thing we *could* do is perform **inter**-patient registration, i.e., register two chest X-ray images of different patients. However, this is a very challenging problem. Instead, to make our life a bit easier, we will perform **intra**-patient registration: register two images of the same patient. For each patient, we make a synthetic moving image by applying some random elastic deformations. To build this data set, we we used the [Rand2DElasticd](https://docs.monai.io/en/stable/transforms.html#rand2delastic) transform on both the image and the mask. We will use a neural network to learn the deformation field between the fixed image and the moving image.\n", "<img src='https://i.imgur.com/OmoOZ5w.png'></img>"]}, {"cell_type": "markdown", "id": "e8a7d448", "metadata": {}, "source": ["Similarly as in Tutorial 3, make a dictionary of the image file names."]}, {"cell_type": "code", "execution_count": null, "id": "ccc75ada", "metadata": {}, "outputs": [], "source": ["import os\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import glob\n", "import monai\n", "from PIL import Image\n", "import torch\n", "\n", "def build_dict_ribs(data_path, mode='train'):\n", "    \"\"\"\n", "    This function returns a list of dictionaries, each dictionary containing the keys 'img' and 'mask' \n", "    that returns the path to the corresponding image.\n", "    \n", "    Args:\n", "        data_path (str): path to the root folder of the data set.\n", "        mode (str): subset used. Must correspond to 'train', 'val' or 'test'.\n", "        \n", "    Returns:\n", "        (List[Dict[str, str]]) list of the dictionnaries containing the paths of X-ray images and masks.\n", "    \"\"\"\n", "    # test if mode is correct\n", "    if mode not in [\"train\", \"val\", \"test\"]:\n", "        raise ValueError(f\"Please choose a mode in ['train', 'val', 'test']. Current mode is {mode}.\")\n", "    \n", "    # define empty dictionary\n", "    dicts = []\n", "    # list all .png files in directory, including the path\n", "    paths_xray = glob.glob(os.path.join(data_path, mode, 'img', '*.png'))\n", "    # make a corresponding list for all the mask files\n", "    for xray_path in paths_xray:\n", "        if mode == 'test':\n", "            suffix = 'val'\n", "        else:\n", "            suffix = mode\n", "        # find the binary mask that belongs to the original image, based on indexing in the filename\n", "        image_index = os.path.split(xray_path)[1].split('_')[-1].split('.')[0]\n", "        # define path to mask file based on this index and add to list of mask paths\n", "        mask_path = os.path.join(data_path, mode, 'mask', f'VinDr_RibCXR_{suffix}_{image_index}.png')\n", "        if os.path.exists(mask_path):\n", "            dicts.append({'fixed': xray_path, 'moving': xray_path, 'fixed_mask': mask_path, 'moving_mask': mask_path})\n", "    return dicts\n", "\n", "class LoadRibData(monai.transforms.Transform):\n", "    \"\"\"\n", "    This custom Monai transform loads the data from the rib segmentation dataset.\n", "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n", "    \"\"\"\n", "    def __init__(self, keys=None):\n", "        pass\n", "\n", "    def __call__(self, sample):\n", "        fixed = Image.open(sample['fixed']).convert('L') # import as grayscale image\n", "        fixed = np.array(fixed, dtype=np.uint8)\n", "        moving = Image.open(sample['moving']).convert('L') # import as grayscale image\n", "        moving = np.array(moving, dtype=np.uint8)        \n", "        fixed_mask = Image.open(sample['fixed_mask']).convert('L') # import as grayscale image\n", "        fixed_mask = np.array(fixed_mask, dtype=np.uint8)\n", "        moving_mask = Image.open(sample['moving_mask']).convert('L') # import as grayscale image\n", "        moving_mask = np.array(moving_mask, dtype=np.uint8)        \n", "        # mask has value 255 on rib pixels. Convert to binary array\n", "        fixed_mask[np.where(fixed_mask==255)] = 1\n", "        moving_mask[np.where(moving_mask==255)] = 1        \n", "        return {'fixed': fixed, 'moving': moving, 'fixed_mask': fixed_mask, 'moving_mask': moving_mask, 'img_meta_dict': {'affine': np.eye(2)}, \n", "                'mask_meta_dict': {'affine': np.eye(2)}}"]}, {"cell_type": "markdown", "id": "066510f6", "metadata": {}, "source": ["Then we make a training dataset like before. The <code>Rand2DElasticd</code> transform here determines how much deformation is in the 'moving' image. "]}, {"cell_type": "code", "execution_count": null, "id": "c4a18395", "metadata": {}, "outputs": [], "source": ["train_dict_list = build_dict_ribs(data_path, mode='train')\n", "\n", "# constructDataset from list of paths + transform\n", "transform = monai.transforms.Compose(\n", "[\n", "    LoadRibData(),\n", "    monai.transforms.AddChanneld(keys=['fixed', 'moving', 'fixed_mask', 'moving_mask']),\n", "    monai.transforms.Resized(keys=['fixed', 'moving', 'fixed_mask', 'moving_mask'], spatial_size=(256, 256),  mode=['bilinear', 'bilinear', 'nearest', 'nearest']),\n", "    monai.transforms.HistogramNormalized(keys=['fixed', 'moving']),\n", "    monai.transforms.ScaleIntensityd(keys=['fixed', 'moving'], minv=0.0, maxv=1.0),\n", "    monai.transforms.Rand2DElasticd(keys=['moving', 'moving_mask'], spacing=(64, 64), \n", "                                    magnitude_range=(-8, 8), prob=1, mode=['bilinear', 'nearest']),    \n", "])\n", "train_dataset = monai.data.Dataset(train_dict_list, transform=transform)"]}, {"cell_type": "markdown", "id": "0c451d13", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "  \u2328 <b>Exercise</b>: Visualize fixed and moving training images associated to their comparison image with <code>visualize_fmc_sample</code>.\n", "    <p>Try different methods to create the comparison image. How well do these different methods allow you to qualitatively assess the quality of the registration?</p>\n", "     <p>More information on this method is available in  <a href=\"https://scikit-image.org/docs/stable/api/skimage.util.html#skimage.util.compare_images\">scikit-image documentation</a></p> \n", "\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "f2eb9cc8", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["def visualize_fmc_sample(sample, method=\"checkerboard\"):\n", "    \"\"\"\n", "    Plot three images: fixed, moving and comparison.\n", "    \n", "    Args:\n", "        sample (dict): sample of dataset created with `build_dataset`.\n", "        method (str): method used by `skimage.util.compare_image`.\n", "    \"\"\"\n", "    import skimage.util as skut \n", "    \n", "    skut_methods = [\"diff\", \"blend\", \"checkerboard\"]\n", "    if method not in skut_methods:\n", "        raise ValueError(f\"Method must be chosen in {skut_methods}.\\n\"\n", "                         f\"Current value is {method}.\")\n", "    \n", "    \n", "    fixed = np.squeeze(sample['fixed'])\n", "    moving = np.squeeze(sample['moving'])\n", "    comp_checker = skut.compare_images(fixed, moving, method=method)\n", "    axs = plt.figure(constrained_layout=True, figsize=(15, 5)).subplot_mosaic(\"FMC\")\n", "    axs['F'].imshow(fixed, cmap='gray')\n", "    axs['F'].set_title('Fixed')\n", "    axs['M'].imshow(moving, cmap='gray')\n", "    axs['M'].set_title('Moving')\n", "    axs['C'].imshow(comp_checker, cmap='gray')\n", "    axs['C'].set_title('Comparison')\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "ff4a7e9c", "metadata": {}, "outputs": [], "source": ["sample = train_dataset[0]\n", "for method in [\"diff\", \"blend\", \"checkerboard\"]:\n", "    print(f\"Method {method}\")\n", "    visualize_fmc_sample(sample, method=method)"]}, {"cell_type": "markdown", "id": "3adfca98", "metadata": {}, "source": ["Now we apply a little trick. Because applying the random deformation in each training iteration will be very costly, we only apply the deformation once and we make a new dataset based on the deformed images. Running the cell below make take a few minutes."]}, {"cell_type": "code", "execution_count": null, "id": "042b40b1", "metadata": {}, "outputs": [], "source": ["import tqdm\n", "\n", "train_loader = monai.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n", "\n", "samples = []\n", "for train_batch in tqdm.tqdm(train_loader):\n", "    samples.append(train_batch)\n", "\n", "# Make a new dataset and dataloader using the transformed images\n", "train_dataset = monai.data.Dataset(samples, transform=monai.transforms.SqueezeDimd(keys=['fixed', 'moving', 'fixed_mask', 'moving_mask']))\n", "train_loader = monai.data.DataLoader(train_dataset, batch_size=16, shuffle=False)"]}, {"cell_type": "markdown", "id": "e3f540a1", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Create <code>val_dataset</code> and <code>val_loader</code>, corresponding to the DataSet and DataLoader for your validation set. The transforms can be the same as in the training set.\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "c1e8aad3", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["# Your code goes here"]}, {"cell_type": "markdown", "id": "71292715", "metadata": {}, "source": ["### Model\n", "\n", "We use again the U-Net architecture as in the previous tutorial. However, this time our input / output structure is quite different:\n", "- the network takes as input two images: the *moving* and *fixed* images.\n", "- it outputs one tensor representing the *deformation field*.\n", "\n", "<img src='https://i.imgur.com/rvZfwr2.png' width=600></img>\n", "\n", "\n", "This *deformation field* can be applied to the *moving* image with the `monai.networks.blocks.Warp` block of Monai.\n", "\n", "<img src='https://i.imgur.com/gj7JnOy.png' width=500></img>\n", "\n", "\n", "This deformed moving image is then compared to the *fixed* image: if they are similar, the deformation field is correctly registering the moving image on the fixed image. Keep in mind that this is done on **training** data, and we want the U-Net to learn to predict a proper deformation field given two new and unseen images. So we're not optimizing for a pair of images as would be done in conventional iterative registration, but training a model that can generalize.\n", "\n", "<img src='https://i.imgur.com/aM7OrR4.png' width=300></img>\n"]}, {"cell_type": "markdown", "id": "2cd27736", "metadata": {}, "source": ["Before starting, let's check that you can work on a GPU by runnning the following cell:\n", "- if the device is \"cuda\" you are working on a GPU,\n", "- if the device is \"cpu\" call a teacher."]}, {"cell_type": "code", "execution_count": null, "id": "94af5ac6", "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["if torch.cuda.is_available():\n", "    device = torch.device(\"cuda\")\n", "elif torch.backends.mps.is_available():\n", "    device = torch.device(\"mps\")\n", "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]=\"1\"\n", "else:\n", "    device = \"cpu\"\n", "print(f'The used device is {device}')"]}, {"cell_type": "markdown", "id": "49842cde", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Construct a U-Net with suitable settings and name it <code>model</code>.\n", "    <p>Check that you can correctly apply its output to the input moving image with the <code>warp_layer</code>!</p>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "e39a91fb", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["model = # FILL IN\n", "\n", "warp_layer = monai.networks.blocks.Warp().to(device)"]}, {"cell_type": "markdown", "id": "a6ace06b", "metadata": {}, "source": ["### Objective function\n", "\n", "We evaluate the similarity between the fixed image and the deformed moving image with the `MSELoss()`. The L1 or SSIM losses seen in the previous section could also be used. Furthermore, the deformation field is regularized with `BendingEnergyLoss`. This is a penalty that takes the smoothness of the deformation field into account: if it's not smooth enough, the bending energy is high. Thus, our model will favor smooth deformation fields.\n", "\n", "Finally, we pick an optimizer, in this case again an Adam optimizer."]}, {"cell_type": "code", "execution_count": null, "id": "96e40774", "metadata": {}, "outputs": [], "source": ["image_loss = torch.nn.MSELoss()\n", "regularization = monai.losses.BendingEnergyLoss()\n", "optimizer = torch.optim.Adam(model.parameters(), 1e-3)"]}, {"cell_type": "markdown", "id": "93e82b60", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Exercise</b>: Add a learning rate scheduler that lowers the learning rate by a factor ten every 100 epochs.</p>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "id": "c976a586", "metadata": {"tags": ["student"]}, "outputs": [], "source": ["# Your code goes here"]}, {"cell_type": "markdown", "id": "e7a589d0", "metadata": {}, "source": ["To warp the moving image using the predicted deformation field and *then* compute the loss between the deformed image and the fixed image, we define a forward function which does all this. The output of this function is `pred_image`. "]}, {"cell_type": "code", "execution_count": null, "id": "8fd0c7f9", "metadata": {}, "outputs": [], "source": ["def forward(batch_data, model):\n", "    \"\"\"\n", "    Applies the model to a batch of data.\n", "    \n", "    Args:\n", "        batch_data (dict): a batch of samples computed by a DataLoader.\n", "        model (Module): a model computing the deformation field.\n", "    \n", "    Returns:\n", "        ddf (Tensor): batch of deformation fields.\n", "        pred_image (Tensor): batch of deformed moving images.\n", "    \n", "    \"\"\"\n", "    fixed_image = batch_data[\"fixed\"].to(device).float()\n", "    moving_image = batch_data[\"moving\"].to(device).float()\n", "    \n", "    # predict DDF\n", "    ddf = model(torch.cat((moving_image, fixed_image), dim=1))\n", "\n", "    # warp moving image and label with the predicted ddf\n", "    pred_image = warp_layer(moving_image, ddf)\n", "\n", "    return ddf, pred_image"]}, {"cell_type": "markdown", "id": "26b2e7c5", "metadata": {}, "source": ["You can supervise the training process in W&B, in which at each epoch a batch of validation images are used to compute the comparison images of your choice, based on the parameter `method`."]}, {"cell_type": "code", "execution_count": null, "id": "dfb2440f", "metadata": {}, "outputs": [], "source": ["def log_to_wandb(epoch, train_loss, val_loss, pred_batch, fixed_batch, method=\"checkerboard\"):\n", "    \"\"\" Function that logs ongoing training variables to W&B \"\"\"\n", "    import skimage.util as skut\n", "    \n", "    log_imgs = []\n", "    for fixed_pt, pred_pt in zip(pred_batch, fixed_batch):\n", "        fixed_np = np.squeeze(fixed_pt.cpu().detach())\n", "        pred_np = np.squeeze(pred_pt.cpu().detach())\n", "        comp_checker = skut.compare_images(fixed_np, pred_np, method=method)\n", "        log_imgs.append(wandb.Image(comp_checker))\n", "\n", "    # Send epoch, losses and images to W&B\n", "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'results': log_imgs})"]}, {"cell_type": "markdown", "id": "96f5d659", "metadata": {}, "source": ["### Training time\n", "\n", "Use the following cells to train your network. You may choose different parameters to improve the performance!"]}, {"cell_type": "code", "execution_count": null, "id": "7c4e7416", "metadata": {}, "outputs": [], "source": ["# Choose your parameters\n", "\n", "max_epochs = 200\n", "reg_weight = 0 # By default 0, but you can investigate what it does"]}, {"cell_type": "code", "execution_count": null, "id": "5c1d9f95", "metadata": {}, "outputs": [], "source": ["from tqdm import tqdm\n", "\n", "run = wandb.init(\n", "    project='tutorial4_registration',\n", "    config={\n", "        'lr': optimizer.param_groups[0][\"lr\"],\n", "        'batch_size': train_loader.batch_size,\n", "        'regularization': reg_weight,\n", "        'loss_function': str(image_loss)\n", "    }\n", ")\n", "# Do not hesitate to enrich this list of settings to be able to correctly keep track of your experiments!\n", "# For example you should add information on your model...\n", "\n", "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics\n", "\n", "for epoch in tqdm(range(max_epochs)):    \n", "    model.train()\n", "    epoch_loss = 0\n", "    for batch_data in train_loader:\n", "        optimizer.zero_grad()\n", "\n", "        ddf, pred_image = forward(batch_data, model)\n", "\n", "        fixed_image = batch_data[\"fixed\"].to(device).float()\n", "        reg = regularization(ddf)\n", "        loss = image_loss(pred_image, fixed_image) + reg_weight * reg\n", "        loss.backward()\n", "        optimizer.step()\n", "        epoch_loss += loss.item()\n", "\n", "    epoch_loss /= len(train_loader)\n", "\n", "    model.eval()\n", "    val_epoch_loss = 0\n", "    for batch_data in val_loader:\n", "        ddf, pred_image = forward(batch_data, model)\n", "        fixed_image = batch_data[\"fixed\"].to(device).float()\n", "        reg = regularization(ddf)\n", "        loss = image_loss(pred_image, fixed_image) + reg_weight * reg\n", "        val_epoch_loss += loss.item()\n", "    val_epoch_loss /= len(val_loader)\n", "\n", "    log_to_wandb(epoch, epoch_loss, val_epoch_loss, pred_image, fixed_image)\n", "    \n", "run.finish()    "]}, {"cell_type": "markdown", "id": "3fbf7904", "metadata": {}, "source": ["### Evaluation of the trained model\n", "\n", "Now that the model has been trained, it's time to evaluate its performance. Use the code below to visualize samples and deformation fields. \n", "> \u2753 Are you satisfied with these registration results? Do they seem anatomically plausible? Try out different regularization factors (<code>reg_weight</code>) and see what they do to the registration."]}, {"cell_type": "markdown", "id": "65d435ca", "metadata": {"tags": ["student"]}, "source": ["Answer: "]}, {"cell_type": "code", "execution_count": null, "id": "ee8da0ca", "metadata": {}, "outputs": [], "source": ["def visualize_prediction(sample, model, method=\"checkerboard\"):\n", "    \"\"\"\n", "    Plot three images: fixed, moving and comparison.\n", "    \n", "    Args:\n", "        sample (dict): sample of dataset created with `build_dataset`.\n", "        model (Module): a model computing the deformation field.\n", "        method (str): method used by `skimage.util.compare_image`.\n", "    \"\"\"\n", "    import skimage.util as skut \n", "    \n", "    skut_methods = [\"diff\", \"blend\", \"checkerboard\"]\n", "    if method not in skut_methods:\n", "        raise ValueError(f\"Method must be chosen in {skut_methods}.\\n\"\n", "                         f\"Current value is {method}.\")\n", "        \n", "    model.eval()\n", "    \n", "    # Compute deformation field + deformed image\n", "    batch_data = {\n", "        \"fixed\": sample[\"fixed\"].unsqueeze(0),\n", "        \"moving\": sample[\"moving\"].unsqueeze(0),\n", "    }\n", "    ddf, pred_image = forward(batch_data, model)\n", "    ddf = ddf.detach().cpu().numpy().squeeze()\n", "    ddf = np.linalg.norm(ddf, axis=0).squeeze()\n", "    \n", "    # Squeeze images\n", "    fixed = np.squeeze(sample[\"fixed\"])\n", "    moving = np.squeeze(sample[\"moving\"])    \n", "    deformed = np.squeeze(pred_image.detach().cpu())\n", "    \n", "    # Generate comparison image\n", "    comp_checker = skut.compare_images(fixed, deformed, method=method, n_tiles=(4, 4))\n", "    \n", "    # Plot everything\n", "    fig, axs = plt.subplots(1, 5, figsize=(18, 5))    \n", "    axs[0].imshow(fixed, cmap='gray')\n", "    axs[0].set_title('Fixed')\n", "    axs[1].imshow(moving, cmap='gray')\n", "    axs[1].set_title('Moving')\n", "    axs[2].imshow(deformed, cmap='gray')\n", "    axs[2].set_title('Deformed')\n", "    axs[3].imshow(comp_checker, cmap='gray')\n", "    axs[3].set_title('Comparison')    \n", "    dpl = axs[4].imshow(ddf, clim=(0, 10))\n", "    fig.colorbar(dpl, ax=axs[4])\n", "    plt.show()   \n", "    plt.show()\n", "for sample in val_dataset:\n", "    visualize_prediction(sample, model)"]}, {"cell_type": "markdown", "id": "2ad13e6c", "metadata": {}, "source": ["<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n", "    \u2328 <b>Bonus exercise</b>: Compute the Jacobian determinant at each image voxel. How many of these are negative? Can you improve upon this?</p>\n", "</div>"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}