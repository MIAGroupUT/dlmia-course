{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036f8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, listdir, remove\n",
    "import monai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88668c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open Tutorial_6.zip, Tutorial_6.zip.zip or Tutorial_6.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://surfdrive.surf.nl/files/index.php/s/nku51iYxnr8ue7c/download -O Tutorial_6.zip\n",
    "!unzip -qo Tutorial_6.zip\n",
    "data_path = \"Tutorial_6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2122b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sample_list(data_path, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    This function creates a list containing all the samples of a subset.        \n",
    "        \n",
    "    Args:\n",
    "        data_path (str): path to the root folder of the data set.\n",
    "        mode (str): subset that must be loaded. Must be chosen in [\"train\", \"val\", \"test\"].\n",
    "        \n",
    "    Returns:\n",
    "        (List[Dict[str, str]]) list of all samples of the data set. \n",
    "        One sample is a dictionary with the following keys:\n",
    "            - img_path (str): path to the image file.\n",
    "            - idx (str): unique index allowing to identify an individual image (associated to the diagnosis).\n",
    "            - diagnosis (str): value of the diagnosis, \"covid\" or \"normal\".\n",
    "    \"\"\"\n",
    "    \n",
    "    possible_modes = [\"train\", \"val\", \"test\"]\n",
    "    \n",
    "    if mode not in possible_modes:\n",
    "        raise ValueError(f\"Please choose a mode in {possible_modes}.\\n\"\n",
    "                         f\"Current mode is {mode}.\")\n",
    "    \n",
    "    data_path = path.join(data_path, mode)\n",
    "    file_name_list = [file_name for file_name in listdir(data_path) if not file_name.startswith(\".\")]\n",
    "    sample_dict_list = list()\n",
    "    \n",
    "    for file_name in file_name_list:\n",
    "        keys_str = path.splitext(file_name)[0]\n",
    "        keys = {\n",
    "            pair_str.split(\"-\")[0]: pair_str.split(\"-\")[1] \n",
    "            for pair_str in keys_str.split(\"_\")\n",
    "        }\n",
    "        keys[\"img_path\"] = path.join(data_path, file_name)\n",
    "        sample_dict_list.append(keys)\n",
    "    return sample_dict_list\n",
    "\n",
    "\n",
    "class LoadChestData(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This transform loads the image and computes the label corresponding to a sample computed by `build_sample_list`.\n",
    "    After the transform, the sample includes three new keys:\n",
    "        - img (Tensor): corresponds to the chest X-ray image.\n",
    "        - label (int): is the code corresponding to the diagnosis.\n",
    "        - img_meta_dict (dict): includes meta-data that may be useful to apply some transforms of Monai.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.label_code = {\"normal\": 0, \"covid\": 1}\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        from PIL import Image\n",
    "        \n",
    "        image = Image.open(sample[\"img_path\"]).convert('L') # import as grayscale image\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "        label = self.label_code[sample[\"diagnosis\"]]\n",
    "        sample.update({\n",
    "            \"img\": torch.from_numpy(image).unsqueeze(0).float(), \n",
    "            \"label\": label,\n",
    "            \"img_meta_dict\": {\"affine\": np.eye(2)},\n",
    "            \n",
    "        })\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c424e5a2",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tutorial_6/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/638347542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sample_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_sample_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCacheDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sample_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLoadChestData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1260672806.py\u001b[0m in \u001b[0;36mbuild_sample_list\u001b[0;34m(data_path, mode)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfile_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msample_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tutorial_6/train'"
     ]
    }
   ],
   "source": [
    "train_sample_list = build_sample_list(data_path, mode=\"train\")\n",
    "train_data = monai.data.CacheDataset(train_sample_list, transform=LoadChestData())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38d6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(sample):\n",
    "    \"\"\"\n",
    "    Plot the chest X-ray image included in a sample transformed by `LoadChestData`.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if not isinstance(sample, dict):\n",
    "        raise ValueError(f\"Sample should be a dictionary. Current type is {type(sample)}\")\n",
    "    \n",
    "    # Visualize the x-ray and describe the sample in title\n",
    "    image = np.squeeze(sample['img'])\n",
    "    plt.imshow(image, 'gray')\n",
    "    plt.title(f\"Image #{sample['idx']} associated with {sample['diagnosis']} diagnosis\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ec0fa1",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tutorial_6/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/3331066324.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{mode} set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msample_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_sample_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mn_diagnosis_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"covid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1260672806.py\u001b[0m in \u001b[0;36mbuild_sample_list\u001b[0;34m(data_path, mode)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfile_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msample_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tutorial_6/train'"
     ]
    }
   ],
   "source": [
    "modes = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for mode in modes:\n",
    "    print(f\"{mode} set:\")\n",
    "    sample_list = build_sample_list(data_path, mode=mode)\n",
    "    n_diagnosis_dict = {\"normal\": 0, \"covid\": 0}\n",
    "    for sample in sample_list:\n",
    "        diagnosis = sample[\"diagnosis\"]\n",
    "        n_diagnosis_dict[diagnosis] += 1\n",
    "    print(f\"{len(sample_list)} images: {n_diagnosis_dict['normal']} normal / {n_diagnosis_dict['covid']} covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5651c246",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1556628025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvisualize_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "n_samples = 20\n",
    "\n",
    "for sample_idx in range(n_samples):\n",
    "    visualize_sample(train_data[sample_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02fcb5a",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tutorial_6/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/158552130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCacheDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_sample_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1260672806.py\u001b[0m in \u001b[0;36mbuild_sample_list\u001b[0;34m(data_path, mode)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfile_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msample_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tutorial_6/train'"
     ]
    }
   ],
   "source": [
    "train_transforms = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadChestData(),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "        monai.transforms.RandFlipd(keys=['img'], prob=0.5, spatial_axis=1),\n",
    "        monai.transforms.RandGaussianNoised(keys=['img'], prob=0.5, mean=0.0, std=0.1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=\"train\"), transform=train_transforms)\n",
    "train_loader = monai.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "validation_dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=\"val\"), transform=LoadChestData())\n",
    "validation_loader = monai.data.DataLoader(validation_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a4593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not found. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check whether we're using a GPU\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()  # Total number of GPUs\n",
    "    gpu_idx = random.randint(0, n_gpus - 1)  # Random GPU index\n",
    "    device = torch.device(f'cuda:{gpu_idx}')\n",
    "    print('Using GPU: {}'.format(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU not found. Using CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738898fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjelmerwolterink\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e777e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_compose_to_list(transform_compose):\n",
    "    \"\"\"\n",
    "    Transform an object monai.transforms.Compose in a list fully describing the transform.\n",
    "    /!\\ Random seed is not saved, then reproducibility is not enabled.\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "        \n",
    "    if not isinstance(transform_compose, monai.transforms.Compose):\n",
    "        raise TypeError(\"transform_compose should be a monai.transforms.Compose object.\")\n",
    "    \n",
    "    output_list = list()\n",
    "    for transform in transform_compose.transforms:\n",
    "        kwargs = deepcopy(vars(transform))\n",
    "        \n",
    "        # Remove attributes which are not arguments\n",
    "        args = list(transform.__init__.__code__.co_varnames[1: transform.__init__.__code__.co_argcount])\n",
    "        for key, obj in vars(transform).items():\n",
    "            if key not in args:\n",
    "                del kwargs[key]\n",
    "\n",
    "        output_list.append({\"class\": transform.__class__, \"kwargs\": kwargs})\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57819396",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1155661857.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = monai.networks.nets.Classifier(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0min_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "model = monai.networks.nets.Classifier(\n",
    "    in_shape=train_dataset[0][\"img\"].shape,\n",
    "    classes=2,\n",
    "    channels=[16, 32, 64, 128, 128, 128],\n",
    "    strides=[2, 2, 2, 2, 2],\n",
    "    num_res_units=0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b5e310",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/3236778461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Set the loss function and the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m run = wandb.init(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set your parameters here\n",
    "learning_rate = 1e-4\n",
    "epochs = 20\n",
    "\n",
    "# Set the loss function and the optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "run = wandb.init(\n",
    "    project='tutorial6_explainability',\n",
    "    name='covid_detection',\n",
    "    config={\n",
    "        'loss function': str(loss_function), \n",
    "        'lr': learning_rate,\n",
    "        'transform': from_compose_to_list(train_transforms),\n",
    "        'batch_size': train_loader.batch_size,\n",
    "        'epochs': epochs,\n",
    "        'n_conv': len(model.net)\n",
    "    }\n",
    ")\n",
    "\n",
    "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics later\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()    \n",
    "    epoch_loss = 0\n",
    "    for batch_data in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data[\"img\"].to(device))\n",
    "        loss = loss_function(outputs, batch_data[\"label\"].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    val_loss = 0\n",
    "    for batch_data in validation_loader:\n",
    "        model.eval()\n",
    "        outputs = model(batch_data[\"img\"].to(device))\n",
    "        loss = loss_function(outputs, batch_data[\"label\"].to(device))\n",
    "        val_loss+= loss.item()\n",
    "    val_loss = val_loss / len(validation_loader)\n",
    "    \n",
    "    wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "\n",
    "# Log trained model in W&B\n",
    "torch.save(model.state_dict(), r'covid_classifier.pt')\n",
    "artifact = wandb.Artifact(name=f\"covid_classifier\", type=\"model\")\n",
    "artifact.add_file(r'covid_classifier.pt')\n",
    "wandb.log_artifact(artifact)\n",
    "remove(r'covid_classifier.pt')\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a439a4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction(dataloader, model):\n",
    "    \"\"\"\n",
    "    Computes a DataFrame whose rows correspond to images in the data set wrapped by dataloader.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: a DataLoader wrapping a DataSet.\n",
    "        model: a torch or monai network.\n",
    "        \n",
    "    Returns:\n",
    "        A pandas DataFrame with 4 columns:\n",
    "        - img_path (str): path to the image file,\n",
    "        - diagnosis (str): diagnosis (\"covid\" or \"normal\")\n",
    "        - label (int): ground truth label corresponding to the diagnosis (0 is \"normal\" and 1 is \"covid\")\n",
    "        - prediction (int): prediction of the network, corresponding to the node with the highest value.\n",
    "            Can be directly compared to \"label\".\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    results_df = pd.DataFrame(columns=[\"img_path\", \"diagnosis\", \"label\", \"prediction\"])\n",
    "    \n",
    "    for batch_dict in dataloader:\n",
    "        image_paths, diagnoses, labels = batch_dict[\"img_path\"], batch_dict[\"diagnosis\"], batch_dict[\"label\"]\n",
    "        images = batch_dict[\"img\"]\n",
    "        outputs = model(images.to(device))\n",
    "        prediction = torch.argmax(outputs.data, axis=1)\n",
    "        for idx in range(len(prediction)):\n",
    "            row = [image_paths[idx], diagnoses[idx], labels[idx].item(), prediction[idx].item()]\n",
    "            row_df = pd.DataFrame([row], columns=results_df.columns)\n",
    "            results_df = pd.concat([results_df, row_df])\n",
    "    \n",
    "    results_df.reset_index(inplace=True, drop=True)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(dataloader, model):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for the labels and predictions \"normal\" and \"covid\"\n",
    "    \n",
    "    Args:\n",
    "        dataloader (DataLoader): a DataLoader wrapping the evaluated data set.\n",
    "        model (Module): a torch or monai network.\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame) the confusion matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_df = compute_prediction(dataloader, model)\n",
    "    confusion_df = pd.DataFrame(index=[\"covid\", \"normal\"], columns=[\"covid\", \"normal\"])\n",
    "    confusion_df.loc[\"normal\", \"normal\"] = len(prediction_df[(prediction_df.label == 0) & (prediction_df.prediction == 0)])\n",
    "    confusion_df.loc[\"normal\", \"covid\"] = len(prediction_df[(prediction_df.label == 0) & (prediction_df.prediction == 1)])\n",
    "    confusion_df.loc[\"covid\", \"covid\"] = len(prediction_df[(prediction_df.label == 1) & (prediction_df.prediction == 1)])\n",
    "    confusion_df.loc[\"covid\", \"normal\"] = len(prediction_df[(prediction_df.label == 1) & (prediction_df.prediction == 0)])\n",
    "    return confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0faff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(run_id, confusion_df, mode=\"test\"):\n",
    "    \"\"\"\n",
    "    Saves the values of a confusion matrix to W&B interface.\n",
    "    \n",
    "    Args:\n",
    "        run_id (str): ID of the run you want to log to.\n",
    "            In the training cell, this value was assigned to the variable \"run_id\".\n",
    "            You can also retrieve it in the log of the cell or on the W&B interface.\n",
    "        confusion_df (pd.DataFrame): output of `compute_confusion_matrix`.\n",
    "        mode (str): name of the subset used to compute `confusion_df`.\n",
    "            May correspond to \"train\", \"validation\" or \"test\".\n",
    "    \"\"\"\n",
    "    print(f\"Logging the results on {mode} set of run {run_id}\")\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"tutorial6_explainability/{run_id}\")\n",
    "    run.summary[f\"{mode}_TP\"] = confusion_df.loc[\"covid\", \"covid\"]\n",
    "    run.summary[f\"{mode}_FP\"] = confusion_df.loc[\"normal\", \"covid\"]\n",
    "    run.summary[f\"{mode}_TN\"] = confusion_df.loc[\"normal\", \"normal\"]\n",
    "    run.summary[f\"{mode}_FN\"] = confusion_df.loc[\"covid\", \"normal\"]\n",
    "    run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "535a9ac2",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tutorial_6/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/4193834962.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCacheDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_sample_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLoadChestData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfusion_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1260672806.py\u001b[0m in \u001b[0;36mbuild_sample_list\u001b[0;34m(data_path, mode)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfile_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msample_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tutorial_6/test'"
     ]
    }
   ],
   "source": [
    "mode = \"test\"\n",
    "dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=mode), transform=LoadChestData())\n",
    "dataloader = monai.data.DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "confusion_df = compute_confusion_matrix(dataloader, model)\n",
    "print(confusion_df)\n",
    "log_confusion_matrix(run_id, confusion_df, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ad7c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "    \"\"\"\n",
    "    Produces Grad-CAM to a monai.networks.nets.Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "    def generate_gradients(self, input_batch, target_class=None):\n",
    "        \"\"\"\n",
    "        Generate the gradients map corresponding to the input_tensor.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor (Tensor): tensor representing a batch of images.\n",
    "            target_class (int): allows to choose from which node the gradients are back-propagated.\n",
    "                Default will back-propagate from the node corresponding to the true class of the image.\n",
    "            \n",
    "        Returns:\n",
    "            (Tensor): the gradients map\n",
    "        \"\"\"\n",
    "        input_tensor = input_batch[\"img\"].to(self.device)        \n",
    "        # Dissect model\n",
    "        conv_part = self.model.net\n",
    "        final_part = self.model.final\n",
    "        \n",
    "        # Get last conv feature map\n",
    "        feature_maps = conv_part(input_tensor).detach()\n",
    "        feature_maps.requires_grad = True\n",
    "        model_output = final_part(feature_maps)\n",
    "        # Target for backprop\n",
    "        one_hot_output = torch.zeros_like(model_output)\n",
    "        if target_class is not None:\n",
    "            one_hot_output[:, target_class] = 1\n",
    "        else:\n",
    "            labels = input_batch[\"label\"]\n",
    "            for i, target_class in enumerate(labels):\n",
    "                one_hot_output[i, target_class] = 1\n",
    "        one_hot_output = one_hot_output.to(self.device)\n",
    "        # Backward pass\n",
    "        model_output.backward(gradient=one_hot_output)\n",
    "        # Convert Pytorch variable to numpy array\n",
    "        gradients = feature_maps.grad\n",
    "        pooled_gradients = torch.mean(gradients, dim=[2, 3]).unsqueeze(2).unsqueeze(3)\n",
    "        \n",
    "        # Weight feature maps according to pooled gradients\n",
    "        feature_maps.requires_grad = False\n",
    "        feature_maps *= pooled_gradients\n",
    "        # Take the mean of all weighted feature maps\n",
    "        grad_cam = torch.mean(feature_maps, dim=1).cpu()\n",
    "        resize_transform = monai.transforms.Resize(input_tensor.shape[-2::], mode=\"bilinear\")\n",
    "        \n",
    "        return resize_transform(grad_cam).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc4fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grad_cam(batch_dict, model, target_class=None, v_display=None):\n",
    "    \"\"\"\n",
    "    Plots chest X-rays images with their corresponding grad-CAM maps.\n",
    "    \n",
    "    Args:\n",
    "        batch_dict (dict): batch of samples produced by a DataLoader.\n",
    "        model (Classifier): a monai Classifier with two output classes.\n",
    "        target_class (int): allows to choose from which node the gradients are back-propagated.\n",
    "            Default will back-propagate from the node corresponding to the true class of the image.\n",
    "        v_display (float): changes the scale of the gradient maps.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    gradients_transform = GradCam(model)\n",
    "    gradients = gradients_transform.generate_gradients(batch_dict, target_class)\n",
    "    outputs = model(batch_dict[\"img\"].to(device))\n",
    "    prediction = torch.argmax(outputs.data, axis=1)\n",
    "    for i in range(len(gradients)):\n",
    "        plt.imshow(batch_dict[\"img\"][i, 0], cmap=\"gray\")\n",
    "        if v_display is None:\n",
    "            v = max(-gradients.min(), gradients.max())\n",
    "        else:\n",
    "            v = v_display\n",
    "        plt.imshow(gradients[i, 0], alpha=0.5, vmin=-v, vmax=v, cmap=\"bwr\")\n",
    "        plt.title(f\"Label={sample['label'][i]}, prediction={prediction[i]}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0db8b9ec",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tutorial_6/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/4288390740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCacheDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_sample_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLoadChestData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvisualize_grad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1260672806.py\u001b[0m in \u001b[0;36mbuild_sample_list\u001b[0;34m(data_path, mode)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfile_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msample_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tutorial_6/test'"
     ]
    }
   ],
   "source": [
    "test_dataset = monai.data.CacheDataset(build_sample_list(data_path, mode=\"test\"), transform=LoadChestData())\n",
    "test_loader = monai.data.DataLoader(test_dataset, batch_size=16)\n",
    "for sample in test_loader:\n",
    "    visualize_grad_cam(sample, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b13f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "class MaskingTransform(monai.transforms.Transform):\n",
    "    \"\"\"This transform applies a binary mask of the same size as the image it is applied to.\"\"\"\n",
    "    def __init__(self, mask_pt, label=None, value=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mask_pt (Tensor): a binary mask that will be applied to occlude an image.\n",
    "            label (int): if given, transform will only be performed on images with the given label.\n",
    "                Default will transform all images.\n",
    "            value (float): constant value used to perturb the image.\n",
    "        \"\"\"\n",
    "        self.label = label\n",
    "        self.value = value\n",
    "        self.mask_pt = mask_pt.float()\n",
    "        self.invert_mask_pt = self.invert_mask(self.mask_pt)\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        sample = copy(sample)\n",
    "        \n",
    "        if self.label is None or self.label == sample[\"label\"]:\n",
    "            image = sample[\"img\"] * self.invert_mask_pt + self.mask_pt * self.value\n",
    "            sample[\"img\"] = image\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def invert_mask(pt):\n",
    "        negative_image = -pt + 1\n",
    "        return (negative_image - negative_image.min()) / (negative_image.max() - negative_image.min())\n",
    "\n",
    "    \n",
    "class CropResizeTransform(monai.transforms.Transform):\n",
    "    \"\"\"This transform crop a region of interest and resize the image to its initial size.\"\"\"\n",
    "    def __init__(self, roi_center, roi_size, label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            roi_center (Tuple[int, int]): coordinates of the center of the region of interest.\n",
    "            roi_size (Tuple[int, int]): size of the region of interest.\n",
    "            label (int): if given, transform will only be performed on images with the given label.\n",
    "                Default will transform all images.\n",
    "        \"\"\"\n",
    "        self.label = label\n",
    "        self.crop_transform = monai.transforms.SpatialCrop(roi_center=roi_center, roi_size=roi_size)\n",
    "        self.resize_transform = monai.transforms.Resize((512, 512), mode=\"bilinear\")\n",
    "        \n",
    "    def __call__(self, sample):        \n",
    "        sample = copy(sample)\n",
    "        \n",
    "        if self.label is None or self.label == sample[\"label\"]:\n",
    "            image = self.resize_transform(self.crop_transform(sample[\"img\"]))\n",
    "            sample[\"img\"] = image\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9344519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveShoulders(MaskingTransform):\n",
    "    def __init__(self, label=None):\n",
    "        mask_pt = torch.zeros((1, 512, 512))\n",
    "        mask_pt[:, :100, :200] = 1\n",
    "        mask_pt[:, :100, 312:] = 1\n",
    "        super().__init__(mask_pt, label)\n",
    "    \n",
    "\n",
    "class AddSideBackground(MaskingTransform):\n",
    "    def __init__(self, label=None):\n",
    "        mask_pt = torch.zeros((1, 512, 512))\n",
    "        mask_pt[:, 150:, :50] = 1\n",
    "        mask_pt[:, 150:, 512 - 50:] = 1\n",
    "        super().__init__(mask_pt, label)\n",
    "    \n",
    "    \n",
    "class CropSideBackground(CropResizeTransform):\n",
    "    def __init__(self, label=None):\n",
    "        roi_center = (256, 256)\n",
    "        roi_size = (512, 412)\n",
    "        super().__init__(roi_center, roi_size, label)\n",
    "    \n",
    "\n",
    "class CropShouldersUp(CropResizeTransform):\n",
    "    def __init__(self, label=None):\n",
    "        roi_center = (312, 256)\n",
    "        roi_size = (412, 512)\n",
    "        super().__init__(roi_center, roi_size, label)\n",
    "\n",
    "    \n",
    "class RWriter(MaskingTransform):\n",
    "    def __init__(self, label=None, upsampling=3):\n",
    "        from PIL import Image, ImageDraw, ImageFont\n",
    "        \n",
    "        size = 512\n",
    "        h_offset = 120\n",
    "        v_offset = 60\n",
    "\n",
    "        # Create black image with white R letter\n",
    "        image = Image.new(\"L\", (size // upsampling, size // upsampling)) # As the size of the font cannot be easily chosen, a smaller image is created and the resizing will increase the size of the font\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.text((h_offset // upsampling, v_offset // upsampling), \"R\", fill=\"white\")\n",
    "        image = image.resize((size, size))\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        mask_pt = torch.from_numpy(np.asarray(image)).float() / 255\n",
    "        super().__init__(mask_pt, label, value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f659070",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/2835570252.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRemoveShoulders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAddSideBackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCropSideBackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCropShouldersUp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRWriter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "transforms = [RemoveShoulders, AddSideBackground, CropSideBackground, CropShouldersUp, RWriter]\n",
    "\n",
    "print(\"Original image\")\n",
    "visualize_sample(sample)\n",
    "\n",
    "for transform in transforms:\n",
    "    print(f\"Transform {transform.__name__}\")\n",
    "    transformed_sample = transform()(sample)\n",
    "    visualize_sample(transformed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d296f8",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Tutorial_6/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/3838921941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataset = monai.data.CacheDataset(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbuild_sample_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     transform=monai.transforms.Compose(\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mLoadChestData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1260672806.py\u001b[0m in \u001b[0;36mbuild_sample_list\u001b[0;34m(data_path, mode)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfile_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msample_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Tutorial_6/test'"
     ]
    }
   ],
   "source": [
    "test_dataset = monai.data.CacheDataset(\n",
    "    build_sample_list(data_path, mode=\"test\"), \n",
    "    transform=monai.transforms.Compose(\n",
    "        [LoadChestData()]\n",
    "    )\n",
    ")\n",
    "test_loader = monai.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "confusion_df = compute_confusion_matrix(test_loader, model)\n",
    "print(\"Original confusion matrix\")\n",
    "print(confusion_df)\n",
    "\n",
    "test_dataset = monai.data.CacheDataset(\n",
    "    build_sample_list(data_path, mode=\"test\"), \n",
    "    transform=monai.transforms.Compose(\n",
    "        [LoadChestData(), AddSideBackground(), RemoveShoulders(), RWriter()]\n",
    "    )\n",
    ")\n",
    "test_loader = monai.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "confusion_df = compute_confusion_matrix(test_loader, model)\n",
    "print(\"Post-transforms confusion matrix\")\n",
    "print(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3c4baf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open Tutorial_3.zip, Tutorial_3.zip.zip or Tutorial_3.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://surfdrive.surf.nl/files/index.php/s/Y4psc2pQnfkJuoT/download -O Tutorial_3.zip\n",
    "!unzip -qo Tutorial_3.zip\n",
    "data_path = \"ribs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efcd13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def build_dict_ribs(data_path, mode='train'):\n",
    "    \"\"\"\n",
    "    This function returns a list of dictionaries, each dictionary containing the keys 'img' and 'mask' \n",
    "    that returns the path to the corresponding image.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): path to the root folder of the data set.\n",
    "        mode (str): subset used. Must correspond to 'train', 'val' or 'test'.\n",
    "        \n",
    "    Returns:\n",
    "        (List[Dict[str, str]]) list of the dictionaries containing the paths of X-ray images and masks.\n",
    "    \"\"\"\n",
    "    # test if mode is correct\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(f\"Please choose a mode in ['train', 'val', 'test']. Current mode is {mode}.\")\n",
    "    \n",
    "    # define empty dictionary\n",
    "    dicts = []\n",
    "    # list all .png files in directory, including the path\n",
    "    paths_xray = glob.glob(os.path.join(data_path, mode, 'img', '*.png'))\n",
    "    # make a corresponding list for all the mask files\n",
    "    for xray_path in paths_xray:\n",
    "        if mode == 'test':\n",
    "            suffix = 'val'\n",
    "        else:\n",
    "            suffix = mode\n",
    "        # find the binary mask that belongs to the original image, based on indexing in the filename\n",
    "        image_index = os.path.split(xray_path)[1].split('_')[-1].split('.')[0]\n",
    "        # define path to mask file based on this index and add to list of mask paths\n",
    "        mask_path = os.path.join(data_path, mode, 'mask', f'VinDr_RibCXR_{suffix}_{image_index}.png')\n",
    "        if os.path.exists(mask_path):\n",
    "            dicts.append({'img': xray_path, 'mask': mask_path})\n",
    "    return dicts\n",
    "\n",
    "class LoadRibData(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads the data from the rib segmentation dataset.\n",
    "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = Image.open(sample['img']).convert('L') # import as grayscale image\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "        mask = Image.open(sample['mask']).convert('L') # import as grayscale image\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "        # mask has value 255 on rib pixels. Convert to binary array\n",
    "        mask[np.where(mask==255)] = 1\n",
    "        return {'img': image, 'mask': mask, 'img_meta_dict': {'affine': np.eye(2)}, \n",
    "                'mask_meta_dict': {'affine': np.eye(2)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6e3f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1803295577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     ]\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCacheDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_dict_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "validation_dict_list = build_dict_ribs(data_path, mode='val')\n",
    "validation_transform = monai.transforms.Compose(\n",
    "    [\n",
    "        LoadRibData(),\n",
    "        monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "        monai.transforms.HistogramNormalized(keys=['img']),     \n",
    "        monai.transforms.ScaleIntensityd(keys=['img'], minv=0, maxv=1),\n",
    "        monai.transforms.Zoomd(keys=['img', 'mask'], zoom=0.25, mode=['bilinear', 'nearest'], keep_size=False),\n",
    "        # monai.transforms.RandSpatialCropd(keys=['img', 'mask'], roi_size=[384, 384], random_size=False)\n",
    "        monai.transforms.SpatialCropd(keys=['img', 'mask'], roi_center=[300, 300], roi_size=[384 + 64, 384])        \n",
    "    ]\n",
    ")\n",
    "validation_data = monai.data.CacheDataset([validation_dict_list[3]], transform=validation_transform)\n",
    "validation_loader = monai.data.DataLoader(validation_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56679421",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_file = path.join(data_path, \"trainedUNet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3e79f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not found. Using CPU.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ribs/trainedUNet.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/579083867.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m ).to(device)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ribs/trainedUNet.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "\n",
    "# Check whether we're using a GPU\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()  # Total number of GPUs\n",
    "    gpu_idx = random.randint(0, n_gpus - 1)  # Random GPU index\n",
    "    device = torch.device(f'cuda:{gpu_idx}')\n",
    "    print('Using GPU: {}'.format(device))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('GPU not found. Using CPU.')\n",
    "\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels = (8, 16, 32, 64, 128),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(pretrained_file))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0665f434",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/2933356181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_noshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for sample in validation_loader:\n",
    "\n",
    "    img = sample['img'][:, :, :384, :384]    \n",
    "    mask = sample['mask'][:, :, :384, :384]\n",
    "    output_noshift = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()   \n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize = [12, 10])    \n",
    "    # Plot X-ray image\n",
    "    ax[0].imshow(img.squeeze(), 'gray')\n",
    "    # Plot ground truth\n",
    "    mask = np.squeeze(mask)\n",
    "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n",
    "    ax[0].imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    ax[0].set_title('Ground truth')\n",
    "    # Plot output\n",
    "    overlay_output = np.ma.masked_where(output_noshift < 0.1, output_noshift > 0.99)\n",
    "    ax[1].imshow(img.squeeze(), 'gray')\n",
    "    ax[1].imshow(overlay_output.squeeze(), 'Reds', alpha = 0.7, clim=[0,1])\n",
    "    ax[1].set_title('Prediction')\n",
    "    plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5388dc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/1163784073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Original image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_loader' is not defined"
     ]
    }
   ],
   "source": [
    "offset = 1\n",
    "\n",
    "for sample in validation_loader:\n",
    "\n",
    "    # Original image\n",
    "    img = sample['img'][:, :, :384, :384]    \n",
    "    mask = sample['mask'][:, :, :384, :384]\n",
    "    output_noshift = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()   \n",
    "\n",
    "    # Plot X-ray image\n",
    "    fig, ax = plt.subplots(1,2, figsize = [12, 10])    \n",
    "    ax[0].imshow(img.squeeze(), 'gray')\n",
    "    # Plot ground truth\n",
    "    mask = np.squeeze(mask)\n",
    "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n",
    "    ax[0].imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    ax[0].set_title('Ground truth')\n",
    "    # Plot output\n",
    "    overlay_output = np.ma.masked_where(output_noshift < 0.1, output_noshift >0.99)\n",
    "    ax[1].imshow(img.squeeze(), 'gray')\n",
    "    ax[1].imshow(overlay_output.squeeze(), 'Reds', alpha = 0.7, clim=[0,1])\n",
    "    ax[1].set_title('Prediction')\n",
    "    plt.show()\n",
    "    \n",
    "    # Shifted image\n",
    "    img = sample['img'][:, :, offset:offset+384, :384]\n",
    "    mask = sample['mask'][:, :, offset:offset+384, :384]\n",
    "    output = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()\n",
    "\n",
    "    # Plot X-ray image\n",
    "    fig, ax = plt.subplots(1,2, figsize = [12, 10])\n",
    "    ax[0].imshow(img.squeeze(), 'gray')\n",
    "    # Plot ground truth\n",
    "    mask = np.squeeze(mask)\n",
    "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n",
    "    ax[0].imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    ax[0].set_title('Ground truth shifted')\n",
    "    # Plot output\n",
    "    overlay_output = np.ma.masked_where(output < 0.1, output >0.99)\n",
    "    ax[1].imshow(img.squeeze(), 'gray')\n",
    "    ax[1].imshow(overlay_output.squeeze(), 'Reds', alpha = 0.7, clim=[0,1])\n",
    "    ax[1].set_title('Prediction shifted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89e76055",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_noshift' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/2074597593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiffout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_noshift\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'seismic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Offset {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_noshift' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "diffout = output_noshift[offset:, :384] - output[:-offset, :384]\n",
    "plt.imshow(diffout, cmap='seismic', clim=[-1, 1])\n",
    "plt.title('Offset {}'.format(offset))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9972cdde",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_80253/2824012162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_differences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# Set to True to plot difference images for every offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moutput_noshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "norms = []\n",
    "offsets = []\n",
    "plot_differences = False  # Set to True to plot difference images for every offset\n",
    "\n",
    "img = sample['img'][:, :, :384, :384]    \n",
    "mask = sample['mask'][:, :, :384, :384]\n",
    "output_noshift = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()   \n",
    "\n",
    "for offset in range(1, 65):\n",
    "    for sample in validation_loader:\n",
    "        img = sample['img'][:, :, offset:offset+384, :384]\n",
    "        mask = sample['mask'][:, :, offset:offset+384, :384]\n",
    "\n",
    "        output = torch.sigmoid(model(img.to(device))).detach().cpu().numpy().squeeze()  \n",
    "\n",
    "        diffout = (output_noshift[offset:, :384] - output[:-offset, :384])[100:284, 100:284]\n",
    "        offsets.append(offset)\n",
    "        norms.append(np.sum(np.abs(diffout)))\n",
    "        if plot_differences:\n",
    "            plt.figure()\n",
    "            plt.imshow(diffout, cmap='seismic', clim=[-1, 1])\n",
    "            plt.title(f\"Offset {offset}\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(offsets, norms)\n",
    "plt.xlabel('Offset')\n",
    "plt.ylabel('Difference')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}