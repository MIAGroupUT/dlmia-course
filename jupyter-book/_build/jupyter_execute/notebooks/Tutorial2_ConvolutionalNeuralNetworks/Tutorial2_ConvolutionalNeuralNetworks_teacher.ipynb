{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0dfa6f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "# Tutorial 2\n",
    "## May 8, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdd346",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "# Convolution and cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51ebf5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the previous tutorial, you have looked at images. Now, we're going to look at the effect of applying some filters to our image. First, load the image that you used last week.\n",
    "The images used in the different tutorials are available [here](https://surfdrive.surf.nl/files/index.php/s/hHQNmEOYd054V6f).\n",
    "For this tutorial you only need to download the folder \"Tutorial 1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea41c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14fd312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image could not be found. Please check that the file TEV1P1CTI.mhd can be found in \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_file = os.path.join(data_path, r'TEV1P1CTI.mhd')\n",
    "\n",
    "if not os.path.exists(image_file):\n",
    "    print(f'The image could not be found. '\n",
    "          f'Please check that the file TEV1P1CTI.mhd can be found in {data_path}')\n",
    "else:\n",
    "    image = sitk.ReadImage(image_file)\n",
    "    image_array = sitk.GetArrayFromImage(image)\n",
    "    image_array = np.swapaxes(image_array, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19410ecf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To make life simple, we just consider a 2D image, and we downsample the image by a factor 4 so the effect of filters is more clear. There are many ways to do this, here we use the [SciPy](https://docs.scipy.org/doc/scipy/index.html) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427d7b3e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/2552605338.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Only consider one image slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use Scipy to downsample the image by a factor 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_array' is not defined"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage as scnd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Only consider one image slice\n",
    "image_slice = image_array[:, :, 20].squeeze().transpose()\n",
    "\n",
    "# Use Scipy to downsample the image by a factor 4\n",
    "image_slice = scnd.zoom(image_slice, 0.25)\n",
    "\n",
    "plt.title(\"Downsampled slice\")\n",
    "plt.imshow(image_slice, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eaaf60",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Correlation and convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693a812",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we're going to visualize what the difference between convolution and cross-correlation is. You (hopefully) remember from the lecture that convolution of an image $h$ with a kernel $g$ is $f(i,j) = \\sum_m\\sum_n h(i-m, j-n)g(m,n)$ whereas cross-correlation is defined as $f(i,j) = \\sum_m\\sum_n h(i+m, j+n)g(m,n)$. First define a *filter* or *kernel*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb0ec27",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = np.array([[0, 0, 0],\n",
    "              [0, 0, 1],\n",
    "              [0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccb8fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Without running the code below, try to figure out what the kernel will do when applied to an image in either convolution or cross-correlation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615422b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "Imagine that, e.g., $i$ = 10 and $j$ = 10. Then the operator will compute a new value $f(10, 10)$. All kernel elements are 0 except for one, located at $g(0, 1)$. This means that when using convolution, the value at $f(10, 10)$ will be that of $h(10-0, 10-1)=h(10,9)$, so the image is shifting right. When using cross-correlation, the value $f(10, 10)$ will be that of $h(10-0, 10+1)=h(10, 11)$ so the image is shifting left. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2e8c3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "Let's first apply the kernel with cross-correlation `scnd.correlate` and see what it does. We apply the kernel a couple of times so you see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d818d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_slice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/2050052134.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_slice' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_image = image_slice\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "\n",
    "for level in range(20): # The kernel is applied 20 times\n",
    "    filtered_image = scnd.correlate(filtered_image, g, mode='constant', cval=0)\n",
    "    ax.imshow(filtered_image, cmap='gray', clim=[-300, 450])\n",
    "    ax.set_title('Applied {} times'.format(level))\n",
    "    display(fig)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffeade1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "What happens when you use `scnd.correlate` instead of `scnd.convolve`. Try below. Can you explain this behavior?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26ffe3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "The image shifts right. The explanation is in the answer to the previous question. Convolution flips the kernel w.r.t. cross-correlation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38608f6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_slice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/3941258956.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_slice' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_image = image_slice\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for level in range(20): # The kernel is applied 20 times\n",
    "    filtered_image = scnd.convolve(filtered_image, g, mode='constant', cval=0)\n",
    "    ax.imshow(filtered_image, cmap='gray', clim=[-300, 450])\n",
    "    ax.set_title('Applied {} times'.format(level))\n",
    "    display(fig)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ad9da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Of course we're not limited to shifting images left or right, we can also define kernels to, e.g., smooth or sharpen or image. A well-known smoothing kernel is the Gaussian kernel, which resembles a 2D normal distribution. Try to implement the kernel below and apply it once to your image.<br>\n",
    "![img](https://4.bp.blogspot.com/-v4dH8qhFnEE/WqHaTPel8RI/AAAAAAAAI8g/AxIVu5i7mHU5UDcu6BkJQJj_UO11sMomwCLcBGAs/s200/3x3%2BGaussian%2BKernel.png)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122bd6b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "```python\n",
    "g = 1/16 * np.array([[1, 2, 1],\n",
    "                     [2, 4, 2],\n",
    "                     [1, 2, 1]])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "filtered_image = scnd.convolve(image_slice, g, mode='constant', cval=0)\n",
    "ax.imshow(filtered_image, cmap='gray', clim=[-300, 450])\n",
    "ax.set_title('Convolve')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "filtered_image = scnd.correlate(image_slice, g, mode='constant', cval=0)\n",
    "ax.imshow(filtered_image, cmap='gray', clim=[-300, 450])\n",
    "ax.set_title('Cross-correlate')\n",
    "plt.show()\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d52d59",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "What happens if you use correlate, and what happens if you use convolve?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb5431",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso \n",
    "The image gets blurred. In this case, it doesn't matter which operator your use because the kernel is symmetric.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755f28a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "## PyTorch\n",
    "![PyTorch](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png?20211003060202)<br>\n",
    "[PyTorch](https://pytorch.org/) is (along with TensorFlow) one of the two most commonly used Python libraries for deep learning and linear algebra on GPUs. Before we jump into any real deep learning, there is a couple of things that you should know. Like NumPy, PyTorch operates on matrices. However, while NumPy calls these *arrays*, PyTorch calls them *tensors*. We can easily go back and forth between NumPy arrays and PyTorch tensors. See for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381c0511",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[1 2 3]\n",
      "<class 'torch.Tensor'>\n",
      "tensor([1, 2, 3])\n",
      "<class 'numpy.ndarray'>\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Start with a NumPy array\n",
    "a = np.array([1, 2, 3])\n",
    "print(type(a))\n",
    "print(a)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "a_t = torch.from_numpy(a)\n",
    "print(type(a_t))\n",
    "print(a_t)\n",
    "\n",
    "# Convert back to NumPy array\n",
    "a = a_t.numpy()\n",
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff7afa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "PyTorch is supported on CPUs, GPUs, TPUs and (newer) MacBooks. You can set your device as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909cfe93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # CUDA\n",
    "    gpu = torch.device('cuda:0')\n",
    "else:\n",
    "    # MacBook with >M1 chip\n",
    "    gpu = torch.device('mps')\n",
    "\n",
    "try:\n",
    "    a_t = a_t.to(device=gpu)\n",
    "except Exception:\n",
    "    print(\"No GPU was found on your machine.\"\n",
    "          \"Use colab or JupyterLab to access a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290b3e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the tensor is now on the GPU, so we cannot add a tensor that's on the CPU to it. For example, in the following line we initialize a new tensor on the CPU and try to add it but we get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f1c4d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/1424124837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "summed = a_t + torch.Tensor([4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc20f92",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Try to fix this bug so you can add the tensors on the GPU.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5a41b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "You'll also have to move the new `[4, 5 6]` tensor to the GPU.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6f943a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "teacher"
    ]
   },
   "outputs": [],
   "source": [
    "summed = a_t + torch.Tensor([4, 5, 6]).to(device=gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cdf867",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "However, best practice is to initialize your tensors and objects directly on the GPU. For example, the code below will generate a random $1000\\times 1000$ matrix on the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d04225e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.5733e-01,  8.3332e-01,  1.6799e+00,  ..., -2.5142e+00,\n",
      "          2.0756e-01, -1.1494e+00],\n",
      "        [ 2.5139e-01, -1.0730e+00, -5.9246e-01,  ...,  7.0610e-04,\n",
      "         -4.5357e-01, -1.9172e+00],\n",
      "        [ 8.1169e-01,  1.7883e+00, -7.0340e-01,  ...,  9.0216e-01,\n",
      "         -1.2327e+00, -8.7903e-01],\n",
      "        ...,\n",
      "        [ 5.1994e-01, -1.6500e+00,  2.3617e-01,  ..., -6.8722e-01,\n",
      "         -1.2570e+00,  2.0804e-01],\n",
      "        [-6.2757e-01,  5.1785e-01, -1.7138e-01,  ..., -1.6879e-02,\n",
      "          6.6434e-01,  1.1214e+00],\n",
      "        [ 1.8146e+00,  4.1044e-01,  6.0555e-01,  ...,  1.5129e+00,\n",
      "          6.7483e-01,  4.6970e-01]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/torch/_tensor_str.py:116: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)\n"
     ]
    }
   ],
   "source": [
    "rn = torch.randn(1000, 1000, device=gpu)\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6e4f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's perform *convolution* in PyTorch! For this, we use the [<code>conv2d</code>](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function. The code below is not very different from what we have done before, but now everything happens in PyTorch instead of NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e3da2ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_slice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/1120085648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage_slice_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m g_t = torch.tensor([[0, 0, 0],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_slice' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "image_slice_t = torch.from_numpy(image_slice).float()\n",
    "\n",
    "g_t = torch.tensor([[0, 0, 0],\n",
    "                    [0, 0, 1],\n",
    "                    [0, 0, 0]]).float()\n",
    "\n",
    "# We reshape the image because the conv2d function expects [batch_size, channels, width, height] inputs and kernels. You'll see \n",
    "# later what we mean with batch_size and channels.\n",
    "filtered_image_t = F.conv2d(image_slice_t.reshape((1, 1, 128, 128)), g_t.reshape((1, 1, 3, 3)), padding='same').squeeze()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "for level in range(1, 20):\n",
    "    filtered_image_t = F.conv2d(filtered_image_t.reshape((1, 1, 128, 128)), g_t.reshape((1, 1, 3, 3)), padding='same').squeeze()\n",
    "    ax.imshow(filtered_image_t.numpy(), cmap='gray', clim=[-300, 450])\n",
    "    ax.set_title('Applied {} times'.format(level))\n",
    "    display(fig)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596cc70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "And, what do you see? A convolution or a cross-correlation? (Why) is this surprising?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43dc5bd",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "A cross-correlation, the image is shifting to the left. This means that PyTorch does cross-correlation instead of convolution if we call `conv2d`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db455a83",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "All kernels that we have used so far are $3\\times 3$ elements, but this is an arbitrary choice. Can you implement a kernel that lets the image make a [horse jump](https://en.wikipedia.org/wiki/Knight_(chess)) to the top right? Implement the kernel and apply with PyTorch.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74601e6a",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "There are two ways to do this. The first approach is to define a kernel of $5\\times 5$ elements with one non-zero value and apply that kernel once. The second approach is to define two kernels of $3\\times 3$ elements, one for right-shifting and one for up-shifting. Then you can apply the right-shift once, and the up-shift twice.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "792f175a",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_slice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/3457085804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_slice_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### APPROACH ONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m g_t = torch.tensor([[0, 0, 0, 0, 0],\n\u001b[1;32m      5\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_slice' is not defined"
     ]
    }
   ],
   "source": [
    "image_slice_t = torch.from_numpy(image_slice).float()\n",
    "\n",
    "### APPROACH ONE\n",
    "g_t = torch.tensor([[0, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 0, 0],\n",
    "                    [0, 1, 0, 0, 0]]).float()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "filtered_image_t = F.conv2d(image_slice_t.reshape((1, 1, 128, 128)), g_t.reshape((1, 1, 5, 5)), padding='same').squeeze()\n",
    "ax.imshow(filtered_image_t.numpy(), cmap='gray', clim=[-300, 450]);  \n",
    "ax.set_title('In one step')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### APPROACH TWO\n",
    "g_right = torch.tensor([[0, 0, 0],\n",
    "                        [1, 0, 0],\n",
    "                        [0, 0, 0]]).float()\n",
    "\n",
    "g_up = torch.tensor([[0, 0, 0],\n",
    "                        [0, 0, 0],\n",
    "                        [0, 1, 0]]).float()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "filtered_image_t = F.conv2d(image_slice_t.reshape((1, 1, 128, 128)), g_up.reshape((1, 1, 3, 3)), padding='same').squeeze()\n",
    "filtered_image_t = F.conv2d(filtered_image_t.reshape((1, 1, 128, 128)), g_up.reshape((1, 1, 3, 3)), padding='same').squeeze()\n",
    "filtered_image_t = F.conv2d(filtered_image_t.reshape((1, 1, 128, 128)), g_right.reshape((1, 1, 3, 3)), padding='same').squeeze()\n",
    "ax.imshow(filtered_image_t.numpy(), cmap='gray', clim=[-300, 450]);  \n",
    "ax.set_title('In three steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca442f16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Learning parameters from synthetic data\n",
    "In this part of the tutorial, we will define a very simple optimization problem and how to tackle it with PyTorch.\n",
    "In the next part of the tutorial, we will use MONAI, which is a PyTorch extension for medical images.\n",
    "\n",
    "Like in the lecture, we will solve a linear regression problem with one independent input variable $x$ and one\n",
    "dependent output variable $y$. The output variable is a function of the input, i.e.\n",
    "\n",
    "$$y = wx + b$$\n",
    "\n",
    "The measurements contain some noise, and thus, we're looking at an optimization problem\n",
    "\n",
    "$$\\underset{w,b}{arg\\,min}\\frac{1}{m}\\sum_{i}^m (wx_i+b-y_i)^2$$\n",
    "\n",
    "In this example, we will perform basic linear regression using PyTorch.\n",
    "First, we will generate our 'dataset', consisting of $x$ and $y$ values sampled along a line (with some added noise).\n",
    "We here use PyTorch (torch) functions to initialize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16de64b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation of a data set including N samples\n",
    "N = 50\n",
    "x = torch.linspace(0, 4, N, device=gpu)\n",
    "b = 4\n",
    "w = 2\n",
    "noise = torch.normal(torch.zeros(N)).to(gpu)\n",
    "y = (w * x + b + noise).float()\n",
    "\n",
    "# Plot the synthetic data set\n",
    "plt.figure()\n",
    "plt.title(\"Synthetic data\")\n",
    "plt.scatter(x.cpu(), y.cpu(), color='r') # We use this code to map the data back to the CPU\n",
    "plt.xlim(-1, 5)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylim(0, 15)\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a23da",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we want to learn a linear regression, so we don't need a deep neural network with non-linear activations,\n",
    "but only one fully-connected layer. In PyTorch, this is called a\n",
    "[Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer,\n",
    "as it learns a linear relation between inputs and outputs.\n",
    "Take a look at the documentation to see which arguments this layer has.\n",
    "\n",
    "The arguments we define are the following:\n",
    "1. The **number of input features**, which is in this case 1, i.e., x.\n",
    "2. The **number of output features**, which is in this case also 1, i.e., y.\n",
    "3. [Optional] We choose to define whether or not the layer has a **bias term**. By default, this is set to True.\n",
    "4. [Optional] We define here which **device** is used to instantiate the layer. In our case, we use a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e01568d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(in_features=1, out_features=1, bias=True, device=gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b24b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Like any neural network in PyTorch, training this network requires an loss function. Because this is a regression problem, we use a mean squared error loss, or [<code>MSELoss</code>](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) in PyTorch, with default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01722a78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1f8f1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "We now define a simple training procedure to fit a model to a dataset. There are a few *very* important steps in this piece of code. \n",
    "\n",
    "- First, we create the optimizer, which is an essential part of training the model. In this case, we use an [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer, which is the gradient descent optimizer that you saw in the lecture. The optimizer keeps track of the parameters of the model and is initialized with a learning rate.\n",
    "- Second, we loop over a number of iterations and in each iteration we perform the following steps\n",
    "    - `optimizer.zero_grad()` resets all gradients of the model parameters. If we wouldn't do this, gradients would be added up over iterations.\n",
    "    - `output = model(x.unsqueeze(1))` applies the network to the input sample.\n",
    "    - `loss = loss_function(output, y.unsqueeze(1))` computes the MSE loss between the output of the network and the target, i.e., $y$.\n",
    "    - `loss.backward()` backpropagates the prediction loss to all parameters.\n",
    "    - `optimizer.step()` applied gradient descent the parameters by the gradients collected in the backward pass.\n",
    "    \n",
    "These few lines are at the core of anything you'll do in PyTorch. Note that this is only a very short description of the process, if you want to know more, take a look at [this tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cae065",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train(model, iterations, lr, loss_function, x, y):\n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    for iteration in tqdm.tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x.unsqueeze(1))\n",
    "        loss = loss_function(output, y.unsqueeze(1))\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921e9b37",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Does this piece of code use batch gradient descent, mini-batch gradient descent or stochastic gradient descent?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c6e93",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: tip\n",
    "This function uses the full dataset `x` in each iteration and is thus performing batch gradient descent.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684caac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Train the model for 1000 iterations with a learning rate of 0.001 and visualize the loss over time. What happens to the loss curve if you increase or decrease the learning rate (i.e. in steps of factor 10)?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b33b24",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "Setting the learning rate higher will let the loss go down faster, setting the learning rate lower will lead to slower optimization. However, if you set, e.g., `lr`=1 or `lr`=10, you'll see that the loss explodes at some point. \n",
    "\n",
    "```python\n",
    "model_trained, loss = train(model=model, iterations=1000, lr=0.001, loss_function=loss_func, x=x, y=y)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.show()\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ed3e99a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_trained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/3566162013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_trained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_trained' is not defined"
     ]
    }
   ],
   "source": [
    "# Using the code below, you can visualize the fitted line. \n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model_trained(x.unsqueeze(1))\n",
    "plt.plot(x.cpu(), output.cpu(), label='prediction')\n",
    "plt.scatter(x.cpu(), y.cpu(), color='r', label='training data')\n",
    "plt.xlim(-1, 5)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylim(0, 15)\n",
    "plt.ylabel('$y$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8498bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Remember how when we defined `model` we set `bias` to `True`? Try to run the code again but without bias. What do you notice? The following line counts the number of trainable parameters in a model. What's the difference between the model with and without bias?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5adede33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This block counts the number of parameters in the model.\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c69d30",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "If the model doesn't have a bias, the fitted line starts at (0, 0) and it will underestimate the points close to $x=0$. This is not a good model for the data.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8161ef76",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model with bias has 2 parameters\n",
      "The model without bias has 1 parameters\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(in_features=1, out_features=1, bias=True, device=gpu)\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('The model with bias has {} parameters'.format(n_params))\n",
    "\n",
    "model = torch.nn.Linear(in_features=1, out_features=1, bias=False, device=gpu)\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('The model without bias has {} parameters'.format(n_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7c9a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "# A convolutional neural network\n",
    "\n",
    "![MONAI](https://monai.io/assets/img/MONAI-logo_color_full.png)\n",
    "\n",
    "Medical Open Network for Artificial Intelligence ([MONAI](https://monai.io/)) is the library we will use in this course for deep learning on medical images. MONAI is built on the same concepts as PyTorch and its classes in many cases inherit from PyTorch classes. However, MONAI has specific functions and models that make it appropriate for medical images. The 'raw' loaded medical images are often not directly suited for training a neural network; they need to be processed before being used. MONAI offers a streamlined framework for transforming the data and feeding it into the network. In practice, you will use a combination of the two. A typical workflow in both PyTorch and MONAI is as follows:\n",
    "1. Define a dataset\n",
    "2. Define transforms on the dataset to make it suitable for the network\n",
    "3. Build a dataloader\n",
    "4. Build the network / define the optimizer, loss function and hyperparameters\n",
    "5. Start training procedure \n",
    "\n",
    "In this tutorial, we build a classification network for tumor classification, using datasets from [MedMNIST](https://medmnist.com/). This is a collection of datasets with binary (yes or no) or multiclass labels in 2D or 3D. Each sample in these datasets is a $28\\times 28$ pixel (2D) or $28\\times 28\\times 28$ voxel (3D) image. To be able to use MONAI and this repository, we install the corresponding Python package using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515dec59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (1.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.17 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from monai) (1.21.5)\r\n",
      "Requirement already satisfied: torch>=1.8 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from monai) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from torch>=1.8->monai) (4.4.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: medmnist in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (4.66.2)\r\n",
      "Requirement already satisfied: torch in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (1.13.1)\r\n",
      "Requirement already satisfied: torchvision in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (0.14.1)\r\n",
      "Requirement already satisfied: pandas in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (1.3.5)\r\n",
      "Requirement already satisfied: fire in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (0.6.0)\r\n",
      "Requirement already satisfied: numpy in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (1.21.5)\r\n",
      "Requirement already satisfied: Pillow in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (9.4.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (1.0.2)\r\n",
      "Requirement already satisfied: scikit-image in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from medmnist) (0.19.3)\r\n",
      "Requirement already satisfied: termcolor in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from fire->medmnist) (2.3.0)\r\n",
      "Requirement already satisfied: six in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from fire->medmnist) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from pandas->medmnist) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from pandas->medmnist) (2022.7)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image->medmnist) (2.19.3)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image->medmnist) (2.6.3)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image->medmnist) (1.7.3)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image->medmnist) (2021.7.2)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image->medmnist) (1.3.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image->medmnist) (22.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib>=0.11 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-learn->medmnist) (1.1.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-learn->medmnist) (2.2.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from torch->medmnist) (4.4.0)\r\n",
      "Requirement already satisfied: requests in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from torchvision->medmnist) (2.28.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests->torchvision->medmnist) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests->torchvision->medmnist) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests->torchvision->medmnist) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests->torchvision->medmnist) (3.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install monai\n",
    "!pip install medmnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb646bf",
   "metadata": {},
   "source": [
    "After you've installed the `medmnist` package, import it and download the `train` segment of the pneumonia dataset. This is a binary task where patients either have pneumonia or don't. Note that there is also a `validate` and `test` segment that we will download later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecbf9235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/jmwolterink/.medmnist/pneumoniamnist.npz\n"
     ]
    }
   ],
   "source": [
    "import medmnist\n",
    "dataset = medmnist.PneumoniaMNIST(split=\"train\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c792b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets\n",
    "The first step in the procedure consists of building a dataset that contains the images and labels. A dataset is an object, that we build using the PyTorch <code>Dataset</code> class template. Remember the <code> Car </code> class from last tutorial, with class-specific functions accelerate and brake? Here, we build a custom dataset object, where we need to define three functions in the dataset:\n",
    "- `__init__`: runs when constructing the object, place where you can construct paths to the data and save them in an array, or load the images in an array.\n",
    "- `__len__`: returns length of the dataset, i.e. number of samples.\n",
    "- `__getitem__`: returns a sample from the dataset. This function takes an index as input, and returns the corresponding sample in the dataset. In our case, this would be two objects: an image and its corresponding label. This function returns a dictionary containing image and label. The advantage of using a dictionary is that you can keep track of transforms on the data later on in the pipeline.\n",
    "\n",
    "The `MedMNISTData` class below inherits from the MONAI `Dataset` class, which in turn inherits from the PyTorch `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b04ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import monai\n",
    "\n",
    "class MedMNISTData(monai.data.Dataset):\n",
    "    \n",
    "    def __init__(self, datafile, transform=None):\n",
    "        self.data = datafile\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Make getitem return a dictionary with keys ['img', 'label'] for the image and label respectively\n",
    "        image = self.data[index][0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {'img': image, 'label': self.data[index][1]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7015baf9",
   "metadata": {},
   "source": [
    "Note that in `__getitem__` we apply a transform to each image that is selected. Such a transform is simply a function or a composition of functions that can be applied to a sample to transform it into another sample. This typically includes things like intensity normalization, cropping, resampling, etc. and are essential to most deep learning pipelines in medical imaging. Here we compose two simple transforms:\n",
    "- The `ToTensor` transform converts an image into a PyTorch `Tensor` type\n",
    "- The `Normalize` transform normalizes the intensities of an image to mean=0.5 and std=0.5 (in this case)\n",
    "\n",
    "MONAI and other Python libraries like [TorchIO](https://torchio.readthedocs.io/) contain many convenient functions to transform your images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865737a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0536f3",
   "metadata": {},
   "source": [
    "Then, using the cell below, we can initialize a `Dataset` object based on the data that we have just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61084369",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MedMNISTData(dataset, transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf7f4f",
   "metadata": {},
   "source": [
    "The code below visualizes the first sample from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc3e98ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_dataset[0]['img'].squeeze(), cmap='gray')\n",
    "plt.title('Label {}'.format(train_dataset[0]['label']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3f2c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Visualize a number of random samples from the dataset. What is their corresponding label?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03d144",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "```python\n",
    "n_samples = 16\n",
    "n_row = int(np.sqrt(n_samples))\n",
    "n_col = int(np.sqrt(n_samples))\n",
    "fig, axs = plt.subplots(n_row, n_col, figsize=(10, 12)) \n",
    "for r in range(n_row):\n",
    "    for c in range(n_col):\n",
    "        axs[r, c].imshow(train_dataset[r * n_col + c]['img'].squeeze(), cmap='gray')\n",
    "        axs[r, c].set_title('Label = {}'.format(train_dataset[r * n_col + c]['label']))\n",
    "plt.show()\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7211c8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Make a `val_dataset` and a `test_dataset` class for the validation and test set, respectively. Verify that these have 524 and 624 samples.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4115da3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "```python\n",
    "dataset_val_raw = medmnist.PneumoniaMNIST(split=\"val\", download=True)\n",
    "val_dataset = MedMNISTData(dataset_val_raw, transform=data_transform)\n",
    "dataset_test_raw = medmnist.PneumoniaMNIST(split=\"test\", download=True)\n",
    "test_dataset = MedMNISTData(dataset_test_raw, transform=data_transform)\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e315849",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "## Dataloaders\n",
    "\n",
    "Now that we have a dataset, we can build a dataloader that automatically generates batches with given size. As input it uses one of the data sets constructed in the previous section. Here, the batch size is 32. We shuffle all samples so that each minibatch is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c90313bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = monai.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02591c3b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Also initialize a [dataloader](https://docs.monai.io/en/stable/data.html#dataloader) for the validation set and the test set. Call these `val_dataloader` and `test_dataloader`\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba07b53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "```python\n",
    "val_dataloader = monai.data.DataLoader(val_dataset, batch_size=1000)\n",
    "test_dataloader = monai.data.DataLoader(test_dataset, batch_size=1000)\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f402ca5",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "With these settings, will this dataloader support batch gradient descent, mini-batch gradient descent or stochastic gradient descent?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e425b",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "Mini-batch gradient descent. The DataLoader samples mini-batches of 32 images per iteration.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d22f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The network\n",
    "Now our data pre-processing is all set up, we can define our neural network architecture. It is conventional to code neural networks as a <code>torch.nn.Module</code> object, where the <code>init</code> function defines the model's layers, and the <code>forward</code> function defines how data is fed through the network.\n",
    "Note that this network is built up from convolution layers, pooling layers, and the (fully connected) linear layers that we just used in the PyTorch section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20872b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(in_features=9216, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d08075",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "How many layers does this network have? And how many weights?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c42a5",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "The network has\n",
    "- One input layer\n",
    "- Two convolution layers\n",
    "- One max pooling layer\n",
    "- Two linear layers, of which the last one is the output layer\n",
    "\n",
    "In total,  the network has six layers. Note that ReLU is just the activation functions of the convolution and linear layers, and that Flatten simply performs a flattening operation on the data.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58da3628",
   "metadata": {
    "tags": [
     "teacher"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198721"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70a32c",
   "metadata": {},
   "source": [
    "Move the model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ee37673",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Net().to(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43712a9c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train our network, we should again select an **optimizer** and a **loss** function. \n",
    "- A very common option is to use the Adam optimizer, which is an adaptive version of stochastic gradient descent. \n",
    "- Because we are solving a classification problem, a good loss function is binary cross-entropy. Here, we use `BCEWithLogitsLoss`, which operates directly on the output of the network (its logits) and integrates a sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df58d186",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) \n",
    "loss_function = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31861e10",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training procedure for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161ec67",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we have all the ingredients in place to start training our network. Easiest is to define a train function that performs the training procedure. In addition to a forward and backward pass with the training data, we want to perform a validation loop every $N$ epochs, using unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60fe1c04",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_medmnist(model, train_dataloader, val_dataloader, optimizer, epochs, device=gpu, val_freq=1):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        steps = 0\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            images = batch['img'].float().to(device)\n",
    "            labels = batch['label'].float().to(device)\n",
    "            output = model(images) \n",
    "            loss = loss_function(output, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "           \n",
    "        train_loss.append(epoch_loss/steps)\n",
    "\n",
    "        # validation loop\n",
    "        if epoch % val_freq == 0:\n",
    "            steps = 0\n",
    "            val_epoch_loss = 0\n",
    "            model.eval()\n",
    "            for batch in val_dataloader:\n",
    "                images = batch['img'].float().to(device)\n",
    "                labels = batch['label'].float().to(device)\n",
    "                output = model(images) \n",
    "                loss = loss_function(output, labels)\n",
    "                val_epoch_loss += loss.item()\n",
    "                steps += 1\n",
    "            val_loss.append(val_epoch_loss/steps)\n",
    "\n",
    "    return train_loss, val_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc3523",
   "metadata": {},
   "source": [
    "Train your (maybe?) first convolutional neural network! Time to get a coffee, this might take a few minutes.<br> <img src=\"https://i.redd.it/j586af7nxvu41.jpg\" width=\"400px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98e2ed0e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/4140317621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_medmnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "val_freq = 10\n",
    "n_epochs = 100\n",
    "train_loss, val_loss, model = train_medmnist(model, train_dataloader, val_dataloader, optimizer, epochs=n_epochs, val_freq=val_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277af68",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Visualize the training and validation loss using Matplotlib. What do you see? Did your model overfit?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df778bd",
   "metadata": {
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "```python\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(np.linspace(0, n_epochs, int(n_epochs/val_freq)), val_loss, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "The training loss keeps dropping, but the validation/test loss only drops a bit before it starts to rapidly increase. The model is overfitting on the training data. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56a875",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 5e: Performance evaluation\n",
    "By now, you have a trained version of the CNN, that should be able to distinguish between (very small) X-ray images showing pneumonia or not. For classification tasks, the performance is often measured in terms of specificity and sensitivity, as shown in the diagram below:\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Sensitivity_and_specificity_1.01.svg/330px-Sensitivity_and_specificity_1.01.svg.png />\n",
    "\n",
    "For this model, we will make an ROC curve to assess the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82fec116",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a8ad432",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation_results_ROC(model, dataloader):\n",
    "    for data in dataloader:\n",
    "        images = data['img'].float().to(gpu)\n",
    "        labels = data['label']\n",
    "        with torch.no_grad():\n",
    "            output = model(images).cpu()\n",
    "        fpr, tpr, _ = roc_curve(labels, output.squeeze(1))\n",
    "        print(auc(fpr, tpr))\n",
    "        plt.plot(fpr,tpr)\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('False negative rate')\n",
    "        plt.plot([0,1], [0,1], 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef908be9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Does the model perform well for pneumonia classification? Calculate the area under the curve for the validation set as well as for the test set.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b8be2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "teacher"
    ],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Answer\n",
    ":class: seealso\n",
    "```python\n",
    "validation_results_ROC(model, val_dataloader)\n",
    "validation_results_ROC(model, test_dataloader)\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad8c7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "For improvement of the model, it is useful to visualize some inputs and the corresponding predictions of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f8f8cdb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validation_results_visualize(model, dataset):\n",
    "    index = np.random.randint(0, len(dataset))\n",
    "    image = dataset[index]['img']\n",
    "    plt.imshow(image.numpy().squeeze(), cmap='gray')\n",
    "    image = image.float().to(gpu)\n",
    "    label = dataset[index]['label'].item()\n",
    "    with torch.no_grad():\n",
    "        output = F.sigmoid(model(image.unsqueeze(0)))\n",
    "    plt.title(f'Ground truth: {label}, prediction: {int(output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "394209b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49704/1104208159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_results_visualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "validation_results_visualize(model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7caf21",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Select a 3D dataset from MedMNIST (NoduleMNIST3D or VesselMNIST3D) and train and validate a model for that set. Consider which changes you have to make.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}