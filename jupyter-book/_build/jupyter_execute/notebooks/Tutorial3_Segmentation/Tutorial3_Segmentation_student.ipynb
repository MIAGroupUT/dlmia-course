{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116a5a26",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Tutorial 3\n",
    "## May 16, 2024\n",
    "\n",
    "In the last two tutorials, you've taken your first steps in PyTorch and trained your first (convolutional) neural networks. You know now what convolution and cross-correlation do and what the essential steps in training a network are.\n",
    "\n",
    "This week, you will train convolutional neural networks for 2D image segmentation. Everything you do and learn here can be easily applied to 3D images. The data that we will be using today consists of chest X-ray images. The end goal in this dataset is to segment the ribs in these images, as in the image below. However, we will start a bit simpler and just segment every pixel that is a rib. \n",
    "\n",
    "<img src=\"https://vindr.ai/wp-content/uploads/2021/06/xrib-1.png.pagespeed.ic.yhPrlYfYEN.webp\" width=600px></img>\n",
    "\n",
    "First, we set the data path. You can get the required images by downloading the folder \"Tutorial 3\" [here](https://surfdrive.surf.nl/files/index.php/s/vnA6J0xAhyzoYuz) and uploading the folder to your coding environment. \n",
    "\n",
    ":::{admonition} Direct download\n",
    ":class: attention\n",
    "You can also directly download the file to your programming environment. If you are using the UT Jupyter server (as most of you are), open a terminal window on the server and run\n",
    "\n",
    "`wget -O ribs.zip https://surfdrive.surf.nl/files/index.php/s/Y4psc2pQnfkJuoT/download`. This will download the file `ribs.zip`. Then, unzip this file by running `unzip ribs.zip` and you should find all image files on the server.\n",
    ":::\n",
    "\n",
    "Set the data path to the \"ribs\" folder in the package that you have just downloaded in the code block beneath. If you're running this on Colab, don't forget to set a GPU runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ead260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è add path:\n",
    "data_path = \"../data/Tutorial_3/ribs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe596ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please update your data path to an existing folder.\n"
     ]
    }
   ],
   "source": [
    "# check if data_path exists:\n",
    "import os\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Please update your data path to an existing folder.\")\n",
    "elif not set([\"train\", \"val\", \"test\"]).issubset(set(os.listdir(data_path))):\n",
    "    print(\"Please update your data path to the correct folder (should contain train, val and test folders).\")\n",
    "else:\n",
    "    print(\"Congrats! You selected the correct folder :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b59fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (0.19.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (1.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (1.7.3)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (2021.7.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (1.21.5)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (2.19.3)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (9.4.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (22.0)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from scikit-image) (2.6.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (0.16.6)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (65.6.3)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (1.45.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (4.4.0)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (8.0.4)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (1.4.4)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (4.24.4)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (2.28.1)\r\n",
      "Requirement already satisfied: setproctitle in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (5.9.0)\r\n",
      "Requirement already satisfied: PyYAML in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from wandb) (6.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from Click!=8.0.0,>=7.1->wandb) (4.11.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.1->wandb) (3.11.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (1.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.17 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from monai) (1.21.5)\r\n",
      "Requirement already satisfied: torch>=1.8 in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from monai) (1.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages (from torch>=1.8->monai) (4.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "# Install additional packages required for this tutorial\n",
    "!pip install scikit-image\n",
    "!pip install wandb\n",
    "!pip install monai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf3580",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/KISYcqD.png\" width=400px></img>\n",
    "# Weights and Biases\n",
    "In the previous tutorial, you have trained a neural network and monitored the loss curves in a Jupyter notebook. You can imagine that if you train multiple networks with different settings, it's easy to lose track of all loss curves and figure out which model is best. Luckily, there exist excellent so-called MLOps systems to keep track of your experiments. Examples are Tensorboard, Neptune, and Weights & Biases (wandb). Here, we will give you some tips on how to use this last one to keep track of your experiments. Setting everything up is a simple process:\n",
    "\n",
    "1. Register an account at https://wandb.ai/site. Your projects and runs will be shown on this website, which is accessible from anywhere.\n",
    "\n",
    "2. Run the cell below (including ```wandb.login()```) to sign into your account. You will have to paste an API key, which you can find at https://wandb.ai/authorize, and hit Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe3719e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjelmerwolterink\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee7da1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can initialize a run in wandb with ```wandb.init()```. You can specify the project and run name, as well as configuration settings, by passing these as additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d6d34c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/jmwolterink/Library/CloudStorage/OneDrive-UniversityofTwente/Teaching/DLMIA_2023_2024/dlmia-course/notebooks/Tutorial3_Segmentation/wandb/run-20240515_100649-jqv58hq7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mExample run\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/jelmerwolterink/Example%20project\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/jelmerwolterink/Example%20project/runs/jqv58hq7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project='Example project', name='Example run', config={'dataset': 'VinDr-RibCXR'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd1251",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By running the code above, a new run is created. Although we have not logged any information yet, it is possible to see the run on the wandb website. As soon as we will start logging information, it will become visible. It is possible to log information to your run with ```wandb.log()```. When it's time to complete the run, you can stop it with ```run.finish()```. Run the cell below and see what happens on the run webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb2bd41",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.015 MB of 0.015 MB uploaded\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Accuracy ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÑ‚ñÑ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÜ‚ñÖ‚ñÅ‚ñÖ‚ñà‚ñÅ‚ñà‚ñà‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÑ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     Loss ‚ñÜ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñá‚ñÑ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÜ‚ñÉ‚ñÜ‚ñà‚ñÇ‚ñÇ‚ñá‚ñá‚ñÇ‚ñá‚ñá‚ñà‚ñÜ‚ñÉ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñà‚ñÇ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Accuracy 0.38454\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     Loss 0.7066\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mExample run\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jelmerwolterink/Example%20project/runs/jqv58hq7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jelmerwolterink/Example%20project\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240515_100649-jqv58hq7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# time.sleep(1) makes the script wait for 1 second, so the full script takes about 60 seconds to finish\n",
    "for step in range(60):\n",
    "    wandb.log({'Loss': random.random(), 'Accuracy': random.random()})\n",
    "    time.sleep(1)\n",
    "    \n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08e8f2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the remainder of this notebook, we will use wandb to log our loss values to a run webpage. In this way, it is easier to keep track of multiple losses and to compare several runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669491cd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "# Segmentation of chest X-ray images\n",
    "In this part of the tutorial, you will train a convolutional neural network for automatic segmentation of ribs in a chest X-ray image. You will examine the effects of using different loss functions, data augmentations and network architectures. To keep track of all these parameters, we will use wandb.\n",
    "\n",
    "## Data management \n",
    "\n",
    "### Build a CacheDataset\n",
    "\n",
    "At the beginning of the notebook, you defined the path to the rib data. We will now use it to find and load the corresponding images.\n",
    "The data consists of an image and a label. However, the label is in this case not a binary label, but a segmentation mask. Both the segmentation masks and the X-rays are .png files and can be opened using the PIL library.\n",
    "\n",
    "For this tutorial we are not going to code the Dataset class ourselves but instead use a Monai class: [`CacheDataset`](https://docs.monai.io/en/stable/data.html#cachedataset). It has two main advantages:\n",
    "- as the name suggests, it uses a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) mechanism, which means that it is memory efficient (this is partly explained later).\n",
    "- everything is pre-built, so no need to code the `__init__`, `__getitem__` and `__len__` functions anymore. \n",
    "\n",
    "`CacheDataset` only needs the list of your data samples to work. As we are learning a segmentation task, one data sample is composed of two images: the chest X-ray (named `img`) and the mask of the ribs (named `mask`). With Python, we represent this data sample with a dictionary:\n",
    "\n",
    "<img src=\"https://i.imgur.com/yLI4sJz.png\" alt=\"sample\" width=\"450\"/>\n",
    "\n",
    "Then we could give this kind of list to `CacheDataset` to create our data set:\n",
    "\n",
    "<img src=\"https://i.imgur.com/r7JJTgY.png\" alt=\"list of samples\" width=\"500\"/>\n",
    "\n",
    "\n",
    "However, as this list contains all the images, it means that all the images are always in Python memory even though they are not currently used so this is very inefficient. This is why we don't directly give  the images to `CacheDataset`, but the paths to find them on our disk. Then we also need to give a `Transform` object, `LoadRibData` which will transform these paths into the corresponding images only when they are needed.\n",
    "\n",
    "<img src=\"https://i.imgur.com/JacrPFO.png\" alt=\"list of paths\" width=\"650\"/>\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/EUWJj4r.png\" alt=\"transform path to image\" width=\"430\"/>\n",
    "\n",
    "As we don't want to write this list of paths manually, we write the function `build_dict_ribs` which automatically computes it based on the root folder of the data directory `data_path`. This is possible because fortunately your teachers pre-organized the dataset for this tutorial, but be aware that most medical datasets require a lot of cleaning up prior to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd227d28",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import monai\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def build_dict_ribs(data_path, mode='train'):\n",
    "    \"\"\"\n",
    "    This function returns a list of dictionaries, each dictionary containing the keys 'img' and 'mask' \n",
    "    that returns the path to the corresponding image.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): path to the root folder of the data set.\n",
    "        mode (str): subset used. Must correspond to 'train', 'val' or 'test'.\n",
    "        \n",
    "    Returns:\n",
    "        (List[Dict[str, str]]) list of the dictionaries containing the paths of X-ray images and masks.\n",
    "    \"\"\"\n",
    "    # test if mode is correct\n",
    "    if mode not in [\"train\", \"val\", \"test\"]:\n",
    "        raise ValueError(f\"Please choose a mode in ['train', 'val', 'test']. Current mode is {mode}.\")\n",
    "    \n",
    "    # define empty dictionary\n",
    "    dicts = []\n",
    "    # list all .png files in directory, including the path\n",
    "    paths_xray = glob.glob(os.path.join(data_path, mode, 'img', '*.png'))\n",
    "    # make a corresponding list for all the mask files\n",
    "    for xray_path in paths_xray:\n",
    "        if mode == 'test':\n",
    "            suffix = 'val'\n",
    "        else:\n",
    "            suffix = mode\n",
    "        # find the binary mask that belongs to the original image, based on indexing in the filename\n",
    "        image_index = os.path.split(xray_path)[1].split('_')[-1].split('.')[0]\n",
    "        # define path to mask file based on this index and add to list of mask paths\n",
    "        mask_path = os.path.join(data_path, mode, 'mask', f'VinDr_RibCXR_{suffix}_{image_index}.png')\n",
    "        if os.path.exists(mask_path):\n",
    "            dicts.append({'img': xray_path, 'mask': mask_path})\n",
    "    return dicts\n",
    "\n",
    "class LoadRibData(monai.transforms.Transform):\n",
    "    \"\"\"\n",
    "    This custom Monai transform loads the data from the rib segmentation dataset.\n",
    "    Defining a custom transform is simple; just overwrite the __init__ function and __call__ function.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = Image.open(sample['img']).convert('L') # import as grayscale image\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "        mask = Image.open(sample['mask']).convert('L') # import as grayscale image\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "        # mask has value 255 on rib pixels. Convert to binary array\n",
    "        mask[np.where(mask==255)] = 1\n",
    "        return {'img': image, 'mask': mask, 'img_meta_dict': {'affine': np.eye(2)}, \n",
    "                'mask_meta_dict': {'affine': np.eye(2)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c71aa",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Note\n",
    ":class: note\n",
    "Note that `LoadRibData` is now outputing a much more complex dictionary with two additional keys: `img_meta_dict` and `mask_meta_dict`. These contain respectively the meta data of `img` and `mask`: here we decided to add the resolution of the image, corresponding to the key `affine`. This will be useful for Monai to perform some transformations later.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3886ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "A `CacheDataset` needs two arguments to perform our procedure\n",
    "1. The list of samples (represented by dictionaries) which is computed by `build_dict_ribs`.\n",
    "2. A transform object, `LoadRibData`, which will transform the paths in the previous list into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a798e021",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# construct list of dictionaries\n",
    "train_dict_list = build_dict_ribs(data_path, mode='train')\n",
    "# construct CacheDataset from list of paths + transform\n",
    "train_dataset = monai.data.CacheDataset(train_dict_list, transform=LoadRibData())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdda5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "How many samples are in the training, validation, and test set?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da36f9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è print the length of each subset here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80113de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "What are the dimensions of the images? Are they all equal? Will this a problem when training a CNN for segmentation?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b8c5af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è print the dimensions of a bunch of images and masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40b1db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "What is the pixel intensity range of the images? Are they similar? Will this be a problem when training a CNN for segmentation?\n",
    "\n",
    "Visualize the pixel intensity range of a bunch of images. **Hint** Use Matplotlib `hist` ([documentation](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.hist.html)) and `.flatten()` your image (or you will get >2000 histograms per image)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28630d4e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10dbdc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "### Image visualization\n",
    "\n",
    "We now build `visualize_rib_sample` to plot X-ray images on which we overlay their binary mask (in green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ecb49ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_rib_sample(sample, title=None):\n",
    "    # Visualize the x-ray and overlay the mask, using the dictionary as input\n",
    "    image = np.squeeze(sample['img'])\n",
    "    mask = np.squeeze(sample['mask'])\n",
    "    plt.figure(figsize=[10,7])\n",
    "    plt.imshow(image, 'gray')\n",
    "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n",
    "    plt.imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594f73d",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Use `visualize_rib_sample` to visualize samples from the training set.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7ffed5f",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è visualize samples here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c22b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "### Data transforms\n",
    "\n",
    "The training dataset that you have created in the previous step is quite small. In order to improve the generalization of the network, we are going to do some __data augmentation__. This can be easily implemented in the pipeline using the [transforms](https://docs.monai.io/en/stable/transforms.html#) in Monai. \n",
    "\n",
    ":::{admonition} Data augmentation\n",
    ":class: note\n",
    "[Data augmentation](https://d2l.ai/chapter_computer-vision/image-augmentation.html#image-augmentation) is a very common way to enlarge the number of training samples that you use. By applying transformations such as rotation, flipping, scaling to images we *artificially* enlarge the number of samples that the model sees. \n",
    ":::\n",
    "\n",
    "\n",
    "Monai distinguishes *deterministic* and *random* transforms:\n",
    "- *Deterministic* transforms do the exact same thing every time they are called, for example intensity normalization will always have the same outcome. \n",
    "- *Random* transforms can be rotations or flips that do not have the same outcome each time they are called.\n",
    "\n",
    "The random transforms (for example flipping and rotation) need to be performed in the same way on the input image `'img'` as well as on the label `'mask'`. Monai has [adapted transforms](https://docs.monai.io/en/stable/transforms.html#dictionary-transforms) that take dictionaries as an input, such that random transforms can be performed in the same way for both images: you don't want a different angle of rotation for the image and the mask, as they will become misaligned! You can recognize these transforms as they end with a 'd', e.g. `Zoomd` instead of `Zoom`.\n",
    "\n",
    "<img src=\"https://i.imgur.com/I7BErgj.png\" alt=\"rotated sample\" width=\"650\"/>\n",
    "\n",
    "The examples below show how to use one of these dictionary transforms to perform a random horizontal flip and a random rotation. The procedure of applying a transform from Monai consists of two steps:\n",
    "1. Initialize the transform: here you build the object and pass the init variables, such as the probability or range of rotation, and in case of a dictionary transform the keys of the dictionary.\n",
    "2. Apply the transform on a data sample\n",
    "\n",
    "Lastly, most transforms expect a channel dimension for both 2D and 3D images. As we have only one channel, we should add this extra dimension using the `monai.transforms.AddChanneld` transform first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f744489",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49750/1625175677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This picks a random sample, but you can change this value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msample_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvisualize_rib_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Original sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# Load sample\n",
    "index = np.random.choice(np.arange(len(train_dataset))) # This picks a random sample, but you can change this value\n",
    "sample_dict = train_dataset[index]\n",
    "visualize_rib_sample(sample_dict, title=\"Original sample\")\n",
    "\n",
    "# Add channels\n",
    "add_channels_transform = monai.transforms.AddChanneld(keys=['img', 'mask']) # Initialize the transform\n",
    "channels_sample_dict = add_channels_transform(sample_dict) # Apply the transform\n",
    "print(\"Size of the image before AddChanneld transform\", sample_dict[\"img\"].shape)\n",
    "print(\"Size of the image after AddChanneld transform\", channels_sample_dict[\"img\"].shape)\n",
    "\n",
    "# Random flip\n",
    "# here we define the keys, the probability that the flip is performed and the axis to flip over\n",
    "random_flip_transform = monai.transforms.RandFlipd(keys=['img', 'mask'], prob=1, spatial_axis=1)\n",
    "# We put a probability of 1 to always flip the image for visualization purposes.\n",
    "# Please DO NOT DO THAT in the rest of the notebook.\n",
    "flipped_sample_dict = random_flip_transform(channels_sample_dict)\n",
    "visualize_rib_sample(flipped_sample_dict, title=\"(Not quite randomly) flipped sample\")\n",
    "\n",
    "# Random rotation\n",
    "# Here we define the keys in the dictionary that contain the data, the rotation range, but also the interpolation mode. \n",
    "# The interpolation mode defines how new pixel values for the rotated image are computed. \n",
    "# Note that these differ between mask and image, as we want to keep binary labels for the masks, and (bi)linear interpolation\n",
    "# would result in scalar values between 0 and 1.\n",
    "random_rotation_transform = monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=1, mode=['bilinear', 'nearest'])\n",
    "rotated_sample_dict = random_rotation_transform(channels_sample_dict)\n",
    "visualize_rib_sample(rotated_sample_dict, title=\"Randomly rotated sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e51727",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "Note how we concatenated the `AddChannelsd` transform with the flip and rotate transform by applying them after each other. If we have only two transforms, like here, this is an acceptable workflow. However, if we have a whole bunch of transforms that we have to call during training of the network, you can imagine that the code will become very messy. Therefore, we can combine multiple transforms into a single one using [`monai.transforms.Compose`](https://docs.monai.io/en/stable/transforms.html#compose).\n",
    "\n",
    "Upon initialization, `Compose` takes a list of transforms as input and outputs a transform object that concatenates all the transforms. It can be applied in the same way as the transforms shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72744ed4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Use the `Compose` class of Monai to compose `AddChanneld`, `RandFlipd` and `RandRotated` in a single transform. Then apply this composition to a data set sample.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c3b18a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49750/330516906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create the composed transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Apply this new single transform to sample_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# dataset[[1, 3, 4]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# if existing in cache, try to get the index in cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0mcache_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hash_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_num\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# support negative index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m             \u001b[0mcache_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "sample_dict = train_dataset[0]\n",
    "\n",
    "# Create the composed transform\n",
    "\n",
    "# Apply this new single transform to sample_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a4449",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Efficiency\n",
    ":class: note\n",
    "As mentioned before, there is a distinction between random and deterministic transforms. This distinction is important, as their order can have a huge influence on computational efficiency. The [CacheDataset](https://docs.monai.io/en/stable/data.html#cachedataset) class stores the outcomes of all deterministic transforms in memory, and performs the random transforms on-the-fly. Therefore, placing the deterministic transforms before the random ones in combination with caching results in a training and inference procedure will result in higher computational efficiency.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b059c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Compose the `AddChannel`, `ScaleIntensity`, `Zoom`, `RandomFlip` and `RandSpatialCrop` transforms in a computationally efficient order and construct a new training dataset (with `CacheDataset`) using these transforms.\n",
    "    \n",
    "  1. Make sure to visualize some samples to see if the transforms are working as you expect.\n",
    "    \n",
    "  2. Don't forget to put `LoadRibData` as your first transform or your dataset will try to rotate strings representing the paths of your images...\n",
    "    \n",
    "  3. If you identified problems in the previous sections it is time to solve them!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bccd2b02",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è code your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805031f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "### Dataloader\n",
    "\n",
    "We use mini-batch gradient descent during training, so we want to sample batches to train the network on, rather than single instances of the data. For this we use `monai.data.DataLoader`, which efficiently samples batches from the data that can be fed into the network right away. Remember that you have also used a `DataLoader` in Tutorial 2.\n",
    "\n",
    "This [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) provides more information about datasets and dataloaders.\n",
    "\n",
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Construct a dataloader with randomly sampled mini-batches of 16 images for the training set.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73fb7954",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è code your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2e382",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "### Validation set\n",
    "During training, we want to keep an eye on how the network generalizes to unseen data. It could perform very well on its training set, whereas it performs poorly on unseen data (overfitting). For this, we need to construct a validation set consisting of comparable but different samples of data than the training set. For our data, it can be constructed in a similar way as the train set:\n",
    "1. Construct the dictionary of file paths\n",
    "2. Define the transforms that should be applied on the validation data\n",
    "3. Construct the CacheDataset using the dictionary and transform\n",
    "4. Build a validation dataloader using from the CacheDataset\n",
    "\n",
    "In contrast to the training set, we don't apply data augmentation to the validation set. This is important to keep in mind when defining transforms.\n",
    "\n",
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Build a validation dataset and dataloader using these steps. Create a separate validation transform.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f100236",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è code your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e962e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the neural network, loss function, and optimizer\n",
    "\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Now that we have the data loading process out of the way, we set up a U-Net, a loss function, and an optimizer. If possible, use a GPU to substantially speed up computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b9f85c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The used device is cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'The used device is {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912d66f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Setting up a [U-Net](https://arxiv.org/abs/1505.04597) for segmentation in MONAI is as easy as calling the `UNet` function and providing it with the number of input channels, output channels, and feature maps/channels in the intermediate layers. The following provides us with a model that is optimized during training to perform segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90e52d0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(8, 16, 32, 64, 128),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bead79",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "How many levels does the U-Net have? And how many parameters does it have (use the code from last tutorial)? \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bf23de6",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è code your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad78f1d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loss function\n",
    "\n",
    "The loss function should reflect what we want the training model to be able to do. \n",
    "For segmentation, a popular function to use for training the network is the [Dice loss](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient), which measures the overlap between the ground-truth and the model prediction. Monai offers a wide range of different [loss functions](https://docs.monai.io/en/stable/losses.html) that are also suitable for segmentation. In this tutorial, we will also assess the effects of using different loss functions on our network performance.\n",
    "\n",
    "We show how to implement the Dice function in this example. Our network only has one output channel. In order to have all the output values between 0 and 1, so we can compute the Dice, the function first applies a [logistic sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). The Dice loss is computed over the full mini-batch (`batch=True`) to avoid poorly defined loss in individual batch samples. This means that, in a sense, we compute the Dice loss in a 3D stack of 2D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "310d28c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_function =  monai.losses.DiceLoss(sigmoid=True, batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d45029",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Choose an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b180957",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An optimizer algorithm is chosen that performs gradient descent on the network parameters to minimize the loss function. Last week you have used SGD (stochastic gradient descent). In many cases, [Adam](https://arxiv.org/abs/1412.6980) is a good default option. The optimizer operates on the parameters of the previously defined U-Net, i.e. `model`. A learning rate `lr` is provided to the optimizer, which defines how large the changes should be that are made to the network parameters in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57382e20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc898599",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up the training loop\n",
    "In the previous tutorial, we explained how to set up a simple loop for training your network. For training this network, the loop can be set up in a similar way.\n",
    "Instead of keeping track of the train and validation loss on your computer, you can [log](https://docs.wandb.ai/guides/track/log) everything in weights and biases.\n",
    "\n",
    "<div style='background-color:rgba(80,255,80,0.4); padding:20px'>\n",
    "  ‚å® <b>Exercise</b>: \n",
    "Set up a simple loop for training the network and log the following in weights and biases:\n",
    "    \n",
    "- Loss function\n",
    "- Learning rate\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Some validation images, including segmentation masks that display model output and the ground truth (see [W&B documentation](https://docs.wandb.ai/guides/track/log/media#image-overlays)).\n",
    "</div>\n",
    "\n",
    "Below, we provide a ```log_to_wandb()``` function that you could use for this. Just call it once at the end of every epoch, with the right arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4055c65",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'from_compose_to_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49750/3802791442.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m'loss function'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m'transform'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfrom_compose_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'from_compose_to_list' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(\n",
    "    project='tutorial3_segmentation',\n",
    "    name='test',\n",
    "    config={\n",
    "        'loss function': str(loss_function), \n",
    "        'lr': optimizer.param_groups[0][\"lr\"],\n",
    "        'transform': from_compose_to_list(train_transform),\n",
    "        'batch_size': train_loader.batch_size,\n",
    "    }\n",
    ")\n",
    "# Do not hesitate to enrich this list of settings to be able to correctly keep track of your experiments!\n",
    "# For example you should add information on your model...\n",
    "\n",
    "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics\n",
    "\n",
    "def wandb_masks(mask_output, mask_gt):\n",
    "    \"\"\" Function that generates a mask dictionary in format that W&B requires \"\"\"\n",
    "\n",
    "    # Apply sigmoid to model ouput and round to nearest integer (0 or 1)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    mask_output = sigmoid(mask_output)\n",
    "    mask_output = torch.round(mask_output)\n",
    "\n",
    "    # Transform masks to numpy arrays on CPU\n",
    "    # Note: .squeeze() removes all dimensions with a size of 1 (here, it makes the tensors 2-dimensional)\n",
    "    # Note: .detach() removes a tensor from the computational graph to prevent gradient computation for it\n",
    "    mask_output = mask_output.squeeze().detach().cpu().numpy()\n",
    "    mask_gt = mask_gt.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Create mask dictionary with class label and insert masks\n",
    "    class_labels = {1: 'ribs'}\n",
    "    masks = {\n",
    "        'predictions': {'mask_data': mask_output, 'class_labels': class_labels},\n",
    "        'ground truth': {'mask_data': mask_gt, 'class_labels': class_labels}\n",
    "    }\n",
    "    return masks\n",
    "\n",
    "def log_to_wandb(epoch, train_loss, val_loss, batch_data, outputs):\n",
    "    \"\"\" Function that logs ongoing training variables to W&B \"\"\"\n",
    "\n",
    "    # Create list of images that have segmentation masks for model output and ground truth\n",
    "    log_imgs = [wandb.Image(img, masks=wandb_masks(mask_output, mask_gt)) for img, mask_output,\n",
    "                mask_gt in zip(batch_data['img'], outputs, batch_data['mask'])]\n",
    "\n",
    "    # Send epoch, losses and images to W&B\n",
    "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'results': log_imgs})\n",
    "    \n",
    "# ‚å®Ô∏è WRITE YOUR TRAINING LOOP HERE\n",
    "\n",
    "# Store the network parameters        \n",
    "torch.save(model.state_dict(), r'trainedUNet.pt')\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bac4c4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2: Evaluate the trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9adaf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "user_expressions": []
   },
   "source": [
    "## Visual inspection\n",
    "We have trained the network on small patches of the image. In order to see how well it performs on the entire image, we can use the `monai.inferers.SlidingWindowInferer`. This 'slides' the network over patches of the input image. The overlap between patches can also be controlled. \n",
    "\n",
    "![sliding](https://docs.monai.io/en/stable/_images/sliding_window.png)\n",
    "\n",
    "Moreover, we want our final mask to have discrete values (0 or 1). Then, we need to discretize the continuous output of the network. This is why we use `Sigmoid` (clipping the output values between 0 and 1) and `AsDiscrete` (mapping the continuous distribution on the {0, 1} set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5793f627",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visual_evaluation(sample, model):\n",
    "    \"\"\"\n",
    "    Allow the visual inspection of one sample by plotting the X-ray image, the ground truth (green)\n",
    "    and the segmentation map produced by the network (red).\n",
    "    \n",
    "    Args:\n",
    "        sample (Dict[str, torch.Tensor]): sample composed of an X-ray ('img') and a mask ('mask').\n",
    "        model (torch.nn.Module): trained model to evaluate.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inferer = monai.inferers.SlidingWindowInferer(roi_size=[256, 256])\n",
    "    discrete_transform = monai.transforms.AsDiscrete(logit_thresh=0.5, threshold_values=True)\n",
    "    Sigmoid = torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        output = discrete_transform(Sigmoid(inferer(sample['img'].to(device), network=model).cpu())).squeeze()\n",
    "    \n",
    "    fig, ax = plt.subplots(1,3, figsize = [12, 10])\n",
    "    # Plot X-ray image\n",
    "    ax[0].imshow(sample[\"img\"].squeeze(), 'gray')    \n",
    "    ax[1].imshow(sample[\"img\"].squeeze(), 'gray')\n",
    "    # Plot ground truth\n",
    "    mask = np.squeeze(sample['mask'])\n",
    "    overlay_mask = np.ma.masked_where(mask == 0, mask == 1)\n",
    "    ax[1].imshow(overlay_mask, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    ax[1].set_title('Ground truth')\n",
    "    # Plot output\n",
    "    overlay_output = np.ma.masked_where(output < 0.1, output >0.99)\n",
    "    ax[2].imshow(sample['img'].squeeze(), 'gray')\n",
    "    ax[2].imshow(overlay_output, 'Reds', alpha = 0.7, clim=[0,1])\n",
    "    ax[2].set_title('Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4e11000",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmwolterink/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "test_dict = build_dict_ribs(data_path, mode='test')\n",
    "test_transform = monai.transforms.Compose([\n",
    "        LoadRibData(),\n",
    "        monai.transforms.AddChanneld(keys=['img', 'mask']),\n",
    "        monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "        monai.transforms.Zoomd(keys=['img', 'mask'], zoom=0.25, keep_size=False, mode=['bilinear', 'nearest']),\n",
    "    ]\n",
    ")\n",
    "test_set = monai.data.CacheDataset(test_dict, transform=test_transform)\n",
    "test_loader = monai.data.DataLoader(test_set, batch_size=1)\n",
    "\n",
    "for sample in test_loader:\n",
    "    visual_evaluation(sample, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc340ebd",
   "metadata": {},
   "source": [
    "## Compute evaluation metrics\n",
    "\n",
    "Previously, we evaluated the quality of our segmentation on the test set by visually inspecting the quality of the maps produced by our network. Though this is an essential step when developping and debugging a network, this step is also quite subjective, and tedious when there are a lot of images. This is why we need metrics to evaluate the performance of our network! These metrics give us a quantity that represents the performance of our network and enable comparison to other methods.\n",
    "\n",
    "We provide the function `compute_metric` to evaluate the performance of the network on the segmentation task. Similar to the transforms and loss functions, Monai contains many [evaluation metrics](https://docs.monai.io/en/stable/metrics.html) to assess the model's performance. We show how to compute the Dice metric for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f1a95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(dataloader, model, metric_fn):\n",
    "    \"\"\"\n",
    "    This function computes the average value of a metric for a data set.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (monai.data.DataLoader): dataloader wrapping the dataset to evaluate.\n",
    "        model (torch.nn.Module): trained model to evaluate.\n",
    "        metric_fn (function): function computing the metric value from two tensors:\n",
    "            - a batch of outputs,\n",
    "            - the corresponding batch of ground truth masks.\n",
    "        \n",
    "    Returns:\n",
    "        (float) the mean value of the metric\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inferer = monai.inferers.SlidingWindowInferer(roi_size=[256, 256])\n",
    "    discrete_transform = monai.transforms.AsDiscrete(threshold=0.5)\n",
    "    Sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    mean_value = 0\n",
    "    \n",
    "    for sample in dataloader:\n",
    "        with torch.no_grad():\n",
    "            output = discrete_transform(Sigmoid(inferer(sample['img'].to(device), network=model).cpu()))\n",
    "        mean_value += metric_fn(output, sample[\"mask\"])\n",
    "    \n",
    "    return (mean_value / len(dataloader)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c07e8",
   "metadata": {},
   "source": [
    "As we want to log everything in the corresponding run with W&B, we access to the corresponding run with `wandb.Api()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c239d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49750/489699043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tutorial3_segmentation/{run_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_id' is not defined"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(f\"tutorial3_segmentation/{run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6743f",
   "metadata": {},
   "source": [
    "### Dice\n",
    "\n",
    "This is the same metric that the one that was used for the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a2c89",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Find the appropriate function in [`monai.metrics`](https://docs.monai.io/en/stable/metrics.html) to compute the mean Dice with `compute_metric`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d18258d9",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è code your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768cac1c",
   "metadata": {},
   "source": [
    "### Hausdorff distance\n",
    "\n",
    "The Hausdorff distance uses a function f which computes the minimal distance between the farthest point that can be found in a set Y compared to another set X. The Hausdorff takes the worst case between f(X,Y) and f(Y,X).\n",
    "\n",
    "<img src=\"https://i.imgur.com/V7XL8eU.png\" alt=\"illustration of Hausdorff distance from Wikipedia\" width=\"200\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738fb71",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Find the appropriate function in [`monai.metrics`](https://docs.monai.io/en/stable/metrics.html) to compute the Hausdorff distance with `compute_metric`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37967f5b",
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è code your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef40640f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Are you happy with the performance of your model? Can you think of ways to improve the performance?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb2ccd",
   "metadata": {},
   "source": [
    "## Comparison to baseline masks\n",
    "Although you did some visual inspection of the outputs of the model, interpretation of the Dice scores and Hausdorff distances is [not trivial](https://arxiv.org/abs/2104.05642). Therefore, comparing to a baseline model for which we know it performs poorly is a good way to get some idea of how good your model actually is. \n",
    "\n",
    "For our dummy baseline model, instead of rib-shaped segmentations, we let the model output square segmentation maps in the rib region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab016863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "def make_dummy_sample(sample):\n",
    "    M = sample['mask'].squeeze()\n",
    "    labels = measure.label(M)\n",
    "    dummy_labels = np.zeros((labels.shape[0], labels.shape[1]))\n",
    "    for i in np.unique(labels):\n",
    "        if i > 0:\n",
    "            mask_locs = np.where(labels == i)\n",
    "            limits = [np.min(mask_locs[0]), np.max(mask_locs[0]), np.min(mask_locs[1]), np.max(mask_locs[1])]\n",
    "            dummy_labels[limits[0]:limits[1], limits[2]:limits[3]] = 1\n",
    "    return torch.tensor(dummy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32100db5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49750/2614710951.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvisualize_rib_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'original mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvisualize_rib_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_dummy_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dummy segmentation masks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# dataset[[1, 3, 4]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/notebook_collaboration/lib/python3.7/site-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# if existing in cache, try to get the index in cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0mcache_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hash_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_num\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# support negative index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m             \u001b[0mcache_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "sample = test_set[0]\n",
    "visualize_rib_sample(sample, title = 'original mask')\n",
    "visualize_rib_sample({'img': sample['img'], 'mask': make_dummy_sample(sample)}, title = 'dummy segmentation masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd45945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric_dummy(dataloader, metric_fn):\n",
    "    \"\"\"\n",
    "    This function computes the average value of a metric for a data set using the dummy segmentation masks\n",
    "    \n",
    "    Args:\n",
    "        dataloader (monai.data.DataLoader): dataloader wrapping the dataset to evaluate.\n",
    "        metric_fn (function): function computing the metric value from two tensors:\n",
    "            - a batch of outputs,\n",
    "            - the corresponding batch of ground truth masks.\n",
    "        \n",
    "    Returns:\n",
    "        (float) the mean value of the metric\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_value = 0\n",
    "    A = monai.transforms.AddChannel()\n",
    "    \n",
    "    for sample in dataloader:\n",
    "        output = make_dummy_sample(sample)\n",
    "        mean_value += metric_fn(A(A(output)), sample[\"mask\"])\n",
    "    \n",
    "    return (mean_value / len(dataloader)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3aa88089",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49750/2878529152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Mean Dice: {compute_metric_dummy(test_loader, monai.metrics.compute_meandice):.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/07/x0gf6dj176dfjvrn357t5p6h0000gn/T/ipykernel_49750/797961195.py\u001b[0m in \u001b[0;36mcompute_metric_dummy\u001b[0;34m(dataloader, metric_fn)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmean_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean_value\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(f'Mean Dice: {compute_metric_dummy(test_loader, monai.metrics.compute_meandice):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaa138",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "Are you still happy with the performance of your model? \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f5bcb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 3: Multilabel classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d588e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "user_expressions": []
   },
   "source": [
    ":::{admonition} Exercise\n",
    ":class: tip\n",
    "In the data directory, we have also placed a folder called `mask_mc` for the train, val and test set. In these images, we have labeled the left and right 7th rib. This is one of the ribs that's most prone to fracture, and identifying exactly that rib and the side that it's on could be valuable. Thus, this is no longer a binary label segmentation problem, but a multiclass segmentation problem. Try to also segment these images, consider what you will have to change in your code for this to work. \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11306a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚å®Ô∏è code your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}