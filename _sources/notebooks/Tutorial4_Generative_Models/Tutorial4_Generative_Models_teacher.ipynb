{"cells": [{"cell_type": "markdown", "id": "95c877ba", "metadata": {"user_expressions": []}, "source": ["# Tutorial 4\n", "\n", "## May 28, 2024\n", "\n", "In the first tutorials, you have trained and evaluated supervised models for image classification and segmentation. In this tutorial, we will take a closer look at generative models. We will again build on PyTorch, MONAI and Weights & Biases. Let's first make sure that you have access to MONAI. As always, if you're running this on Colab, make sure that you select a runtime with GPU."]}, {"cell_type": "code", "execution_count": null, "id": "9ce72c8c", "metadata": {}, "outputs": [], "source": ["import torch\n", "import random\n", "\n", "# Check whether we're using a GPU\n", "if torch.cuda.is_available():\n", "    n_gpus = torch.cuda.device_count()  # Total number of GPUs\n", "    gpu_idx = random.randint(0, n_gpus - 1)  # Random GPU index\n", "    device = torch.device(f'cuda:{gpu_idx}')\n", "    print('Using GPU: {}'.format(device))\n", "else:\n", "    device = torch.device('cpu')\n", "    print('GPU not found. Using CPU.')"]}, {"cell_type": "markdown", "id": "71e6627a", "metadata": {"user_expressions": []}, "source": [":::{admonition} Pretrained models\n", ":class: note\n", "You can download some pre-trained generator models for the GAN training exercises from [this link](https://drive.google.com/file/d/1tgSM2fRKZ882anc3YY6V1SPDjjN3Zmqo/view?usp=sharing), but of course it's much more interesting to train them yourself. Use \n", "\n", "```\n", "path = 'saved_models/CNN_GAN/Epoch_50.pt'\n", "checkpoint = torch.load(path)\n", "\n", "epoch = checkpoint['epoch']\n", "generator.load_state_dict(checkpoint['generator'])\n", "discriminator.load_state_dict(checkpoint['discriminator'])\n", "optimizer_gen.load_state_dict(checkpoint['optimizer_gen'])\n", "optimizer_dis.load_state_dict(checkpoint['optimizer_dis'])\n", "```\n", "\n", "to load the model (see also: [this link](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n", ":::"]}, {"cell_type": "markdown", "id": "c51de741", "metadata": {"user_expressions": []}, "source": ["You can monitor the GPU activity using `[watch] nvidia-smi` in a terminal. Ultimately you can even identify the ID of the jobs ran by your classmates to know who is using what. \n", "\n", "Consider dialogue before disconnecting them from the network using illegal ways."]}, {"cell_type": "markdown", "id": "657b0583", "metadata": {"user_expressions": []}, "source": ["## 1D generative adversarial network (GAN)\n", "We define a GAN consisting of two neural networks that play a game:\n", "*   The discriminator will learn to distinguish real and fake samples in $z$\n", "*   The generator will generate fake samples in $z$ that the discriminator cannot discriminate\n", "\n", "In the lecture, you have seen this simple 1D problem. We assume that there is a data set of real samples that are drawn from a normal distribution (the black dotted line below). These samples are in the sample domain and are called $x$. The generator network does not know anything about the distribution of the real samples in the sample domain, but will try to converge to a function that maps random noise $z$ to samples that seem to come from the real sample distribution (the green line below). The discriminator network is the *adversary* of the generator and it tries to distinguish real samples from fake samples. It's predictions on $x$ are shown with a blue curve in the figure. \n", "\n", "<img src=\"https://cs.stanford.edu/people/karpathy/gan/gan.png\">"]}, {"cell_type": "markdown", "id": "b73d75f5", "metadata": {"user_expressions": []}, "source": ["First, let's set up the data for this toy problem. We define the mean value of the normal distribution from which **real** samples will be drawn in the sample domain $x$. In addition, we define the dimensionality of the normal distribution $z$ from which noise samples to the generator will be drawn, i.e. the latent space. For now, we can assume a 1-dimensional noise distribution."]}, {"cell_type": "code", "execution_count": null, "id": "3971f422", "metadata": {}, "outputs": [], "source": ["# Determines the distribution of the real samples N(real_mean, 1)\n", "real_mean = 8\n", "# Determines the dimensionality of the latent space\n", "latent_dim = 1"]}, {"cell_type": "markdown", "id": "7637ce55", "metadata": {"user_expressions": []}, "source": ["The cell below defines the generator and discriminator networks. These are very simple networks that map scalars to scalars through a hidden layer. In this case, the networks are actually identical."]}, {"cell_type": "code", "execution_count": null, "id": "f88a0348", "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "\n", "# The discriminator will directly classify the input value\n", "class Discriminator_1D(nn.Module):\n", "    \n", "    def __init__(self):\n", "        super(Discriminator_1D, self).__init__()\n", "        self.layers = nn.Sequential(nn.Linear(in_features=1, out_features=32), \n", "                                    nn.LeakyReLU(),\n", "                                    nn.Linear(in_features=32, out_features=1))\n", "        \n", "    def forward(self, x):\n", "        return self.layers(x)\n", "    \n", "# The generator will transform a single input value\n", "class Generator_1D(nn.Module):\n", "    \n", "    def __init__(self):\n", "        super(Generator_1D, self).__init__()\n", "        self.layers = nn.Sequential(nn.Linear(in_features=1, out_features=32),\n", "                                   nn.LeakyReLU(),\n", "                                   nn.Linear(in_features=32, out_features=1))\n", "        \n", "    def forward(self, x):\n", "        return self.layers(x)"]}, {"cell_type": "markdown", "id": "7d615d8d", "metadata": {"user_expressions": []}, "source": ["Now, we will define the training functions for both networks. Consider what is actually happening in a GAN and how the inputs and outputs are connected. The overall objective function of our system is as follows\n", "\n", "$V^{(D)}(D,G)=\\underset{x\\sim p_{data}}{\\mathbb{E}} [\\log{D(x)}]+\\underset{z\\sim p_z}{\\mathbb{E}} [\\log{(1-D(G(z)))}]$\n", "\n", "There are four important variables when training this GAN\n", "\n", "*   $z$: the noise that will be input to the generator\n", "*   $G(z)$: the output of the generator, i.e. the samples that should approximate the real samples\n", "*   $D(G(z))$: the discriminator's decision based on the fake sample\n", "*   $x$: real samples drawn from the real sample distribution\n", "\n", "The generator $G$ is trying to minimize the overall objective, and the discriminator $D$ tries to maximize it. In other words, the discriminator aims to minimize the binary cross-entropy such that it predicts 1 for any real sample $x$ and 0 for any fake sample $G(z)$. At the same time, the generator tries to get the discriminator to predict 1 any fake sample $G(z)$. \n", "\n", "The cell below defines the optimizers for both networks. Note that while the networks share an objective function, they each have their own optimizer. We use the Adam optimizer in both cases, with the same settings. We use the stable [`BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) loss function, that combines binary cross-entropy calculation with a sigmoid."]}, {"cell_type": "code", "execution_count": null, "id": "9c1b6db2", "metadata": {}, "outputs": [], "source": ["# Initialize both networks\n", "discriminator = Discriminator_1D().to(device)\n", "generator = Generator_1D().to(device)\n", "\n", "# Configure optimizers and loss function\n", "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "\n", "# We use the same loss function for both networks\n", "loss_function = torch.nn.BCEWithLogitsLoss()"]}, {"cell_type": "markdown", "id": "fa484c55", "metadata": {"user_expressions": []}, "source": ["The code below will run the training loop. It's a bit different then you're used to, take a good look.\n", "\n", ":::{admonition} Exercise\n", ":class: tip\n", "In the lecture it was mentioned that the generator should be frozen when training the discriminator and vice versa. Do you see where this is happening?\n", ":::"]}, {"cell_type": "markdown", "id": "5b0982be", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": ["In the block\n", "```python\n", "for param in generator.parameters():\n", "    param.requires_grad = False\n", "for param in discriminator.parameters():\n", "    param.requires_grad = True\n", "```\n", "\n", "and\n", "\n", "```python\n", "for param in generator.parameters():\n", "    param.requires_grad = True\n", "for param in discriminator.parameters():\n", "    param.requires_grad = False  \n", "```"]}, {"cell_type": "markdown", "id": "73e593f8", "metadata": {"user_expressions": []}, "source": ["Now run the cell. This could take a few minutes, but below the cell you will periodically see a plot of the current situation. The plot shows the fake and real data distribution and the discriminators predictions (in blue)."]}, {"cell_type": "code", "execution_count": null, "id": "fe72951b", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "from IPython.display import display, clear_output\n", "\n", "\n", "# We will store the losses here\n", "gen_losses = []\n", "dis_losses = []\n", "\n", "# Training loop\n", "n_samples = 500\n", "iterations = 10000\n", "\n", "fig = plt.figure()\n", "ax = fig.add_subplot(1, 1, 1)\n", "\n", "for iteration in range(1, iterations + 1):\n", "    \n", "    # ========== Train Discriminator ==========\n", "    \n", "    for param in generator.parameters():\n", "        param.requires_grad = False\n", "    for param in discriminator.parameters():\n", "        param.requires_grad = True\n", "    discriminator.zero_grad()\n", "    \n", "    # Get a random set of input noise\n", "    noise = torch.normal(0, 1, size=(n_samples, latent_dim), device=device)\n", "    \n", "    # Also get a sample from the 'real' distribution\n", "    real = torch.normal(real_mean, 1, size=(n_samples, latent_dim), device=device)\n", "    \n", "    # Generate some fake samples using the generator\n", "    fake = generator(noise)\n", "    \n", "    # Concatenate the fake and real images\n", "    dis_input = torch.cat((real, fake))\n", "\n", "    # Make labels for generated and real data (set labels for real samples to 1)\n", "    dis_labels = torch.zeros((2 * n_samples, latent_dim), device=device)\n", "    dis_labels[:n_samples] = 1\n", "\n", "    # Train discriminator with this batch of samples\n", "    predictions = discriminator(dis_input)\n", "    dis_loss = loss_function(predictions, dis_labels)\n", "    dis_loss.backward()\n", "    optimizer_dis.step()\n", "    dis_losses.append(dis_loss.detach().cpu().numpy())\n", "\n", "    # ========== Train Generator ==========\n", "    \n", "    for param in generator.parameters():\n", "        param.requires_grad = True\n", "    for param in discriminator.parameters():\n", "        param.requires_grad = False\n", "    generator.zero_grad()\n", "    \n", "    # Get a random set of input noise\n", "    noise = torch.normal(0, 1, size=(n_samples, latent_dim), device=device)\n", "    \n", "    # From the generator's perspective, the discriminator should predict ones for all samples\n", "    gen_labels = torch.ones((n_samples, latent_dim), device=device)\n", "    \n", "    # Train generator\n", "    fake = generator(noise)\n", "    predictions = discriminator(fake)\n", "    gen_loss = loss_function(predictions, gen_labels)\n", "    gen_loss.backward()\n", "    optimizer_gen.step()\n", "    gen_losses.append(gen_loss.detach().cpu().numpy())\n", "\n", "    # ========== Make plot ==========\n", "    \n", "    # For every 100th iteration, plot samples from real and fake distributions\n", "    if iteration % 100 == 0:\n", "        \n", "        # Generate fake samples and predictions without gradient calculations\n", "        with torch.no_grad():\n", "            \n", "            # Get fake and real samples, together with discriminator predictions from a standard range of values\n", "            noise = torch.normal(0, 1, size=(n_samples, latent_dim), device=device)\n", "            fake = generator(noise)\n", "            real = torch.normal(real_mean, 1, size=(n_samples, latent_dim), device=device)\n", "            predictions = torch.sigmoid(discriminator(torch.arange(-20, 20, 0.5, device=device).view(80, 1)))\n", "            \n", "            # Make new plot and sleep half a second\n", "            ax.cla()\n", "            ax.set(xlim=(-20, 20), ylim=(0, 1))\n", "            ax.hist((torch.squeeze(fake).cpu().numpy(), torch.squeeze(real).cpu().numpy()), density=True, stacked=True, color=('g','k'))\n", "            ax.scatter(np.arange(-20, 20, 0.5), predictions.cpu().numpy(), c='b')\n", "            ax.set_title('Iteration {}'.format(iteration))\n", "            ax.legend(['Discriminator', 'Fake', 'Real'])\n", "            display(fig)\n", "            clear_output(wait=True)\n", "            plt.pause(0.5)"]}, {"cell_type": "markdown", "id": "ec6dd28a", "metadata": {"user_expressions": []}, "source": ["If all is well, the fake and real distributions should overlap nicely after training. The discriminator has essentially pushed the fake samples towards the real distribution and the generator is now able to transform the noise distribution into a distribution of 'real' samples!\n", "\n", ":::{admonition} Exercise\n", ":class: tip\n", "Can you explain what happened to the blue line during training? Why does it look like it does after training?\n", "::: "]}, {"cell_type": "markdown", "id": "af5207a1", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "If all is well, the blue line becomes horizontal: the discriminator $D$ predicts 0.5 for each sample. Hence, it can no longer distinguish between fake and real samples. \n", ":::"]}, {"cell_type": "markdown", "id": "68d3557a", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "Try training the GAN with different input noise distributions for $z$, e.g. a uniform distribution. See if you can find a distribution for the real samples for which the generator fails to generate samples.\n", ":::"]}, {"cell_type": "markdown", "id": "227f2d3e", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "In principle, a uniform distribution shouldn't really be a problem. What could be a problem is if you move the distribution of real samples too far away from 0, by changing the `real_mean` parameter too much.\n", ":::"]}, {"cell_type": "markdown", "id": "7ef0eb61", "metadata": {"user_expressions": []}, "source": ["During training, we have stored the loss values for the discriminator and the generator. We can now plot these. In the neural networks that we've trained so far, we have tried to train a neural network such that the loss decreases until a minimum is reached. "]}, {"cell_type": "code", "execution_count": null, "id": "349cb098", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 5))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(dis_losses)\n", "plt.title('Discriminator loss')\n", "plt.xlabel('Iteration')\n", "plt.ylabel('Loss')\n", "plt.subplot(1, 2, 2)\n", "plt.plot(gen_losses)\n", "plt.title('Generator loss')\n", "plt.xlabel('Iteration')\n", "plt.ylabel('Loss')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "d3d1aff2", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "The loss curves that you get now look different. Can you explain why they're not nicely dropping to zero? Can you explain the value in the loss in the discriminator based on the objective function of the discriminator? Consider that we actually let the discriminator optimize a binary cross-entropy loss.\n", ":::"]}, {"cell_type": "markdown", "id": "d25291a7", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "If all is well, both loss curves converge to $-ln(0.5) ~ 0.69$. In binary cross-entropy, if the prediction is 0.5 for a sample for which the reference is either 0 or 1, the loss is $-ln(0.5)$.\n", ":::"]}, {"cell_type": "markdown", "id": "a8971ae8", "metadata": {"user_expressions": []}, "source": ["# MNIST synthesis\n", "Although it is definitely nice that we can train two networks together to learn the distribution of a real data distribution, generating samples from a normal distribution is in itself not really interesting. Luckily, we can use the same principles to generate images. We will be synthesizing MNIST digits. To prepare for this, we first download the dataset. The code in this cell should look familiar:\n", "- We compose several transforms into one transform\n", "- The `torchvision` library conveniently lets you generate a `Dataset` object that already includes the MNIST digit dataset (and downloads it from the internet)\n", "- The `DataLoader` is just what we have used before.\n", "\n", "However, note that we do not use MONAI here, instead we use an original PyTorch `DataLoader`, which works in just the same way."]}, {"cell_type": "code", "execution_count": null, "id": "080fd432", "metadata": {}, "outputs": [], "source": ["import torchvision\n", "from torchvision.transforms import Compose, PILToTensor, ConvertImageDtype, Normalize\n", "from torch.utils.data import DataLoader\n", "\n", "# Define transform that converts PIL images into Tensors, with values between -1.0 and 1.0\n", "transform = Compose([\n", "    PILToTensor(),\n", "    ConvertImageDtype(torch.float),\n", "    Normalize(mean=0.5, std=0.5)\n", "])\n", "\n", "# Load the MNIST dataset\n", "mnist_data = torchvision.datasets.MNIST(root='datasets', download=True, transform=transform)\n", "data_loader = DataLoader(mnist_data, batch_size=100, shuffle=True)"]}, {"cell_type": "markdown", "id": "fee5503a", "metadata": {"user_expressions": []}, "source": ["The below code gets a batch of samples from the MNIST dataset and let's you plot these. These are the $28\\times 28$ pixel images that you will be synthesizing."]}, {"cell_type": "code", "execution_count": null, "id": "7e28b1c2", "metadata": {}, "outputs": [], "source": ["def plotImages(images, dim=(10, 10), figsize=(10, 10), title=''):\n", "    num_images = dim[0] * dim[1]\n", "    images = images.reshape((num_images, 28, 28))\n", "    fig = plt.figure(figsize=figsize)\n", "    fig.suptitle(title, fontsize=14)\n", "    for i in range(images.shape[0]):\n", "        plt.subplot(dim[0], dim[1], i+1)\n", "        plt.imshow(images[i], interpolation='nearest', cmap='gray_r')\n", "        plt.axis('off')\n", "    plt.tight_layout()\n", "    plt.show()\n", "    \n", "# Get first batch of images and plot them in a grid\n", "images, labels = next(iter(data_loader))\n", "plotImages(images.numpy(), title='MNIST examples')"]}, {"cell_type": "markdown", "id": "8aebefa3", "metadata": {}, "source": ["Most deep learning tutorials build a discriminative model that is able to classify an image into one of the ten digit categories. In this exercise, we are going to do the inverse. Given a point in a latent space (which in our case will be a multi-dimensional Gaussian distribution), we are going to train the network to generate a realistic digit image for this point. The MNIST data set will be used as a set of real samples. \n", "\n", "<img src=\"https://miro.medium.com/max/1000/1*Q7sZcfRj2M64GDD1ncvoCA.jpeg\">\n", "\n"]}, {"cell_type": "markdown", "id": "4d48684a", "metadata": {"user_expressions": []}, "source": ["## The discriminator\n", "As you can see in the image above, we will need a generator and a discriminator network. Let's define these first using the cells below. First, we define the discriminator, which will classify images as either *real* or *fake*."]}, {"cell_type": "code", "execution_count": null, "id": "9e4d04ea", "metadata": {}, "outputs": [], "source": ["class Discriminator_MLP(nn.Module):\n", "    \n", "    def __init__(self):\n", "        super(Discriminator_MLP, self).__init__()\n", "        self.layers = nn.Sequential(nn.Linear(in_features=784, out_features=1024),\n", "                                   nn.LeakyReLU(0.2),\n", "                                   nn.Dropout(0.3),\n", "                                   nn.Linear(in_features=1024, out_features=512),\n", "                                   nn.LeakyReLU(0.2),                                   \n", "                                   nn.Dropout(0.3),                                   \n", "                                   nn.Linear(in_features=512, out_features=256),\n", "                                   nn.LeakyReLU(0.2),                                   \n", "                                   nn.Linear(in_features=256, out_features=1),\n", "                                   nn.Sigmoid())\n", "        \n", "    def forward(self, x):\n", "        return self.layers(x)"]}, {"cell_type": "markdown", "id": "27b89655", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "Is this a convolutional neural network? Why (not)?\n", ":::"]}, {"cell_type": "markdown", "id": "1e2679a2", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "No, there's no convolution layers.\n", ":::"]}, {"cell_type": "markdown", "id": "9aa1a62f", "metadata": {"user_expressions": []}, "source": ["You could argue that digits are a bit more complex than samples from a Gaussian distribution, so let's set the latent space dimensionality for noise sampling a bit higher than 1. We will sample noise from a 10-dimensional distribution. It's good to realize that this is still *much* lower than the 784 dimensions (28x28) that our original MNIST images have."]}, {"cell_type": "code", "execution_count": null, "id": "4572b46d", "metadata": {}, "outputs": [], "source": ["latent_dim = 10"]}, {"cell_type": "markdown", "id": "ca675d21", "metadata": {"user_expressions": []}, "source": ["\n", "## The generator\n", "\n", "The generator is different than the discriminator. It should go from a low-dimensional noise vector to an MNIST image."]}, {"cell_type": "code", "execution_count": null, "id": "1e961c2e", "metadata": {}, "outputs": [], "source": ["class Generator_MLP(nn.Module):\n", "    \n", "    def __init__(self):\n", "        super(Generator_MLP, self).__init__()\n", "        self.layers = nn.Sequential(nn.Linear(in_features=latent_dim, out_features=256),\n", "                                    nn.LeakyReLU(0.2),                                    \n", "                                    nn.Linear(in_features=256, out_features=512),                                    \n", "                                    nn.LeakyReLU(0.2),                                    \n", "                                    nn.Linear(in_features=512, out_features=1024), \n", "                                    nn.LeakyReLU(0.2),                                   \n", "                                    nn.Linear(in_features=1024, out_features=784),\n", "                                    nn.Tanh())\n", "       \n", "    def forward(self, x):\n", "        return self.layers(x)"]}, {"cell_type": "markdown", "id": "c0828a0a", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "Consider the activation functions of the output layers of the generator and discriminator networks. How are they different?\n", ":::"]}, {"cell_type": "markdown", "id": "735cb109", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "The generator has a `Tanh` output, which means that all output values will be squashed between -1 and 1. The discriminator has a sigmoid output because it performs classification.\n", ":::"]}, {"cell_type": "markdown", "id": "b3a3454f", "metadata": {"user_expressions": []}, "source": ["## The model\n", "Now  let's combine the generator and the discriminator. We train both using a binary cross-entropy objective. This is very similar to what we did before."]}, {"cell_type": "code", "execution_count": null, "id": "ec0b0017", "metadata": {}, "outputs": [], "source": ["# Get networks\n", "discriminator = Discriminator_MLP()\n", "generator = Generator_MLP()\n", "\n", "# Push networks to device\n", "discriminator.to(device)\n", "generator.to(device)\n", "\n", "# Configure optimizers and loss function\n", "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "loss = torch.nn.BCELoss()"]}, {"cell_type": "markdown", "id": "c7b83a04", "metadata": {"user_expressions": []}, "source": ["We define some helper functions that will allow us to save and plot the models."]}, {"cell_type": "code", "execution_count": null, "id": "a90f7a4e", "metadata": {}, "outputs": [], "source": ["# Save model checkpoint in a folder called 'saved_models'\n", "def saveModels(epoch, model_name):\n", "    to_save = {\n", "        'epoch': epoch,\n", "        'generator': generator.state_dict(),\n", "        'discriminator': discriminator.state_dict(),\n", "        'optimizer_gen': optimizer_gen.state_dict(),\n", "        'optimizer_dis': optimizer_dis.state_dict()\n", "    }\n", "    directory = os.path.join('saved_models', model_name)\n", "    if not os.path.exists(directory):\n", "        os.makedirs(directory)\n", "    torch.save(to_save, os.path.join(directory, 'Epoch_{}.pt'.format(epoch)))\n", "    \n", "# Plot generated images in a grid\n", "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n", "    with torch.no_grad():\n", "        noise = torch.normal(0, 1, size=(examples, latent_dim), device=device)\n", "        fake_images = generator(noise).cpu().numpy()\n", "        fake_images = fake_images.reshape(examples, 28, 28)\n", "\n", "        fig = plt.figure(figsize=figsize)\n", "        fig.suptitle('Epoch {}'.format(epoch), fontsize=14)\n", "        for i in range(fake_images.shape[0]):\n", "            plt.subplot(dim[0], dim[1], i+1)\n", "            plt.imshow(fake_images[i], interpolation='nearest', cmap='gray_r')\n", "            plt.axis('off')\n", "        plt.tight_layout()\n", "        plt.show()\n", "        return fig"]}, {"cell_type": "markdown", "id": "4294c15a", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "Take a look at the code, it's actually very similar to what we used in the first, 1D, GAN. See if you can find at least two differences. \n", "\n", "If you run the code, synthesized images should be shown periodically. \n", ":::"]}, {"cell_type": "markdown", "id": "352a99ac", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "Some differences:\n", "- We use label smoothing now (the real samples get label 0.9 instead of 1.0). This makes it more difficult for the discriminator to distinguish fake from real images, and is used to make GANs train better.\n", "- The latent space dimensionality is larger\n", "- We flatten the images before they go into the discriminator\n", ":::"]}, {"cell_type": "code", "execution_count": null, "id": "839bc260", "metadata": {}, "outputs": [], "source": ["from tqdm import tqdm\n", "import os\n", "\n", "dis_losses = []\n", "gen_losses = []\n", "\n", "epochs = 20\n", "batch_size = 100\n", "fig = None\n", "\n", "for epoch in range(1, epochs + 1):\n", "    \n", "    # Wrap dataloader into tqdm such that we can print progress while training\n", "    with tqdm(data_loader, unit=\"iterations\") as tqdm_iterator:\n", "        tqdm_iterator.set_description('Epoch {}'.format(epoch))\n", "        \n", "        for i, batch in enumerate(tqdm_iterator):\n", "\n", "            # ========== Train Discriminator ==========\n", "            \n", "            # Freeze generator part\n", "            for param in generator.parameters():\n", "                param.requires_grad = False\n", "            for param in discriminator.parameters():\n", "                param.requires_grad = True\n", "            discriminator.zero_grad()\n", "\n", "            # Get a random set of input noise\n", "            noise = torch.normal(0, 1, size=(batch_size, latent_dim), device=device)\n", "\n", "            # Get real images and flatten the image dimensions\n", "            real_images, _ = batch\n", "            real_images = real_images.to(device).view(batch_size, 784)\n", "\n", "            # Generate some fake MNIST images using the generator\n", "            fake_images = generator(noise)\n", "\n", "            # Concatenate the fake and real images\n", "            dis_input = torch.cat((real_images, fake_images))\n", "\n", "            # Labels for generated and real data\n", "            dis_labels = torch.zeros((2 * batch_size, 1), device=device)\n", "\n", "            # One-sided label smoothing\n", "            dis_labels[:batch_size] = 0.9\n", "\n", "            # Train discriminator with this batch of samples\n", "            predictions = discriminator(dis_input)\n", "            dis_loss = loss(predictions, dis_labels)\n", "            dis_loss.backward()\n", "            optimizer_dis.step()\n", "            dis_losses.append(dis_loss.detach().cpu().numpy())\n", "\n", "            # ========== Train Generator ==========\n", "            \n", "            # Freeze the discriminator part\n", "            for param in generator.parameters():\n", "                param.requires_grad = True\n", "            for param in discriminator.parameters():\n", "                param.requires_grad = False\n", "            generator.zero_grad()\n", "\n", "            # Train generator with a new batch of generated samples\n", "            noise = torch.normal(0, 1, size=(batch_size, latent_dim), device=device)\n", "\n", "            # From the generator's perspective, the discriminator should predict\n", "            # ones for all samples\n", "            gen_labels = torch.ones((batch_size, 1), device=device)\n", "\n", "            # Train the GAN to predict ones\n", "            fake_images = generator(noise)\n", "            predictions = discriminator(fake_images)\n", "            gen_loss = loss(predictions, gen_labels)\n", "            gen_loss.backward()\n", "            optimizer_gen.step()\n", "            gen_losses.append(gen_loss.detach().cpu().numpy())\n", "        \n", "    # Display generated images every 5th epoch\n", "    if epoch % 5 == 0:\n", "        clear_output(wait=True)\n", "        fig = plotGeneratedImages(epoch)\n", "        saveModels(epoch, 'MLP_GAN')"]}, {"cell_type": "markdown", "id": "48f9fb41", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "If all is well, your model has synthesized images of digits. How cool is that?! Are you satisfied with the quality of these images? What could be improved?\n", "\n", "Once again, we can plot the loss curves for the trained model.\n", ":::"]}, {"cell_type": "markdown", "id": "6e58b3d0", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "It's likely that the synthesized images are still a bit noisy, this could be improved.\n", ":::"]}, {"cell_type": "code", "execution_count": null, "id": "5ebf3f9c", "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(dis_losses)\n", "plt.title('Discriminator loss')\n", "plt.xlabel('Iteration')\n", "plt.ylabel('Loss')\n", "plt.subplot(1, 2, 2)\n", "plt.plot(gen_losses)\n", "plt.title('Generator loss')\n", "plt.xlabel('Iteration')\n", "plt.ylabel('Loss')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "c747c487", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "Inspect the loss curves for this model and explain what happens.\n", ":::"]}, {"cell_type": "markdown", "id": "f8c43a1e", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "Similarly to 1D synthesis, both loss terms converge to a value that indicates that the discriminator finds it hard to distinguish samples.\n", ":::"]}, {"cell_type": "markdown", "id": "ec8740a3", "metadata": {"user_expressions": []}, "source": ["## A convolutional model\n", "Thus far the discriminator and generator were both multilayer perceptrons. Now we're going to add in some convolutional layers to turn them into a deep convolutional GAN ([DCGAN](http://arxiv.org/abs/1511.06434))-like architecture. This means that we have to redefine the generator network and a discriminator network. First, we define the discriminator, which is a pretty basic classification CNN, similar to what you used in Tutorial 2."]}, {"cell_type": "code", "execution_count": null, "id": "56aa8ef0", "metadata": {}, "outputs": [], "source": ["class Discriminator_CNN(nn.Module):\n", "    \n", "    def __init__(self):\n", "        super(Discriminator_CNN, self).__init__()\n", "        self.layers = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, stride=2, padding=2),\n", "                                    nn.LeakyReLU(0.2),\n", "                                    nn.Dropout(0.3),\n", "                                    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2),\n", "                                    nn.LeakyReLU(0.2),\n", "                                    nn.Dropout(0.3),\n", "                                    nn.Flatten(),\n", "                                    nn.Linear(in_features=128*7*7, out_features=1),\n", "                                    nn.Sigmoid())\n", "        \n", "    def forward(self, x):\n", "        return self.layers(x)\n", "\n", "class Generator_CNN(nn.Module):\n", "    \n", "    def __init__(self):\n", "        super(Generator_CNN, self).__init__()\n", "        self.linear = nn.Sequential(nn.Linear(in_features=latent_dim, out_features=128*7*7),\n", "                                    nn.LeakyReLU(0.2))\n", "        self.convolutional = nn.Sequential(nn.Upsample(size=(14, 14)),\n", "                                           nn.Conv2d(in_channels=128, out_channels=64, kernel_size=5, padding='same'),\n", "                                           nn.LeakyReLU(0.2),\n", "                                           nn.Upsample(size=(28, 28)),\n", "                                           nn.Conv2d(in_channels=64, out_channels=1, kernel_size=5, padding='same'),\n", "                                           nn.Tanh())\n", "        \n", "    def forward(self, x):\n", "        x = self.linear(x)\n", "        x = x.view(-1, 128, 7, 7)        \n", "        x = self.convolutional(x)\n", "        return x"]}, {"cell_type": "markdown", "id": "6457bb90", "metadata": {"user_expressions": []}, "source": ["Let's build our GAN model like before."]}, {"cell_type": "code", "execution_count": null, "id": "64a7c549", "metadata": {}, "outputs": [], "source": ["# Get networks\n", "discriminator = Discriminator_CNN()\n", "generator = Generator_CNN()\n", "\n", "# Push networks to device\n", "discriminator.to(device)\n", "generator.to(device)\n", "\n", "# Configure optimizers and loss function\n", "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "loss = torch.nn.BCELoss()"]}, {"cell_type": "markdown", "id": "3e6aa8fc", "metadata": {"user_expressions": []}, "source": ["Train the model using the code below. Inspect the samples that come out. \n", "\n", ":::{admonition} Exercise\n", ":class: tip\n", "What are some differences between these samples and the ones generated by the multilayer perceptron GAN? Can you explain these differences?\n", ":::"]}, {"cell_type": "markdown", "id": "968310c7", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "The samples generated with convolutional layers are likely to be more smooth/complete than the ones made with MLPs.\n", ":::"]}, {"cell_type": "code", "execution_count": null, "id": "a54db581", "metadata": {}, "outputs": [], "source": ["dis_losses = []\n", "gen_losses = []\n", "\n", "epochs = 50\n", "batch_size = 100\n", "\n", "for epoch in range(1, epochs + 1):\n", "    \n", "    # Wrap dataloader into tqdm such that we can print progress while training\n", "    with tqdm(data_loader, unit=\"iterations\") as tqdm_iterator:\n", "        tqdm_iterator.set_description('Epoch {}'.format(epoch))\n", "        \n", "        for i, batch in enumerate(tqdm_iterator):\n", "\n", "            # ========== Train Discriminator ==========\n", "            \n", "            # Freeze generator part\n", "            for param in generator.parameters():\n", "                param.requires_grad = False\n", "            for param in discriminator.parameters():\n", "                param.requires_grad = True\n", "            discriminator.zero_grad()\n", "\n", "            # Get a random set of input noise\n", "            noise = torch.normal(0, 1, size=(batch_size, latent_dim), device=device)\n", "\n", "            # Get real images and flatten the image dimensions\n", "            real_images, _ = batch\n", "            real_images = real_images.to(device)\n", "\n", "            # Generate some fake MNIST images using the generator\n", "            fake_images = generator(noise)\n", "\n", "            # Concatenate the fake and real images\n", "            dis_input = torch.cat((real_images, fake_images))\n", "\n", "            # Labels for generated and real data\n", "            dis_labels = torch.zeros((2 * batch_size, 1), device=device)\n", "\n", "            # One-sided label smoothing\n", "            dis_labels[:batch_size] = 0.9\n", "\n", "            # Train discriminator with this batch of samples\n", "            predictions = discriminator(dis_input)\n", "            dis_loss = loss(predictions, dis_labels)\n", "            dis_loss.backward()\n", "            optimizer_dis.step()\n", "            dis_losses.append(dis_loss.detach().cpu().numpy())\n", "\n", "            # ========== Train Generator ==========\n", "            \n", "            # Freeze the discriminator part\n", "            for param in generator.parameters():\n", "                param.requires_grad = True\n", "            for param in discriminator.parameters():\n", "                param.requires_grad = False\n", "            generator.zero_grad()\n", "\n", "            # Train generator with a new batch of generated samples\n", "            noise = torch.normal(0, 1, size=(batch_size, latent_dim), device=device)\n", "\n", "            # From the generator's perspective, the discriminator should predict\n", "            # ones for all samples\n", "            gen_labels = torch.ones((batch_size, 1), device=device)\n", "\n", "            # Train the GAN to predict ones\n", "            fake_images = generator(noise)\n", "            predictions = discriminator(fake_images)\n", "            gen_loss = loss(predictions, gen_labels)\n", "            gen_loss.backward()\n", "            optimizer_gen.step()\n", "            gen_losses.append(gen_loss.detach().cpu().numpy())\n", "        \n", "    # Every 5th epoch, display generated images and save model\n", "    if epoch % 5 == 0:\n", "        clear_output(wait=True)\n", "        plotGeneratedImages(epoch)\n", "        saveModels(epoch, 'CNN_GAN')"]}, {"cell_type": "markdown", "id": "749e9d74", "metadata": {"user_expressions": []}, "source": ["## Interpolation in the latent space\n", "In the past two models, we have used a 10-dimensional latent space. We're going to explore the content of this latent space a bit more. We randomly pick two points in the latent space and make a linear interpolation between these two points. Then we generate images from each of the interpolated latent points. "]}, {"cell_type": "code", "execution_count": null, "id": "53fde182", "metadata": {}, "outputs": [], "source": ["# Sample two points from noise distribution\n", "noise_a = torch.normal(0, 1, size=(1, latent_dim), device=device)\n", "noise_b = torch.normal(0, 1, size=(1, latent_dim), device=device)\n", "\n", "# Interpolate in steps of 10% between the two points\n", "noise = torch.zeros((10, latent_dim), dtype=torch.float, device=device)\n", "for i in range(10):\n", "    ni = i * 0.1\n", "    noise[i, :] = ni * noise_a + (1 - ni) * noise_b\n", "\n", "# Generate images from interpolated points\n", "with torch.no_grad():\n", "    fake_images = generator(noise)\n", "    fake_images = fake_images.cpu().numpy()\n", "    plotImages(fake_images, dim=(1, 10), figsize=(10, 2), title='Generated images from interpolated points')"]}, {"cell_type": "markdown", "id": "bad7cd8c", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "Explain what you see in this figure.\n", ":::"]}, {"cell_type": "markdown", "id": "b76026c1", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "You should see a smooth(ish) interpolation between two randomly generated digits. \n", ":::"]}, {"cell_type": "markdown", "id": "dc050436", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "Interpolation works, but what happens when you extrapolate out of the latent space distribution? Consider how the noise vectors are drawn. Inspect generated samples that are further away from the mode of your latent space.\n", ":::    "]}, {"cell_type": "markdown", "id": "a08727a5", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "The further we move away from the mode of the noise distribution, the less realistic the samples become. The assumption in our model is that the samples follow a Gaussian distribution, and the generator will not know how to properly handle noise that is far away from the center.\n", ":::"]}, {"cell_type": "markdown", "id": "fbb5b88e", "metadata": {"user_expressions": []}, "source": ["# MedMNIST image synthesis"]}, {"cell_type": "markdown", "id": "069b8be2", "metadata": {"user_expressions": []}, "source": ["In this section, we're going to synthesize images from [MedMNIST](https://medmnist.com/). This is a collection of datasets with binary (yes or no) or multiclass labels in 2D or 3D. If you didn't do it in Tutorial 2 you have to first install `medmnist` with the following commandline:"]}, {"cell_type": "code", "execution_count": null, "id": "cb43cc07", "metadata": {}, "outputs": [], "source": ["!pip install medmnist"]}, {"cell_type": "markdown", "id": "98f44cea", "metadata": {"user_expressions": []}, "source": ["As before, we can define a `MedMNISTData` class for these kinds of images."]}, {"cell_type": "code", "execution_count": null, "id": "6fbbb842", "metadata": {}, "outputs": [], "source": ["import os\n", "import monai\n", "import torchvision.transforms as transforms\n", "import medmnist\n", "\n", "class MedMNISTData(monai.data.Dataset):\n", "    \n", "    def __init__(self, datafile, transform=None):\n", "        self.data = datafile\n", "        self.transform = transform\n", "        \n", "        \n", "    def __getitem__(self, index):\n", "        # Make getitem return one tensor corresponding to the image\n", "        image = self.data[index][0]\n", "        if self.transform:\n", "            image = self.transform(image)\n", "        return image\n", "    \n", "    def __len__(self):\n", "        return len(self.data)\n", "\n", "data_transform = transforms.Compose([\n", "    transforms.ToTensor(),\n", "    transforms.Normalize(mean=[.5], std=[.5])\n", "])\n", "\n", "dataset = medmnist.PneumoniaMNIST(split=\"train\", download=True)\n", "train_dataset = MedMNISTData(dataset, transform=data_transform)\n", "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)"]}, {"cell_type": "markdown", "id": "b0eecf0d", "metadata": {"user_expressions": []}, "source": [":::{admonition} Exercise\n", ":class: tip\n", "In this second-to-last part of the practical you're going to repurpose the code that you have used so far to synthesize MedMNIST images. By now you should have sufficient experience with Python that you are able to fill in the block of code below and train your model. Reach out if you get stuck! <b>Good luck!</b>\n", ":::"]}, {"cell_type": "markdown", "id": "c8716d66", "metadata": {"tags": ["teacher"], "user_expressions": []}, "source": [":::{admonition} Answer key\n", ":class: seealso\n", "```python\n", "# Get networks\n", "discriminator = Discriminator_CNN()\n", "generator = Generator_CNN()\n", "\n", "# Push networks to device\n", "discriminator.to(device)\n", "generator.to(device)\n", "\n", "# Configure optimizers and loss function\n", "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n", "loss = torch.nn.BCELoss()\n", "\n", "dis_losses = []\n", "gen_losses = []\n", "\n", "epochs = 50\n", "batch_size = 100\n", "\n", "for epoch in range(1, epochs + 1):\n", "    \n", "    # Wrap dataloader into tqdm such that we can print progress while training\n", "    with tqdm(train_loader, unit=\"iterations\") as tqdm_iterator:\n", "        tqdm_iterator.set_description('Epoch {}'.format(epoch))\n", "        \n", "        for i, batch in enumerate(tqdm_iterator):\n", "            # ========== Train Discriminator ==========\n", "            \n", "            # Freeze generator part\n", "            for param in generator.parameters():\n", "                param.requires_grad = False\n", "            for param in discriminator.parameters():\n", "                param.requires_grad = True\n", "            discriminator.zero_grad()\n", "\n", "            # Get a random set of input noise\n", "            batch_size = len(batch)\n", "            noise = torch.normal(0, 1, size=(batch_size, latent_dim), device=device)\n", "\n", "            # Get real images and flatten the image dimensions\n", "            real_images = batch.to(device)\n", "            \n", "            # Generate some fake MNIST images using the generator\n", "            with torch.no_grad():\n", "                fake_images = generator(noise)\n", "\n", "            # Concatenate the fake and real images\n", "            dis_input = torch.cat((real_images, fake_images))\n", "\n", "            # Labels for generated and real data\n", "            dis_labels = torch.zeros((2 * batch_size, 1), device=device)\n", "\n", "            # One-sided label smoothing\n", "            dis_labels[:batch_size] = 0.9\n", "\n", "            # Train discriminator with this batch of samples\n", "            predictions = discriminator(dis_input)\n", "            dis_loss = loss(predictions, dis_labels)\n", "            dis_loss.backward()\n", "            optimizer_dis.step()\n", "            dis_losses.append(dis_loss.detach().cpu().numpy())\n", "\n", "            # ========== Train Generator ==========\n", "            \n", "            # Freeze the discriminator part\n", "            for param in generator.parameters():\n", "                param.requires_grad = True\n", "            for param in discriminator.parameters():\n", "                param.requires_grad = False\n", "            generator.zero_grad()\n", "\n", "            # Train generator with a new batch of generated samples\n", "            noise = torch.normal(0, 1, size=(batch_size, latent_dim), device=device)\n", "\n", "            # From the generator's perspective, the discriminator should predict\n", "            # ones for all samples\n", "            gen_labels = torch.ones((batch_size, 1), device=device)\n", "\n", "            # Train the GAN to predict ones\n", "            fake_images = generator(noise)\n", "            predictions = discriminator(fake_images)\n", "            gen_loss = loss(predictions, gen_labels)\n", "            gen_loss.backward()\n", "            optimizer_gen.step()\n", "            gen_losses.append(gen_loss.detach().cpu().numpy())\n", "        \n", "    # Every 5th epoch, display generated images and save model\n", "    if epoch % 5 == 0:\n", "        clear_output(wait=True)\n", "        plotGeneratedImages(epoch)\n", "        saveModels(epoch, 'CNN_GAN')\n", "        ```\n", ":::"]}, {"cell_type": "markdown", "id": "b9b4c8d0", "metadata": {"user_expressions": []}, "source": ["## Conditional MedMNIST synthesis\n", ":::{admonition} Exercise\n", ":class: tip\n", "For all MedMMNIST samples we already have labels (0, 1, ..., n). Try to change the MedMNIST synthesis code such that you can ask the generator to generate specific labels. I.e., try to train a conditional GAN. You can look for some inspiration in [this paper](https://arxiv.org/pdf/1411.1784.pdf), in particular Sec. 4.1. Remember that you already got the MedMNIST labels when loading the data set and you used them in Tutorial 2. You will have to add these labels to the discriminator and generator network. The easiest way is to just concatenate them to your input vectors.\n", ":::\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}